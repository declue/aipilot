#!/usr/bin/env python3
"""
DSPilot CLI ì‹¤í–‰ ê´€ë¦¬ ëª¨ë“ˆ
"""

import json
import re
from typing import Any, Callable, Dict, List, Optional

from dspilot_cli.constants import (
    ANALYSIS_PROMPT_TEMPLATE,
    FINAL_ANALYSIS_PROMPT_TEMPLATE,
    Defaults,
    ExecutionPlan,
    ExecutionStep,
    UserChoiceType,
)
from dspilot_cli.interaction_manager import InteractionManager
from dspilot_cli.output_manager import OutputManager
from dspilot_core.llm.agents.base_agent import BaseAgent
from dspilot_core.llm.mcp.mcp_tool_manager import MCPToolManager
from dspilot_core.llm.models.conversation_message import ConversationMessage


class ExecutionManager:
    """ê³„íš ìˆ˜ë¦½ ë° ì‹¤í–‰ ê´€ë¦¬ë¥¼ ë‹´ë‹¹í•˜ëŠ” í´ë˜ìŠ¤"""

    def __init__(
        self,
        output_manager: OutputManager,
        interaction_manager: InteractionManager,
        llm_agent: BaseAgent,
        mcp_tool_manager: MCPToolManager,
        validate_mode: str = Defaults.VALIDATE_MODE,
        max_step_retries: int = Defaults.MAX_STEP_RETRIES
    ) -> None:
        """
        ì‹¤í–‰ ê´€ë¦¬ì ì´ˆê¸°í™”

        Args:
            output_manager: ì¶œë ¥ ê´€ë¦¬ì
            interaction_manager: ìƒí˜¸ì‘ìš© ê´€ë¦¬ì
            llm_agent: LLM ì—ì´ì „íŠ¸
            mcp_tool_manager: MCP ë„êµ¬ ê´€ë¦¬ì
            validate_mode: ê²°ê³¼ ê²€ì¦ ëª¨ë“œ
            max_step_retries: ìµœëŒ€ ë‹¨ê³„ ì¬ì‹œë„ íšŸìˆ˜
        """
        self.output_manager = output_manager
        self.interaction_manager = interaction_manager
        self.llm_agent = llm_agent
        self.mcp_tool_manager = mcp_tool_manager
        self.max_step_retries = max_step_retries

        # ë²”ìš© ê²°ê³¼ ê²€ì¦ê¸° ì´ˆê¸°í™” (í•„ìš” ì‹œ)
        try:
            from dspilot_core.llm.utils.argument_fixer import (
                GenericArgumentFixer,  # pylint: disable=import-error
            )
            from dspilot_core.llm.utils.result_validator import (
                GenericResultValidator,  # pylint: disable=import-error
            )
            self.result_validator = GenericResultValidator(
                llm_service=self.llm_agent.llm_service,
                mode=validate_mode
            )
            self.argument_fixer = GenericArgumentFixer(self.llm_agent.llm_service)
        except Exception:  # noqa: broad-except
            self.result_validator = None
            self.argument_fixer = None

    async def analyze_request_and_plan(self, user_message: str) -> Optional[ExecutionPlan]:
        """
        ìš”ì²­ ë¶„ì„ ë° ì‹¤í–‰ ê³„íš ìˆ˜ë¦½

        Args:
            user_message: ì‚¬ìš©ì ë©”ì‹œì§€

        Returns:
            ì‹¤í–‰ ê³„íš (ë„êµ¬ê°€ í•„ìš”í•˜ì§€ ì•Šìœ¼ë©´ None)
        """
        try:
            # ì‚¬ìš© ê°€ëŠ¥í•œ ë„êµ¬ ëª©ë¡ í™•ì¸
            available_tools = await self._get_available_tools()
            if not available_tools:
                return None

            # ë„êµ¬ ëª©ë¡ ìƒì„±
            tools_desc = "\n".join([
                f"- {tool.name}: {tool.description}"
                for tool in available_tools
            ])

            # ê³„íš ìˆ˜ë¦½ í”„ë¡¬í”„íŠ¸
            analysis_prompt = ANALYSIS_PROMPT_TEMPLATE.format(
                user_message=user_message,
                tools_desc=tools_desc
            )

            context = [ConversationMessage(
                role="user", content=analysis_prompt)]
            response = await self.llm_agent.llm_service.generate_response(context)

            # JSON íŒŒì‹±
            plan_data = self._parse_plan_response(response.response)
            if plan_data and plan_data.get("need_tools", False):
                return self._create_execution_plan(plan_data.get("plan", {}))

        except Exception as e:
            self.output_manager.log_if_debug(f"ê³„íš ìˆ˜ë¦½ ì‹¤íŒ¨: {e}", "warning")

        return None

    async def execute_interactive_plan(self, plan: ExecutionPlan, original_prompt: str, 
                                      streaming_callback: Optional[Callable[[str], None]] = None) -> Dict[str, Any]:
        """
        ëŒ€í™”í˜• ê³„íš ì‹¤í–‰

        Args:
            plan: ì‹¤í–‰ ê³„íš
            original_prompt: ì›ë³¸ í”„ë¡¬í”„íŠ¸
            streaming_callback: ìŠ¤íŠ¸ë¦¬ë° ì½œë°± í•¨ìˆ˜
        Returns:
            Dict[str, Any]: {
               "step_results": Dict[int, Any],
               "errors": List[str]
            }
        """
        if not plan.steps:
            return {"step_results": {}, "errors": []}

        self.output_manager.print_execution_plan(plan.__dict__)
        step_results: Dict[int, Any] = {}
        errors: List[str] = []

        for step in plan.steps:
            success = await self._execute_step(step, step_results, original_prompt)
            if not success:
                # ì¤‘ë‹¨ëœ ê²½ìš° ì˜¤ë¥˜ë¡œ ê°„ì£¼í•˜ê³  ì¢…ë£Œ
                errors.append(f"ë‹¨ê³„ {step.step} ì¤‘ë‹¨")
                break

        # ìµœì¢… ê²°ê³¼ ë¶„ì„ ë° ì¶œë ¥
        await self._generate_final_response(original_prompt, step_results, streaming_callback)

        # ì¶”ê°€: ê²°ê³¼ ë‚´ error í‚¤ì›Œë“œ íƒì§€
        for raw in step_results.values():
            try:
                data = json.loads(raw)
                if isinstance(data, dict) and data.get("error"):
                    errors.append(str(data.get("error")))
            except Exception:
                if re.search(r"error", str(raw), re.IGNORECASE):
                    errors.append(str(raw))

        return {"step_results": step_results, "errors": errors}

    async def _execute_step(self, step: ExecutionStep, step_results: Dict[int, Any], original_prompt: str) -> bool:
        """
        ë‹¨ì¼ ë‹¨ê³„ ì‹¤í–‰

        Args:
            step: ì‹¤í–‰ ë‹¨ê³„
            step_results: ì´ì „ ë‹¨ê³„ ê²°ê³¼ë“¤
            original_prompt: ì›ë³¸ í”„ë¡¬í”„íŠ¸

        Returns:
            ê³„ì† ì§„í–‰ ì—¬ë¶€
        """
        self.output_manager.print_step_info(step.step, step.description)

        # ì‚¬ìš©ì í™•ì¸ (full-auto ëª¨ë“œê°€ ì•„ë‹Œ ê²½ìš°)
        user_choice = self.interaction_manager.get_user_confirmation(
            step.confirm_message, step.tool_name, step.arguments
        )

        if user_choice == UserChoiceType.SKIP:
            self.output_manager.print_step_skipped(step.step)
            return True
        elif user_choice == UserChoiceType.MODIFY:
            # ì‚¬ìš©ìê°€ ìˆ˜ì •ì„ ì›í•˜ëŠ” ê²½ìš°
            new_prompt = self.interaction_manager.get_new_request()
            if new_prompt:
                # ìƒˆë¡œìš´ ìš”ì²­ìœ¼ë¡œ ì²˜ë¦¬ (ì´ê²ƒì€ ì™¸ë¶€ì—ì„œ ì²˜ë¦¬í•´ì•¼ í•¨)
                # ì—¬ê¸°ì„œëŠ” ë‹¨ìˆœíˆ ì‹¤í–‰ì„ ì¤‘ë‹¨
                return False
        elif user_choice == UserChoiceType.CANCEL:
            self.output_manager.print_task_cancelled()
            return False

        # ë„êµ¬ ì‹¤í–‰
        try:
            retries = 0
            while retries <= self.max_step_retries:
                self.output_manager.print_step_execution(step.tool_name)

                # ì´ì „ ë‹¨ê³„ ê²°ê³¼ ì°¸ì¡° ì²˜ë¦¬
                processed_args = self._process_step_arguments(
                    step.arguments, step_results)

                # ë„êµ¬ ì‹¤í–‰
                try:
                    result = await self.mcp_tool_manager.call_mcp_tool(step.tool_name, processed_args)
                    exec_error = ""
                except Exception as exec_e:
                    exec_error = str(exec_e)
                    result = json.dumps({"error": exec_error})

                # ê²°ê³¼ ê²€ì¦ (ì˜µì…˜)
                needs_retry = False
                if self.result_validator:
                    eval_res = await self.result_validator.evaluate(
                        user_prompt=original_prompt,
                        tool_name=step.tool_name,
                        tool_args=processed_args,
                        raw_result=result
                    )
                    needs_retry = self.result_validator.needs_retry(eval_res)
                    if needs_retry:
                        self.output_manager.print_warning(
                            f"âš ï¸ ê²°ê³¼ ì‹ ë¢°ë„ ë‚®ìŒ â†’ ì¬ì‹œë„ {retries+1}/{self.max_step_retries}")

                # ì‹¤í–‰ ì˜ˆì™¸ê°€ ìˆì—ˆìœ¼ë©´ ë¬´ì¡°ê±´ retry
                if exec_error:
                    needs_retry = True

                if not needs_retry:
                    step_results[step.step] = result
                    self.output_manager.print_step_completed(step.step)
                    return True

                retries += 1

                # ë§¤ê°œë³€ìˆ˜ ìˆ˜ì • ì‹œë„
                if self.argument_fixer:
                    fixed = await self.argument_fixer.suggest(
                        user_prompt=original_prompt,
                        tool_name=step.tool_name,
                        original_args=processed_args,
                        error_msg=exec_error or "low_confidence_result"
                    )
                    if fixed:
                        processed_args.update(fixed)
                        self.output_manager.print_info(
                            f"ğŸ”§ íŒŒë¼ë¯¸í„° ìë™ ìˆ˜ì • ì ìš©: {fixed}")
                        continue

            # ì¬ì‹œë„ ëª¨ë‘ ì‹¤íŒ¨ â†’ ì˜¤ë¥˜ ì²˜ë¦¬
            self.output_manager.print_step_error(step.step, "ê²°ê³¼ ê²€ì¦ ì‹¤íŒ¨")
            return False
        except Exception as e:
            error_msg = str(e)
            self.output_manager.print_step_error(step.step, error_msg)
            return False

    async def _get_available_tools(self) -> List[Any]:
        """ì‚¬ìš© ê°€ëŠ¥í•œ ë„êµ¬ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°"""
        available_tools = []
        if self.mcp_tool_manager and hasattr(self.mcp_tool_manager, 'get_langchain_tools'):
            try:
                available_tools = await self.mcp_tool_manager.get_langchain_tools()
            except Exception as e:
                self.output_manager.log_if_debug(
                    f"ë„êµ¬ ëª©ë¡ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {e}", "warning")
        return available_tools

    def _parse_plan_response(self, response_text: str) -> Optional[Dict[str, Any]]:
        """ì‘ë‹µì—ì„œ JSON ê³„íš íŒŒì‹±"""
        try:
            start_idx = response_text.find("{")
            end_idx = response_text.rfind("}") + 1
            if start_idx != -1 and end_idx != 0:
                json_str = response_text[start_idx:end_idx]
                return json.loads(json_str)
        except (json.JSONDecodeError, ValueError):
            pass
        return None

    def _create_execution_plan(self, plan_data: Dict[str, Any]) -> ExecutionPlan:
        """ê³„íš ë°ì´í„°ë¡œë¶€í„° ExecutionPlan ê°ì²´ ìƒì„±"""
        steps = []
        for step_data in plan_data.get("steps", []):
            step = ExecutionStep(
                step=step_data.get("step", 0),
                description=step_data.get("description", ""),
                tool_name=step_data.get("tool_name", ""),
                arguments=step_data.get("arguments", {}),
                confirm_message=step_data.get("confirm_message", "")
            )
            steps.append(step)

        return ExecutionPlan(
            description=plan_data.get("description", "ë„êµ¬ ì‹¤í–‰ ê³„íš"),
            steps=steps
        )

    def _process_step_arguments(self,
                                arguments: Dict[str, Any],
                                step_results: Dict[int, Any]) -> Dict[str, Any]:
        """
        ë‹¨ê³„ ë§¤ê°œë³€ìˆ˜ ì²˜ë¦¬ (ì´ì „ ë‹¨ê³„ ê²°ê³¼ ì°¸ì¡°)

        Args:
            arguments: ì›ë³¸ ë§¤ê°œë³€ìˆ˜
            step_results: ì´ì „ ë‹¨ê³„ ê²°ê³¼ë“¤

        Returns:
            ì²˜ë¦¬ëœ ë§¤ê°œë³€ìˆ˜
        """
        processed = {}

        for key, value in arguments.items():
            if isinstance(value, str) and value.startswith("$step_"):
                # ì´ì „ ë‹¨ê³„ ê²°ê³¼ ì°¸ì¡°
                try:
                    step_num = int(value.split("_")[1])
                    if step_num in step_results:
                        processed[key] = step_results[step_num]
                    else:
                        processed[key] = value  # ì°¸ì¡° ì‹¤íŒ¨ ì‹œ ì›ë³¸ ìœ ì§€
                except (ValueError, IndexError):
                    processed[key] = value
            else:
                processed[key] = value

        return processed

    async def _generate_final_response(self,
                                       original_prompt: str,
                                       step_results: Dict[int, Any],
                                       streaming_callback: Optional[Callable[[str], None]] = None) -> None:
        """
        ìµœì¢… ì‘ë‹µ ìƒì„±

        Args:
            original_prompt: ì›ë³¸ í”„ë¡¬í”„íŠ¸
            step_results: ë‹¨ê³„ ì‹¤í–‰ ê²°ê³¼ë“¤
            streaming_callback: ìŠ¤íŠ¸ë¦¬ë° ì½œë°± í•¨ìˆ˜
        """
        if not step_results:
            return

        # ê²°ê³¼ ìš”ì•½
        results_summary = "\n".join([
            f"ë‹¨ê³„ {step}: {str(result)[:Defaults.RESULT_SUMMARY_MAX_LENGTH]}..."
            if len(str(result)) > Defaults.RESULT_SUMMARY_MAX_LENGTH
            else f"ë‹¨ê³„ {step}: {result}"
            for step, result in step_results.items()
        ])

        # ìµœì¢… ë¶„ì„ í”„ë¡¬í”„íŠ¸
        final_prompt = FINAL_ANALYSIS_PROMPT_TEMPLATE.format(
            original_prompt=original_prompt,
            results_summary=results_summary
        )

        try:
            context = [ConversationMessage(role="user", content=final_prompt)]
            
            # ìŠ¤íŠ¸ë¦¬ë° ëª¨ë“œì¸ ê²½ìš° ì½œë°±ê³¼ í•¨ê»˜ ì‘ë‹µ ìƒì„±
            if streaming_callback:
                self.output_manager.start_streaming_output()
                response = await self.llm_agent.llm_service.generate_response(context, streaming_callback)
                self.output_manager.finish_streaming_output()
            else:
                response = await self.llm_agent.llm_service.generate_response(context)

            response_data = {
                "response": response.response,
                "used_tools": list(step_results.keys()),
                "step_results": step_results
            }
            
            # ìŠ¤íŠ¸ë¦¬ë° ëª¨ë“œê°€ ì•„ë‹Œ ê²½ìš°ì—ë§Œ ì‘ë‹µ ì¶œë ¥ (ìŠ¤íŠ¸ë¦¬ë° ëª¨ë“œì—ì„œëŠ” ì´ë¯¸ ì¶œë ¥ë¨)
            if not streaming_callback:
                self.output_manager.print_response(
                    response.response,
                    response_data.get("used_tools", [])
                )

        except Exception as e:
            self.output_manager.log_if_debug(f"ìµœì¢… ì‘ë‹µ ìƒì„± ì‹¤íŒ¨: {e}", "error")
            # í´ë°±: ì›ì‹œ ê²°ê³¼ ì¶œë ¥
            self.output_manager.print_success("ì‘ì—… ì™„ë£Œ")
            self.output_manager.print_info(f"ê²°ê³¼: {results_summary}")
