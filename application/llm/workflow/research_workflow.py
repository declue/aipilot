"""
ì—°êµ¬/ì¡°ì‚¬ ì „ìš© ì›Œí¬í”Œë¡œìš°
ì •ë³´ ìˆ˜ì§‘ â†’ ë¶„ì„ â†’ ì¢…í•© ê²°ë¡  ë„ì¶œ ê³¼ì •
"""

import logging
from typing import Any, Callable, Optional

from application.llm.workflow.base_workflow import BaseWorkflow
from application.util.logger import setup_logger

logger = setup_logger(__name__) or logging.getLogger(__name__)


class ResearchWorkflow(BaseWorkflow):
    """ì—°êµ¬/ì¡°ì‚¬ ì›Œí¬í”Œë¡œìš°"""

    def __init__(self):
        self.steps = [
            "ì •ë³´_ìˆ˜ì§‘",
            "ë°ì´í„°_ë¶„ì„",
            "ì¢…í•©_ì •ë¦¬",
            "ê²°ë¡ _ë„ì¶œ",
        ]

    async def run(
        self, agent: Any, message: str, streaming_callback: Optional[Callable[[str], None]] = None
    ) -> str:
        """
        ì—°êµ¬ ì›Œí¬í”Œë¡œìš° ì‹¤í–‰

        Args:
            agent: LLM ì—ì´ì „íŠ¸
            message: ì—°êµ¬ ì£¼ì œ
            streaming_callback: ìŠ¤íŠ¸ë¦¬ë° ì½œë°±

        Returns:
            str: ì—°êµ¬ ê²°ê³¼
        """
        try:
            logger.info(f"ì—°êµ¬ ì›Œí¬í”Œë¡œìš° ì‹œì‘: {message[:50]}...")

            # 1ë‹¨ê³„: ì—°êµ¬ ê³„íš ìˆ˜ë¦½
            research_plan = await self._create_research_plan(agent, message, streaming_callback)
            
            # 2ë‹¨ê³„: ì •ë³´ ìˆ˜ì§‘
            collected_info = await self._collect_information(agent, research_plan, streaming_callback)
            
            # 3ë‹¨ê³„: ë°ì´í„° ë¶„ì„
            analysis_result = await self._analyze_data(agent, collected_info, streaming_callback)
            
            # 4ë‹¨ê³„: ìµœì¢… ì¢…í•© ë³´ê³ ì„œ ì‘ì„±
            final_report = await self._generate_final_report(
                agent, message, analysis_result, streaming_callback
            )

            logger.info("ì—°êµ¬ ì›Œí¬í”Œë¡œìš° ì™„ë£Œ")
            return final_report

        except Exception as e:
            logger.error(f"ì—°êµ¬ ì›Œí¬í”Œë¡œìš° ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
            return f"ì—°êµ¬ ì›Œí¬í”Œë¡œìš° ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"

    async def _create_research_plan(
        self, agent: Any, topic: str, streaming_callback: Optional[Callable[[str], None]] = None
    ) -> str:
        """ì—°êµ¬ ê³„íš ìˆ˜ë¦½"""
        planning_prompt = f"""
        ë‹¤ìŒ ì£¼ì œì— ëŒ€í•œ ì²´ê³„ì ì¸ ì—°êµ¬ ê³„íšì„ ìˆ˜ë¦½í•´ì£¼ì„¸ìš”:

        ì£¼ì œ: {topic}

        ë‹¤ìŒ ì‚¬í•­ë“¤ì„ í¬í•¨í•´ì£¼ì„¸ìš”:
        1. ì—°êµ¬ ëª©ì ê³¼ ë²”ìœ„
        2. í•µì‹¬ ì§ˆë¬¸ë“¤ (3-5ê°œ)
        3. í•„ìš”í•œ ì •ë³´ ìœ í˜•
        4. ì¡°ì‚¬ ë°©ë²•ë¡ 
        5. ì˜ˆìƒ ê²°ê³¼ë¬¼

        ê°„ê²°í•˜ê³  êµ¬ì²´ì ì¸ ê³„íšì„ ì‘ì„±í•´ì£¼ì„¸ìš”.
        """

        if streaming_callback:
            streaming_callback("ğŸ” ì—°êµ¬ ê³„íš ìˆ˜ë¦½ ì¤‘...\n\n")

        # ê¸°ë³¸ ì‘ë‹µ ìƒì„± ë©”ì„œë“œ ì‚¬ìš©
        if hasattr(agent, "_generate_basic_response"):
            plan = await agent._generate_basic_response(planning_prompt, streaming_callback)
        else:
            plan = "ì—°êµ¬ ê³„íš ìˆ˜ë¦½ ê¸°ëŠ¥ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

        logger.debug("ì—°êµ¬ ê³„íš ìˆ˜ë¦½ ì™„ë£Œ")
        return plan

    async def _collect_information(
        self, agent: Any, research_plan: str, streaming_callback: Optional[Callable[[str], None]] = None
    ) -> str:
        """ì •ë³´ ìˆ˜ì§‘ ë‹¨ê³„"""
        collection_prompt = f"""
        ë‹¤ìŒ ì—°êµ¬ ê³„íšì— ë”°ë¼ ì •ë³´ ìˆ˜ì§‘ì„ ì§„í–‰í•´ì£¼ì„¸ìš”:

        {research_plan}

        ì‚¬ìš© ê°€ëŠ¥í•œ ë„êµ¬ë“¤ì„ í™œìš©í•˜ì—¬ ë‹¤ì–‘í•œ ê´€ì ì—ì„œ ì •ë³´ë¥¼ ìˆ˜ì§‘í•˜ê³ ,
        ê° ì •ë³´ì˜ ì¶œì²˜ì™€ ì‹ ë¢°ì„±ì„ ëª…ì‹œí•´ì£¼ì„¸ìš”.

        íŠ¹íˆ ë‹¤ìŒ ì‚¬í•­ì— ì¤‘ì ì„ ë‘ì„¸ìš”:
        - ìµœì‹  ì •ë³´ì™€ ë™í–¥
        - ë‹¤ì–‘í•œ ê´€ì ê³¼ ì˜ê²¬
        - êµ¬ì²´ì ì¸ ë°ì´í„°ì™€ ì‚¬ë¡€
        - ì „ë¬¸ê°€ ê²¬í•´ë‚˜ ì—°êµ¬ ê²°ê³¼
        """

        if streaming_callback:
            streaming_callback("ğŸ“š ì •ë³´ ìˆ˜ì§‘ ì¤‘...\n\n")

        # MCP ë„êµ¬ê°€ ìˆëŠ” ê²½ìš° í™œìš©
        if hasattr(agent, "mcp_tool_manager") and agent.mcp_tool_manager:
            # ReactAgentì˜ generate_response ì‚¬ìš©í•˜ì—¬ ë„êµ¬ í™œìš©
            if hasattr(agent, "generate_response"):
                result = await agent.generate_response(collection_prompt, streaming_callback)
                return result.get("response", "ì •ë³´ ìˆ˜ì§‘ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")

        # ê¸°ë³¸ ì‘ë‹µ ìƒì„±
        if hasattr(agent, "_generate_basic_response"):
            info = await agent._generate_basic_response(collection_prompt, streaming_callback)
        else:
            info = "ì •ë³´ ìˆ˜ì§‘ ê¸°ëŠ¥ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

        logger.debug("ì •ë³´ ìˆ˜ì§‘ ì™„ë£Œ")
        return info

    async def _analyze_data(
        self, agent: Any, collected_info: str, streaming_callback: Optional[Callable[[str], None]] = None
    ) -> str:
        """ë°ì´í„° ë¶„ì„ ë‹¨ê³„"""
        analysis_prompt = f"""
        ìˆ˜ì§‘ëœ ë‹¤ìŒ ì •ë³´ë¥¼ ë¶„ì„í•´ì£¼ì„¸ìš”:

        {collected_info}

        ë¶„ì„ ì‹œ ë‹¤ìŒ ê´€ì ë“¤ì„ ê³ ë ¤í•´ì£¼ì„¸ìš”:
        1. í•µì‹¬ íŒ¨í„´ê³¼ íŠ¸ë Œë“œ ì‹ë³„
        2. ìƒì¶©ë˜ëŠ” ì •ë³´ë‚˜ ì˜ê²¬ ë¶„ì„
        3. ë°ì´í„°ì˜ ì‹ ë¢°ì„±ê³¼ í•œê³„ì  í‰ê°€
        4. ìˆ¨ê²¨ì§„ ì¸ì‚¬ì´íŠ¸ë‚˜ ì‹œì‚¬ì  ë„ì¶œ
        5. ì¶”ê°€ ì¡°ì‚¬ê°€ í•„ìš”í•œ ì˜ì—­ ì‹ë³„

        ê°ê´€ì ì´ê³  ë…¼ë¦¬ì ì¸ ë¶„ì„ì„ ì œê³µí•´ì£¼ì„¸ìš”.
        """

        if streaming_callback:
            streaming_callback("ğŸ”¬ ë°ì´í„° ë¶„ì„ ì¤‘...\n\n")

        if hasattr(agent, "_generate_basic_response"):
            analysis = await agent._generate_basic_response(analysis_prompt, streaming_callback)
        else:
            analysis = "ë°ì´í„° ë¶„ì„ ê¸°ëŠ¥ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

        logger.debug("ë°ì´í„° ë¶„ì„ ì™„ë£Œ")
        return analysis

    async def _generate_final_report(
        self,
        agent: Any,
        original_topic: str,
        analysis_result: str,
        streaming_callback: Optional[Callable[[str], None]] = None,
    ) -> str:
        """ìµœì¢… ì¢…í•© ë³´ê³ ì„œ ì‘ì„±"""
        report_prompt = f"""
        ì›ë˜ ì£¼ì œ: {original_topic}

        ë¶„ì„ ê²°ê³¼: {analysis_result}

        ìœ„ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ë‹¤ìŒ êµ¬ì¡°ì˜ ì¢…í•© ë³´ê³ ì„œë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”:

        # ì—°êµ¬ ë³´ê³ ì„œ: {original_topic}

        ## 1. ìš”ì•½ (Executive Summary)
        - í•µì‹¬ ë°œê²¬ì‚¬í•­ 3-5ì¤„ë¡œ ìš”ì•½

        ## 2. ì£¼ìš” ë°œê²¬ì‚¬í•­ (Key Findings)
        - ê°€ì¥ ì¤‘ìš”í•œ ë°œê²¬ì‚¬í•­ë“¤ì„ ìš°ì„ ìˆœìœ„ ìˆœìœ¼ë¡œ

        ## 3. ìƒì„¸ ë¶„ì„ (Detailed Analysis)
        - ìˆ˜ì§‘ëœ ë°ì´í„°ì˜ ì‹¬ì¸µ ë¶„ì„
        - íŒ¨í„´, íŠ¸ë Œë“œ, ì¸ì‚¬ì´íŠ¸

        ## 4. ì‹œì‚¬ì  (Implications)
        - ì‹¤ë¬´ì /ì „ëµì  ì‹œì‚¬ì 
        - í–¥í›„ ì „ë§

        ## 5. ì œí•œì‚¬í•­ (Limitations)
        - ì—°êµ¬ì˜ í•œê³„ì 
        - ì¶”ê°€ ì¡°ì‚¬ í•„ìš” ì˜ì—­

        ## 6. ê²°ë¡  ë° ê¶Œì¥ì‚¬í•­ (Conclusions & Recommendations)
        - ëª…í™•í•œ ê²°ë¡ 
        - êµ¬ì²´ì ì¸ ê¶Œì¥ì‚¬í•­

        ì „ë¬¸ì ì´ê³  êµ¬ì¡°í™”ëœ ë³´ê³ ì„œë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”.
        """

        if streaming_callback:
            streaming_callback("ğŸ“ ìµœì¢… ë³´ê³ ì„œ ì‘ì„± ì¤‘...\n\n")

        if hasattr(agent, "_generate_basic_response"):
            report = await agent._generate_basic_response(report_prompt, streaming_callback)
        else:
            report = "ìµœì¢… ë³´ê³ ì„œ ì‘ì„± ê¸°ëŠ¥ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

        logger.debug("ìµœì¢… ë³´ê³ ì„œ ì‘ì„± ì™„ë£Œ")
        return report 