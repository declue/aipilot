# LLM ëª¨ë“ˆ ì‚¬ìš© ì˜ˆì‹œ

## ğŸ†• ì ì‘í˜• ì›Œí¬í”Œë¡œìš° (Adaptive Workflow)

ìƒˆë¡œìš´ ì ì‘í˜• ì›Œí¬í”Œë¡œìš°ëŠ” ì‚¬ìš©ì ìš”ì²­ì„ ìë™ìœ¼ë¡œ ë¶„ì„í•˜ì—¬ í•„ìš”í•œ ë„êµ¬ì™€ ë‹¨ê³„ë¥¼ ê³„íší•˜ê³  ì‹¤í–‰í•©ë‹ˆë‹¤.

```python
from application.llm.workflow import AdaptiveWorkflow

# ì ì‘í˜• ì›Œí¬í”Œë¡œìš° ì‚¬ìš©
adaptive_workflow = AdaptiveWorkflow()

# ë³µí•©ì ì¸ ìš”ì²­ ì²˜ë¦¬ (ê²€ìƒ‰ + ì €ì¥)
result = await adaptive_workflow.run(
    agent, 
    "ì˜¤ëŠ˜ ì£¼ìš” ë‰´ìŠ¤ë¥¼ news.json íŒŒì¼ë¡œ ì €ì¥í•´ì¤˜"
)
# â†’ 1ë‹¨ê³„: ë‰´ìŠ¤ ê²€ìƒ‰, 2ë‹¨ê³„: ë°ì´í„° ì •ì œ, 3ë‹¨ê³„: JSON íŒŒì¼ ì €ì¥

# ë‹¤ë‹¨ê³„ ë¶„ì„ ì‘ì—…
result = await adaptive_workflow.run(
    agent,
    "í˜„ì¬ ì£¼ì‹ì‹œì¥ ë™í–¥ì„ ë¶„ì„í•˜ê³  ë³´ê³ ì„œë¥¼ ë§Œë“¤ì–´ì¤˜"  
)
# â†’ 1ë‹¨ê³„: ì£¼ì‹ ì •ë³´ ìˆ˜ì§‘, 2ë‹¨ê³„: ë°ì´í„° ë¶„ì„, 3ë‹¨ê³„: ë³´ê³ ì„œ ìƒì„±
```

### ğŸ¯ íŠ¹ì§•

- **í‚¤ì›Œë“œ ë¬´ê´€**: íŠ¹ì • í‚¤ì›Œë“œë‚˜ ì¡°ê±´ì— ì˜ì¡´í•˜ì§€ ì•ŠëŠ” ë²”ìš©ì  ì ‘ê·¼
- **ìë™ ê³„íš**: LLMì´ ì§ì ‘ í•„ìš”í•œ ë‹¨ê³„ë“¤ì„ ë¶„ì„í•˜ê³  ê³„íš ìˆ˜ë¦½
- **ìˆœì°¨ ì‹¤í–‰**: ë‹¨ê³„ë³„ ì˜ì¡´ì„±ì„ ê³ ë ¤í•œ ìˆœì°¨ì  ë„êµ¬ ì‹¤í–‰
- **ê²°ê³¼ í†µí•©**: ê° ë‹¨ê³„ ê²°ê³¼ë¥¼ ì¢…í•©í•˜ì—¬ ì™„ì „í•œ ìµœì¢… ì‘ë‹µ ìƒì„±

## 1. ê¸°ë³¸ ì—ì´ì „íŠ¸ ì‚¬ìš©

```python
from application.llm import AgentFactory

# ì—ì´ì „íŠ¸ ìƒì„± (ì´ì œ ê¸°ë³¸ì ìœ¼ë¡œ ì ì‘í˜• ì›Œí¬í”Œë¡œìš° ì§€ì›)
agent = AgentFactory.create_agent(config_manager)

# ë³µí•© ì‘ì—… ìš”ì²­
response = await agent.generate_response("ì˜¤ëŠ˜ ë‚ ì”¨ì™€ ë‰´ìŠ¤ë¥¼ í™•ì¸í•˜ê³  ìš”ì•½í•´ì¤˜")
# â†’ ìë™ìœ¼ë¡œ ë‚ ì”¨ ì¡°íšŒ + ë‰´ìŠ¤ ê²€ìƒ‰ + ê²°ê³¼ í†µí•©

# ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ ë‹¨ê³„ë³„ ì§„í–‰ ìƒí™© í™•ì¸
def streaming_callback(chunk):
    print(chunk, end="", flush=True)

response = await agent.generate_response(
    "ì£¼ì‹ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•˜ê³  ì°¨íŠ¸ë¥¼ ìƒì„±í•´ì¤˜", 
    streaming_callback
)
```

## 2. ì„¤ì • ê²€ì¦

```python
from application.llm.validators import LLMConfigValidator, MCPConfigValidator
from application.llm.models import LLMConfig, MCPConfig

# LLM ì„¤ì • ê²€ì¦ (adaptive ëª¨ë“œ í¬í•¨)
try:
    config = LLMConfig(
        api_key="test", 
        model="gpt-4o-mini", 
        temperature=0.7,
        mode="adaptive"  # ì ì‘í˜• ì›Œí¬í”Œë¡œìš° ëª¨ë“œ
    )
    LLMConfigValidator.validate_config(config)
    print("âœ… LLM ì„¤ì • ìœ íš¨")
except Exception as e:
    print(f"âŒ LLM ì„¤ì • ì˜¤ë¥˜: {e}")

# MCP ì„¤ì • ê²€ì¦
try:
    mcp_config = MCPConfig.from_dict({
        "enabled": True,
        "mcp_servers": {
            "search": {"command": "uvx", "args": ["mcp-server-duckduckgo"]},
            "file": {"command": "python", "args": ["tools/file_explorer_mcp/file_mcp_tool.py"]}
        }
    })
    MCPConfigValidator.validate_config(mcp_config)
    print("âœ… MCP ì„¤ì • ìœ íš¨")
except Exception as e:
    print(f"âŒ MCP ì„¤ì • ì˜¤ë¥˜: {e}")
```

## 3. ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§

```python
from application.llm.monitoring import get_metrics, PerformanceTracker

# ì„±ëŠ¥ ì¶”ì 
async def example_with_tracking():
    tracker = PerformanceTracker(
        operation_name="adaptive_workflow",
        agent_type="ReactAgent", 
        model="gpt-4o-mini",
        track_metrics=True
    )
    
    async with tracker.atrack():
        # ì ì‘í˜• ì›Œí¬í”Œë¡œìš° ì‹¤í–‰
        result = await agent.generate_response("ë³µí•©ì ì¸ ì‘ì—… ìš”ì²­")
        return result

# ë©”íŠ¸ë¦­ìŠ¤ ì¡°íšŒ
metrics = get_metrics()
summary = metrics.get_summary_report()
print(f"ì´ ìš”ì²­: {summary['total_requests']}")
print(f"ì„±ê³µë¥ : {summary['success_rate']:.2%}")
print(f"í‰ê·  ì‘ë‹µ ì‹œê°„: {summary['average_response_time']:.2f}ì´ˆ")
```

## 4. ì›Œí¬í”Œë¡œìš° ì‚¬ìš©

```python
from application.llm.workflow import get_workflow

# ì ì‘í˜• ì›Œí¬í”Œë¡œìš° (ê¶Œì¥)
adaptive_workflow = get_workflow("adaptive")()
result = await adaptive_workflow.run(agent, "ë³µí•©ì ì¸ ìš”ì²­")

# ì—°êµ¬ ì›Œí¬í”Œë¡œìš°
research_workflow = get_workflow("research")()
result = await research_workflow.run(agent, "AI ê¸°ìˆ  ë™í–¥ ë¶„ì„")

# ë¬¸ì œ í•´ê²° ì›Œí¬í”Œë¡œìš°  
problem_workflow = get_workflow("problem_solving")()
result = await problem_workflow.run(agent, "ì„œë²„ ì„±ëŠ¥ ì´ìŠˆ í•´ê²°")

# ë‹¤ë‹¨ê³„ ì›Œí¬í”Œë¡œìš°
multi_step_workflow = get_workflow("multi_step")()
result = await multi_step_workflow.run(agent, "ë³µì¡í•œ í”„ë¡œì íŠ¸ ê³„íš ìˆ˜ë¦½")
```

## 5. ì»¤ìŠ¤í…€ ì›Œí¬í”Œë¡œìš°

```python
from application.llm.workflow import BaseWorkflow, register_workflow

class CodeReviewWorkflow(BaseWorkflow):
    async def run(self, agent, message, streaming_callback=None):
        # 1ë‹¨ê³„: ì½”ë“œ ë¶„ì„
        analysis_prompt = f"ë‹¤ìŒ ì½”ë“œë¥¼ ë¶„ì„í•´ì£¼ì„¸ìš”:\n{message}"
        analysis = await agent._generate_basic_response(analysis_prompt)
        
        # 2ë‹¨ê³„: ê°œì„ ì  ë„ì¶œ
        improvement_prompt = f"ë¶„ì„ ê²°ê³¼:\n{analysis}\n\nê°œì„ ì ì„ ì œì•ˆí•´ì£¼ì„¸ìš”."
        improvements = await agent._generate_basic_response(improvement_prompt)
        
        return f"## ì½”ë“œ ë¶„ì„\n{analysis}\n\n## ê°œì„  ì œì•ˆ\n{improvements}"

# ì›Œí¬í”Œë¡œìš° ë“±ë¡ ë° ì‚¬ìš©
register_workflow("code_review", CodeReviewWorkflow)
workflow = get_workflow("code_review")()
result = await workflow.run(agent, "def hello(): print('world')")
```

## 6. MCP ë„êµ¬ í™œìš©

```python
from application.llm.mcp import MCPManager, MCPToolManager

# MCP ì´ˆê¸°í™”
mcp_manager = MCPManager(config_manager)
mcp_tool_manager = MCPToolManager(mcp_manager, config_manager)

await mcp_tool_manager.initialize()

# ì‚¬ìš© ê°€ëŠ¥í•œ ë„êµ¬ í™•ì¸
tools = await mcp_tool_manager.get_langchain_tools()
print(f"ì‚¬ìš© ê°€ëŠ¥í•œ ë„êµ¬: {[tool.name for tool in tools]}")

# ë„êµ¬ ì§ì ‘ í˜¸ì¶œ
result = await mcp_tool_manager.call_mcp_tool("search_web", {"query": "Python"})
print(result)
```

## 7. ì»¤ìŠ¤í…€ í”„ë¡œì„¸ì„œ

```python
from application.llm.processors import ToolResultProcessor

class WeatherProcessor(ToolResultProcessor):
    def can_process(self, tool_name: str) -> bool:
        return "weather" in tool_name.lower()
    
    def process(self, tool_name: str, tool_result: str) -> str:
        import json
        try:
            data = json.loads(tool_result)
            result = data.get("result", {})
            
            return f"""
ğŸŒ¤ï¸ ë‚ ì”¨ ì •ë³´:
ğŸ“ ìœ„ì¹˜: {result.get('location', 'N/A')}
ğŸŒ¡ï¸ ì˜¨ë„: {result.get('temperature', 'N/A')}Â°C
ğŸ’§ ìŠµë„: {result.get('humidity', 'N/A')}%
ğŸŒ¬ï¸ ë°”ëŒ: {result.get('wind_speed', 'N/A')} m/s
â˜ï¸ ìƒíƒœ: {result.get('condition', 'N/A')}
"""
        except:
            return f"ë‚ ì”¨ ì •ë³´ ì²˜ë¦¬ ì‹¤íŒ¨: {tool_result}"
    
    def get_priority(self) -> int:
        return 80

# í”„ë¡œì„¸ì„œ ë“±ë¡ (BaseAgent ì¸ìŠ¤í„´ìŠ¤ì—ì„œ)
agent._get_processor_registry().register(WeatherProcessor())
```

## 8. êµ¬ì¡°í™”ëœ ë¡œê¹…

```python
from application.llm.utils.logging_utils import get_llm_logger, LogLevel

logger = get_llm_logger(__name__)

# ì¼ë°˜ ë¡œê·¸
logger.info("ì—ì´ì „íŠ¸ ì‹œì‘", context={"user_id": "123", "session_id": "abc"})

# Agent í™œë™ ë¡œê·¸
logger.log_agent_activity(
    agent_type="ReactAgent",
    operation="adaptive_workflow", 
    message="ì ì‘í˜• ì›Œí¬í”Œë¡œìš° ì™„ë£Œ",
    duration=2.5,
    success=True
)

# MCP ì´ë²¤íŠ¸ ë¡œê·¸
logger.log_mcp_event(
    server_name="search",
    operation="tool_call",
    message="ê²€ìƒ‰ ë„êµ¬ í˜¸ì¶œ",
    tool_name="search_web"
)

# ì›Œí¬í”Œë¡œìš° ë¡œê·¸
logger.log_workflow_event(
    workflow_name="adaptive",
    step="data_collection",
    message="ì •ë³´ ìˆ˜ì§‘ ë‹¨ê³„ ì™„ë£Œ"
)
```

## ğŸš€ CLIì—ì„œ ì ì‘í˜• ì›Œí¬í”Œë¡œìš° ì‚¬ìš©

```bash
# CLI ì‹¤í–‰
python dspilot_cli.py

# ë³µí•© ì‘ì—… ìš”ì²­ ì˜ˆì‹œ
ğŸ‘¤ You: ì˜¤ëŠ˜ ì£¼ìš” ë‰´ìŠ¤ 3ê±´ì„ news.jsonìœ¼ë¡œ ì €ì¥í•´ì¤˜

# ìë™ìœ¼ë¡œ ë‹¤ìŒ ë‹¨ê³„ê°€ ì‹¤í–‰ë©ë‹ˆë‹¤:
# ğŸ”„ ì›Œí¬í”Œë¡œìš° ì‹¤í–‰ ì‹œì‘ (3ë‹¨ê³„)
# âœ… ì›¹ì—ì„œ ë‰´ìŠ¤ ê²€ìƒ‰
# âœ… ë‰´ìŠ¤ ë°ì´í„° ì •ì œ ë° êµ¬ì¡°í™”  
# âœ… JSON íŒŒì¼ë¡œ ì €ì¥
# ğŸ¯ ì›Œí¬í”Œë¡œìš° ì™„ë£Œ: 3/3ë‹¨ê³„ ì„±ê³µ
```

ì´ ì˜ˆì‹œë“¤ì„ ì°¸ê³ í•˜ì—¬ ìƒˆë¡œìš´ ì ì‘í˜• ì›Œí¬í”Œë¡œìš°ë¥¼ íš¨ê³¼ì ìœ¼ë¡œ í™œìš©í•˜ì„¸ìš”! ğŸš€
