# LLM ëª¨ë“ˆ ì‚¬ìš© ì˜ˆì‹œ

## ğŸ†• ì—ì´ì „íŠ¸ ì›Œí¬í”Œë¡œìš° (Agent Workflow)

ìƒˆë¡œìš´ ì—ì´ì „íŠ¸ ì›Œí¬í”Œë¡œìš°ëŠ” ì‚¬ìš©ìì™€ ëŒ€í™”í˜•ìœ¼ë¡œ ì†Œí†µí•˜ë©° ë‹¨ê³„ë³„ë¡œ ì‘ì—…ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.

```python
from application.llm.workflow import AgentWorkflow

# ì—ì´ì „íŠ¸ ì›Œí¬í”Œë¡œìš° ì‚¬ìš©
agent_workflow = AgentWorkflow()

# ëŒ€í™”í˜• ìš”ì²­ ì²˜ë¦¬
result = await agent_workflow.run(
    agent, 
    "ì˜¤ëŠ˜ ì£¼ìš” ë‰´ìŠ¤ë¥¼ news.json íŒŒì¼ë¡œ ì €ì¥í•´ì¤˜"
)
# â†’ ì‚¬ìš©ìì™€ ë‹¨ê³„ë³„ ìƒí˜¸ì‘ìš©ì„ í†µí•´ ì§„í–‰

# ê³„íš ìŠ¹ì¸ ê¸°ë°˜ ì‘ì—…
result = await agent_workflow.run(
    agent,
    "í˜„ì¬ ì£¼ì‹ì‹œì¥ ë™í–¥ì„ ë¶„ì„í•˜ê³  ë³´ê³ ì„œë¥¼ ë§Œë“¤ì–´ì¤˜"  
)
# â†’ ê³„íš ì œì•ˆ â†’ ì‚¬ìš©ì ìŠ¹ì¸ â†’ ì‹¤í–‰ â†’ ê²€í†  â†’ ë‹¤ìŒ ë‹¨ê³„
```

### ğŸ¯ íŠ¹ì§•

- **ëŒ€í™”í˜•**: ì‚¬ìš©ìì™€ ë‹¨ê³„ë³„ í”¼ë“œë°±ì„ ì£¼ê³ ë°›ìœ¼ë©° ì§„í–‰
- **ê³„íš ìŠ¹ì¸**: ê° ë‹¨ê³„ë§ˆë‹¤ ì‚¬ìš©ìì˜ ìŠ¹ì¸ì„ ë°›ê³  ì§„í–‰
- **ìƒíƒœ ìœ ì§€**: ì›Œí¬í”Œë¡œìš° ìƒíƒœë¥¼ ì¶”ì í•˜ê³  ì¤‘ë‹¨/ì¬ê°œ ê°€ëŠ¥
- **ìœ ì—°ì„±**: ì‚¬ìš©ì í”¼ë“œë°±ì— ë”°ë¥¸ ê³„íš ìˆ˜ì • ë° ê°œì„ 

## 1. ê¸°ë³¸ ì—ì´ì „íŠ¸ ì‚¬ìš©

```python
from application.llm import AgentFactory

# ì—ì´ì „íŠ¸ ìƒì„± (ì´ì œ ê¸°ë³¸ì ìœ¼ë¡œ ì—ì´ì „íŠ¸ ì›Œí¬í”Œë¡œìš° ì§€ì›)
agent = AgentFactory.create_agent(config_manager)

# ë³µí•© ì‘ì—… ìš”ì²­
response = await agent.generate_response("ì˜¤ëŠ˜ ë‚ ì”¨ì™€ ë‰´ìŠ¤ë¥¼ í™•ì¸í•˜ê³  ìš”ì•½í•´ì¤˜")
# â†’ ì‚¬ìš©ìì™€ ìƒí˜¸ì‘ìš©í•˜ë©° ë‚ ì”¨ ì¡°íšŒ + ë‰´ìŠ¤ ê²€ìƒ‰ + ê²°ê³¼ í†µí•©

# ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ ë‹¨ê³„ë³„ ì§„í–‰ ìƒí™© í™•ì¸
def streaming_callback(chunk):
    print(chunk, end="", flush=True)

response = await agent.generate_response(
    "ì£¼ì‹ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•˜ê³  ì°¨íŠ¸ë¥¼ ìƒì„±í•´ì¤˜", 
    streaming_callback
)
```

## 2. ì„¤ì • ê²€ì¦

```python
from application.llm.validators import LLMConfigValidator, MCPConfigValidator
from application.llm.models import LLMConfig, MCPConfig

# LLM ì„¤ì • ê²€ì¦ (workflow ëª¨ë“œ í¬í•¨)
try:
    config = LLMConfig(
        api_key="test", 
        model="gpt-4o-mini", 
        temperature=0.7,
        mode="workflow"  # ì›Œí¬í”Œë¡œìš° ëª¨ë“œ
    )
    LLMConfigValidator.validate_config(config)
    print("âœ… LLM ì„¤ì • ìœ íš¨")
except Exception as e:
    print(f"âŒ LLM ì„¤ì • ì˜¤ë¥˜: {e}")

# MCP ì„¤ì • ê²€ì¦
try:
    mcp_config = MCPConfig.from_dict({
        "enabled": True,
        "mcp_servers": {
            "search": {"command": "uvx", "args": ["mcp-server-duckduckgo"]},
            "file": {"command": "python", "args": ["tools/file_explorer_mcp/file_mcp_tool.py"]}
        }
    })
    MCPConfigValidator.validate_config(mcp_config)
    print("âœ… MCP ì„¤ì • ìœ íš¨")
except Exception as e:
    print(f"âŒ MCP ì„¤ì • ì˜¤ë¥˜: {e}")
```

## 3. ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§

```python
from application.llm.monitoring import get_metrics, PerformanceTracker

# ì„±ëŠ¥ ì¶”ì 
async def example_with_tracking():
    tracker = PerformanceTracker(
        operation_name="agent_workflow",
        agent_type="ReactAgent", 
        model="gpt-4o-mini",
        track_metrics=True
    )
    
    async with tracker.atrack():
        # ì—ì´ì „íŠ¸ ì›Œí¬í”Œë¡œìš° ì‹¤í–‰
        result = await agent.generate_response("ë³µí•©ì ì¸ ì‘ì—… ìš”ì²­")
        return result

# ë©”íŠ¸ë¦­ìŠ¤ ì¡°íšŒ
metrics = get_metrics()
summary = metrics.get_summary_report()
print(f"ì´ ìš”ì²­: {summary['total_requests']}")
print(f"ì„±ê³µë¥ : {summary['success_rate']:.2%}")
print(f"í‰ê·  ì‘ë‹µ ì‹œê°„: {summary['average_response_time']:.2f}ì´ˆ")
```

## 4. ì›Œí¬í”Œë¡œìš° ì‚¬ìš©

```python
from application.llm.workflow import get_workflow

# ì—ì´ì „íŠ¸ ì›Œí¬í”Œë¡œìš° (ëŒ€í™”í˜•, ê¶Œì¥)
agent_workflow = get_workflow("agent")()
result = await agent_workflow.run(agent, "ë³µí•©ì ì¸ ìš”ì²­")

# ê¸°ë³¸ ì§ˆì˜ì‘ë‹µ ì›Œí¬í”Œë¡œìš° (ë‹¨ìˆœí•œ ì§ˆë¬¸)
basic_workflow = get_workflow("basic")()
result = await basic_workflow.run(agent, "Pythonì—ì„œ ë¦¬ìŠ¤íŠ¸ì™€ íŠœí”Œì˜ ì°¨ì´ì ì€?")

# ì „ë¬¸ ë¦¬ì„œì¹˜ ì›Œí¬í”Œë¡œìš° (ì›¹ê²€ìƒ‰ ê¸°ë°˜)
research_workflow = get_workflow("research")()
result = await research_workflow.run(agent, "2024ë…„ AI ê¸°ìˆ  ë™í–¥ ë¶„ì„")
```

## 5. ì»¤ìŠ¤í…€ ì›Œí¬í”Œë¡œìš°

```python
from application.llm.workflow import BaseWorkflow, register_workflow

class CodeReviewWorkflow(BaseWorkflow):
    async def run(self, agent, message, streaming_callback=None):
        # 1ë‹¨ê³„: ì½”ë“œ ë¶„ì„
        analysis_prompt = f"ë‹¤ìŒ ì½”ë“œë¥¼ ë¶„ì„í•´ì£¼ì„¸ìš”:\n{message}"
        analysis = await agent._generate_basic_response(analysis_prompt)
        
        # 2ë‹¨ê³„: ê°œì„ ì  ë„ì¶œ
        improvement_prompt = f"ë¶„ì„ ê²°ê³¼:\n{analysis}\n\nê°œì„ ì ì„ ì œì•ˆí•´ì£¼ì„¸ìš”."
        improvements = await agent._generate_basic_response(improvement_prompt)
        
        return f"## ì½”ë“œ ë¶„ì„\n{analysis}\n\n## ê°œì„  ì œì•ˆ\n{improvements}"

# ì›Œí¬í”Œë¡œìš° ë“±ë¡ ë° ì‚¬ìš©
register_workflow("code_review", CodeReviewWorkflow)
workflow = get_workflow("code_review")()
result = await workflow.run(agent, "def hello(): print('world')")
```

## 6. MCP ë„êµ¬ í™œìš©

```python
from application.llm.mcp import MCPManager, MCPToolManager

# MCP ì´ˆê¸°í™”
mcp_manager = MCPManager(config_manager)
mcp_tool_manager = MCPToolManager(mcp_manager, config_manager)

await mcp_tool_manager.initialize()

# ì‚¬ìš© ê°€ëŠ¥í•œ ë„êµ¬ í™•ì¸
tools = await mcp_tool_manager.get_langchain_tools()
print(f"ì‚¬ìš© ê°€ëŠ¥í•œ ë„êµ¬: {[tool.name for tool in tools]}")

# ë„êµ¬ ì§ì ‘ í˜¸ì¶œ
result = await mcp_tool_manager.call_mcp_tool("search_web", {"query": "Python"})
print(result)
```

## 7. ì»¤ìŠ¤í…€ í”„ë¡œì„¸ì„œ

```python
from application.llm.processors import ToolResultProcessor

class WeatherProcessor(ToolResultProcessor):
    def can_process(self, tool_name: str) -> bool:
        return "weather" in tool_name.lower()
    
    def process(self, tool_name: str, tool_result: str) -> str:
        import json
        try:
            data = json.loads(tool_result)
            result = data.get("result", {})
            
            return f"""
ğŸŒ¤ï¸ ë‚ ì”¨ ì •ë³´:
ğŸ“ ìœ„ì¹˜: {result.get('location', 'N/A')}
ğŸŒ¡ï¸ ì˜¨ë„: {result.get('temperature', 'N/A')}Â°C
ğŸ’§ ìŠµë„: {result.get('humidity', 'N/A')}%
ğŸŒ¬ï¸ ë°”ëŒ: {result.get('wind_speed', 'N/A')} m/s
â˜ï¸ ìƒíƒœ: {result.get('condition', 'N/A')}
"""
        except:
            return f"ë‚ ì”¨ ì •ë³´ ì²˜ë¦¬ ì‹¤íŒ¨: {tool_result}"
    
    def get_priority(self) -> int:
        return 80

# í”„ë¡œì„¸ì„œ ë“±ë¡ (BaseAgent ì¸ìŠ¤í„´ìŠ¤ì—ì„œ)
agent._get_processor_registry().register(WeatherProcessor())
```

## 8. êµ¬ì¡°í™”ëœ ë¡œê¹…

```python
from application.llm.utils.logging_utils import get_llm_logger, LogLevel

logger = get_llm_logger(__name__)

# ì¼ë°˜ ë¡œê·¸
logger.info("ì—ì´ì „íŠ¸ ì‹œì‘", context={"user_id": "123", "session_id": "abc"})

# Agent í™œë™ ë¡œê·¸
logger.log_agent_activity(
    agent_type="ReactAgent",
    operation="agent_workflow", 
    message="ì—ì´ì „íŠ¸ ì›Œí¬í”Œë¡œìš° ì™„ë£Œ",
    duration=2.5,
    success=True
)

# MCP ì´ë²¤íŠ¸ ë¡œê·¸
logger.log_mcp_event(
    server_name="search",
    operation="tool_call",
    message="ê²€ìƒ‰ ë„êµ¬ í˜¸ì¶œ",
    tool_name="search_web"
)

# ì›Œí¬í”Œë¡œìš° ë¡œê·¸
logger.log_workflow_event(
    workflow_name="agent",
    step="data_collection",
    message="ì •ë³´ ìˆ˜ì§‘ ë‹¨ê³„ ì™„ë£Œ"
)
```

## ğŸš€ CLIì—ì„œ ì—ì´ì „íŠ¸ ì›Œí¬í”Œë¡œìš° ì‚¬ìš©

```bash
# CLI ì‹¤í–‰
python dspilot_cli.py

# ë³µí•© ì‘ì—… ìš”ì²­ ì˜ˆì‹œ
ğŸ‘¤ You: ì˜¤ëŠ˜ ì£¼ìš” ë‰´ìŠ¤ 3ê±´ì„ news.jsonìœ¼ë¡œ ì €ì¥í•´ì¤˜

# ìë™ìœ¼ë¡œ ë‹¤ìŒ ë‹¨ê³„ê°€ ì‹¤í–‰ë©ë‹ˆë‹¤:
# ğŸ”„ ì›Œí¬í”Œë¡œìš° ì‹¤í–‰ ì‹œì‘ (3ë‹¨ê³„)
# âœ… ì›¹ì—ì„œ ë‰´ìŠ¤ ê²€ìƒ‰
# âœ… ë‰´ìŠ¤ ë°ì´í„° ì •ì œ ë° êµ¬ì¡°í™”  
# âœ… JSON íŒŒì¼ë¡œ ì €ì¥
# ğŸ¯ ì›Œí¬í”Œë¡œìš° ì™„ë£Œ: 3/3ë‹¨ê³„ ì„±ê³µ
```

ì´ ì˜ˆì‹œë“¤ì„ ì°¸ê³ í•˜ì—¬ ìƒˆë¡œìš´ ì—ì´ì „íŠ¸ ì›Œí¬í”Œë¡œìš°ë¥¼ íš¨ê³¼ì ìœ¼ë¡œ í™œìš©í•˜ì„¸ìš”! ğŸš€
