# LLM ëª¨ë“ˆ ì‚¬ìš© ì˜ˆì‹œ

## 1. ê¸°ë³¸ ì—ì´ì „íŠ¸ ì‚¬ìš©

```python
from application.llm import AgentFactory

# ì—ì´ì „íŠ¸ ìƒì„±
agent = AgentFactory.create_agent(config_manager)

# ê¸°ë³¸ ì±„íŒ…
response = await agent.generate_response("ì•ˆë…•í•˜ì„¸ìš”!")
print(response["response"])

# ìŠ¤íŠ¸ë¦¬ë° ì±„íŒ…
def streaming_callback(chunk):
    print(chunk, end="", flush=True)

response = await agent.generate_response("ìŠ¤íŠ¸ë¦¬ë° í…ŒìŠ¤íŠ¸", streaming_callback)
```

## 2. ì„¤ì • ê²€ì¦

```python
from application.llm.validators import LLMConfigValidator, MCPConfigValidator
from application.llm.models import LLMConfig, MCPConfig

# LLM ì„¤ì • ê²€ì¦
try:
    config = LLMConfig(api_key="test", model="gpt-4o-mini", temperature=0.7)
    LLMConfigValidator.validate_config(config)
    print("âœ… LLM ì„¤ì • ìœ íš¨")
except Exception as e:
    print(f"âŒ LLM ì„¤ì • ì˜¤ë¥˜: {e}")

# MCP ì„¤ì • ê²€ì¦
try:
    mcp_config = MCPConfig.from_dict({
        "enabled": True,
        "mcp_servers": {
            "search": {"command": "uvx", "args": ["mcp-server-duckduckgo"]}
        }
    })
    MCPConfigValidator.validate_config(mcp_config)
    print("âœ… MCP ì„¤ì • ìœ íš¨")
except Exception as e:
    print(f"âŒ MCP ì„¤ì • ì˜¤ë¥˜: {e}")
```

## 3. ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§

```python
from application.llm.monitoring import get_metrics, PerformanceTracker

# ì„±ëŠ¥ ì¶”ì 
async def example_with_tracking():
    tracker = PerformanceTracker(
        operation_name="example_operation",
        agent_type="BasicAgent", 
        model="gpt-4o-mini",
        track_metrics=True
    )
    
    async with tracker.atrack():
        # ì‹¤ì œ ì‘ì—… ìˆ˜í–‰
        result = await agent.generate_response("í…ŒìŠ¤íŠ¸ ë©”ì‹œì§€")
        return result

# ë©”íŠ¸ë¦­ìŠ¤ ì¡°íšŒ
metrics = get_metrics()
summary = metrics.get_summary_report()
print(f"ì´ ìš”ì²­: {summary['total_requests']}")
print(f"ì„±ê³µë¥ : {summary['success_rate']:.2%}")
print(f"í‰ê·  ì‘ë‹µ ì‹œê°„: {summary['average_response_time']:.2f}ì´ˆ")
```

## 4. ì›Œí¬í”Œë¡œìš° ì‚¬ìš©

```python
from application.llm.workflow import get_workflow

# ì—°êµ¬ ì›Œí¬í”Œë¡œìš°
research_workflow = get_workflow("research")()
result = await research_workflow.run(agent, "AI ê¸°ìˆ  ë™í–¥ ë¶„ì„")

# ë¬¸ì œ í•´ê²° ì›Œí¬í”Œë¡œìš°  
problem_workflow = get_workflow("problem_solving")()
result = await problem_workflow.run(agent, "ì„œë²„ ì„±ëŠ¥ ì´ìŠˆ í•´ê²°")

# ë‹¤ë‹¨ê³„ ì›Œí¬í”Œë¡œìš°
multi_step_workflow = get_workflow("multi_step")()
result = await multi_step_workflow.run(agent, "ë³µì¡í•œ í”„ë¡œì íŠ¸ ê³„íš ìˆ˜ë¦½")
```

## 5. ì»¤ìŠ¤í…€ ì›Œí¬í”Œë¡œìš°

```python
from application.llm.workflow import BaseWorkflow, register_workflow

class CodeReviewWorkflow(BaseWorkflow):
    async def run(self, agent, message, streaming_callback=None):
        # 1ë‹¨ê³„: ì½”ë“œ ë¶„ì„
        analysis_prompt = f"ë‹¤ìŒ ì½”ë“œë¥¼ ë¶„ì„í•´ì£¼ì„¸ìš”:\n{message}"
        analysis = await agent._generate_basic_response(analysis_prompt)
        
        # 2ë‹¨ê³„: ê°œì„ ì  ë„ì¶œ
        improvement_prompt = f"ë¶„ì„ ê²°ê³¼:\n{analysis}\n\nê°œì„ ì ì„ ì œì•ˆí•´ì£¼ì„¸ìš”."
        improvements = await agent._generate_basic_response(improvement_prompt)
        
        return f"## ì½”ë“œ ë¶„ì„\n{analysis}\n\n## ê°œì„  ì œì•ˆ\n{improvements}"

# ì›Œí¬í”Œë¡œìš° ë“±ë¡ ë° ì‚¬ìš©
register_workflow("code_review", CodeReviewWorkflow)
workflow = get_workflow("code_review")()
result = await workflow.run(agent, "def hello(): print('world')")
```

## 6. MCP ë„êµ¬ í™œìš©

```python
from application.llm.mcp import MCPManager, MCPToolManager

# MCP ì´ˆê¸°í™”
mcp_manager = MCPManager(config_manager)
mcp_tool_manager = MCPToolManager(mcp_manager, config_manager)

await mcp_tool_manager.initialize()

# ì‚¬ìš© ê°€ëŠ¥í•œ ë„êµ¬ í™•ì¸
tools = await mcp_tool_manager.get_langchain_tools()
print(f"ì‚¬ìš© ê°€ëŠ¥í•œ ë„êµ¬: {[tool.name for tool in tools]}")

# ë„êµ¬ ì§ì ‘ í˜¸ì¶œ
result = await mcp_tool_manager.call_mcp_tool("search_web", {"query": "Python"})
print(result)
```

## 7. ì»¤ìŠ¤í…€ í”„ë¡œì„¸ì„œ

```python
from application.llm.processors import ToolResultProcessor

class WeatherProcessor(ToolResultProcessor):
    def can_process(self, tool_name: str) -> bool:
        return "weather" in tool_name.lower()
    
    def process(self, tool_name: str, tool_result: str) -> str:
        import json
        try:
            data = json.loads(tool_result)
            result = data.get("result", {})
            
            return f"""
ğŸŒ¤ï¸ ë‚ ì”¨ ì •ë³´:
ğŸ“ ìœ„ì¹˜: {result.get('location', 'N/A')}
ğŸŒ¡ï¸ ì˜¨ë„: {result.get('temperature', 'N/A')}Â°C
ğŸ’§ ìŠµë„: {result.get('humidity', 'N/A')}%
ğŸŒ¬ï¸ ë°”ëŒ: {result.get('wind_speed', 'N/A')} m/s
â˜ï¸ ìƒíƒœ: {result.get('condition', 'N/A')}
"""
        except:
            return f"ë‚ ì”¨ ì •ë³´ ì²˜ë¦¬ ì‹¤íŒ¨: {tool_result}"
    
    def get_priority(self) -> int:
        return 80

# í”„ë¡œì„¸ì„œ ë“±ë¡ (BaseAgent ì¸ìŠ¤í„´ìŠ¤ì—ì„œ)
agent._get_processor_registry().register(WeatherProcessor())
```

## 8. êµ¬ì¡°í™”ëœ ë¡œê¹…

```python
from application.llm.utils.logging_utils import get_llm_logger, LogLevel

logger = get_llm_logger(__name__)

# ì¼ë°˜ ë¡œê·¸
logger.info("ì—ì´ì „íŠ¸ ì‹œì‘", context={"user_id": "123", "session_id": "abc"})

# Agent í™œë™ ë¡œê·¸
logger.log_agent_activity(
    agent_type="ReactAgent",
    operation="generate_response", 
    message="ì‘ë‹µ ìƒì„± ì™„ë£Œ",
    duration=2.5,
    success=True
)

# MCP ì´ë²¤íŠ¸ ë¡œê·¸
logger.log_mcp_event(
    server_name="search",
    operation="tool_call",
    message="ê²€ìƒ‰ ë„êµ¬ í˜¸ì¶œ",
    tool_name="search_web"
)

# ì›Œí¬í”Œë¡œìš° ë¡œê·¸
logger.log_workflow_event(
    workflow_name="research",
    step="data_collection",
    message="ì •ë³´ ìˆ˜ì§‘ ë‹¨ê³„ ì™„ë£Œ"
)
```

ì´ ì˜ˆì‹œë“¤ì„ ì°¸ê³ í•˜ì—¬ LLM ëª¨ë“ˆì„ íš¨ê³¼ì ìœ¼ë¡œ í™œìš©í•˜ì„¸ìš”! ğŸš€
