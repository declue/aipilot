"""
Langchain ê¸°ë°˜ LLM ì—ì´ì „íŠ¸ - create_react_agent ì‚¬ìš© (OpenAI Compatible)
"""

import logging
import uuid
from typing import Any, Callable, Dict, List, Optional

from langchain_core.messages import HumanMessage
from langchain_core.runnables import RunnableConfig
from langchain_openai import ChatOpenAI

# langgraph importë¥¼ try-catchë¡œ ì²˜ë¦¬
try:
    from langgraph.checkpoint.memory import MemorySaver
    from langgraph.prebuilt import create_react_agent
    LANGGRAPH_AVAILABLE = True
    CreateReactAgentFunc: Optional[Any] = create_react_agent
    MemorySaverClass: Optional[Any] = MemorySaver
except ImportError:
    LANGGRAPH_AVAILABLE = False
    CreateReactAgentFunc = None
    MemorySaverClass = None

from application.llm.interfaces.llm_interface import LLMInterface
from application.llm.models.llm_config import LLMConfig
from application.llm.services.conversation_service import ConversationService
from application.llm.services.llm_service import LLMService
from application.llm.workflow.workflow_utils import get_workflow
from application.util.logger import setup_logger

logger = setup_logger("llm_agent") or logging.getLogger("llm_agent")

if not LANGGRAPH_AVAILABLE:
    logger.warning("langgraphë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ReAct ëª¨ë“œê°€ ë¹„í™œì„±í™”ë©ë‹ˆë‹¤.")


class LLMAgent(LLMInterface):
    def __init__(self, config_manager: Any, mcp_tool_manager: Optional[Any] = None) -> None:
        self.config_manager = config_manager
        self.mcp_tool_manager = mcp_tool_manager
        
        # ì„¤ì • ë¡œë“œ
        self._load_config()
        
        # ì„œë¹„ìŠ¤ ì´ˆê¸°í™”
        self.llm_service = LLMService(self.llm_config)
        self.conversation_service = ConversationService()
        
        # ReAct ì—ì´ì „íŠ¸ ê´€ë ¨ (langgraphê°€ ì‚¬ìš© ê°€ëŠ¥í•œ ê²½ìš°ë§Œ)
        self.react_agent: Optional[Any] = None
        self.checkpointer: Optional[Any] = None
        if LANGGRAPH_AVAILABLE and MemorySaverClass is not None:
            self.checkpointer = MemorySaverClass()
        
        self.thread_id = str(uuid.uuid4())
        
        # íˆìŠ¤í† ë¦¬ (í•˜ìœ„ í˜¸í™˜ì„±)
        self.history: List[Dict[str, str]] = []
        
        logger.info("LLM ì—ì´ì „íŠ¸ ì´ˆê¸°í™” ì™„ë£Œ")

    def _load_config(self) -> None:
        """ì„¤ì • ë¡œë“œ"""
        try:
            # í”„ë¡œí•„ ê¸°ë°˜ ì„¤ì • ë¡œë“œ (modeì™€ workflow í¬í•¨)
            llm_config_dict = self.config_manager.get_llm_config()
            
            self.llm_config = LLMConfig.from_dict(llm_config_dict)
            logger.debug(f"LLM ì„¤ì • ë¡œë“œ ì™„ë£Œ: {self.llm_config.model}, ëª¨ë“œ: {self.llm_config.mode}")
            
        except Exception as e:
            logger.error(f"ì„¤ì • ë¡œë“œ ì‹¤íŒ¨: {e}")
            # ê¸°ë³¸ ì„¤ì •ìœ¼ë¡œ í´ë°± - ëª¨ë“  í•„ìˆ˜ í•„ë“œ í¬í•¨
            self.llm_config = LLMConfig(
                api_key="",
                base_url=None,
                model="gpt-4o-mini",
                max_tokens=1000,
                temperature=0.7,
                streaming=True,
                mode="basic",
                workflow=None
            )

    async def generate_response(
        self,
        user_message: str,
        streaming_callback: Optional[Callable[[str], None]] = None,
    ) -> Dict[str, Any]:
        try:
            logger.info(f"LLM ì‘ë‹µ ìƒì„± ì‹œì‘: {user_message[:50]}...")
            
            # ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€
            self.add_user_message(user_message)
            
            # ëª¨ë“œì— ë”°ë¥¸ ì²˜ë¦¬
            mode = self._get_llm_mode()
            logger.info(f"ğŸ¯ í˜„ì¬ LLM ëª¨ë“œ: {mode}")
            logger.info(f"ğŸ”§ MCP ë„êµ¬ ê´€ë¦¬ì ì¡´ì¬: {self.mcp_tool_manager is not None}")
            logger.info(f"ğŸš€ LANGGRAPH ì‚¬ìš© ê°€ëŠ¥: {LANGGRAPH_AVAILABLE}")
            
            if mode == "workflow":
                logger.info("ğŸ“‹ ì›Œí¬í”Œë¡œìš° ëª¨ë“œë¡œ ì²˜ë¦¬")
                return await self._handle_workflow_mode(user_message, streaming_callback)
            elif mode == "mcp_tools" and self.mcp_tool_manager and LANGGRAPH_AVAILABLE:
                logger.info("ğŸ¤– ReAct ì—ì´ì „íŠ¸ ëª¨ë“œë¡œ ì²˜ë¦¬")
                return await self._handle_react_agent_mode(user_message, streaming_callback)
            else:
                logger.info("âš¡ ê¸°ë³¸ ëª¨ë“œë¡œ ì²˜ë¦¬")
                return await self._handle_basic_mode(user_message, streaming_callback)                
        except Exception as e:
            logger.error(f"ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
            import traceback
            logger.error(f"ì‘ë‹µ ìƒì„± ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()}")
            return self._create_error_response("ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤", str(e))

    async def generate_response_streaming(
        self,
        user_message: str,
        streaming_callback: Optional[Callable[[str], None]] = None,
    ) -> Dict[str, Any]:
        """ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ìƒì„± (í•˜ìœ„ í˜¸í™˜ì„±)"""
        return await self.generate_response(user_message, streaming_callback)

    async def _handle_basic_mode(
        self,
        user_message: str,
        streaming_callback: Optional[Callable[[str], None]] = None,
    ) -> Dict[str, Any]:
        """ê¸°ë³¸ ëª¨ë“œ ì²˜ë¦¬"""
        try:
            logger.debug("ê¸°ë³¸ ëª¨ë“œë¡œ ì‘ë‹µ ìƒì„± ì¤‘...")
            response = await self._generate_basic_response(user_message, streaming_callback)
            logger.debug(f"ê¸°ë³¸ ì‘ë‹µ ìƒì„± ì™„ë£Œ: {len(response)} ë¬¸ì")
            return self._create_response_data(response)
        except Exception as e:
            logger.error(f"ê¸°ë³¸ ëª¨ë“œ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
            return self._create_error_response("ê¸°ë³¸ ëª¨ë“œ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤", str(e))

    async def _handle_workflow_mode(
        self,
        user_message: str,
        streaming_callback: Optional[Callable[[str], None]] = None,
    ) -> Dict[str, Any]:
        """ì›Œí¬í”Œë¡œìš° ëª¨ë“œ ì²˜ë¦¬"""
        try:
            workflow_name = self.llm_config.workflow or "basic_chat"
            workflow_class = get_workflow(workflow_name)
            workflow = workflow_class()
            
            result = await workflow.run(self, user_message, streaming_callback)
            
            return {
                "response": result,
                "workflow": workflow_name,
                "reasoning": "",
                "used_tools": []
            }
            
        except Exception as e:
            logger.error(f"ì›Œí¬í”Œë¡œìš° ëª¨ë“œ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
            return {
                "response": "ì›Œí¬í”Œë¡œìš° ì²˜ë¦¬ ì¤‘ ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.",
                "workflow": self.llm_config.workflow or "basic_chat",
                "reasoning": str(e),
                "used_tools": []
            }

    async def _handle_react_agent_mode(
        self,
        user_message: str,
        streaming_callback: Optional[Callable[[str], None]] = None,
    ) -> Dict[str, Any]:
        """ReAct ì—ì´ì „íŠ¸ ëª¨ë“œ ì²˜ë¦¬"""
        try:
            logger.info("ğŸ¤– ReAct ì—ì´ì „íŠ¸ ëª¨ë“œë¡œ ì‘ë‹µ ìƒì„± ì¤‘...")
            
            if not LANGGRAPH_AVAILABLE:
                logger.error("âŒ langgraphê°€ ì‚¬ìš©í•  ìˆ˜ ì—†ìŒ")
                logger.info("ğŸ”„ ê¸°ë³¸ ëª¨ë“œë¡œ í´ë°±")
                return await self._handle_basic_mode(user_message, streaming_callback)
            
            if not self.mcp_tool_manager:
                logger.error("âŒ MCP ë„êµ¬ ê´€ë¦¬ìê°€ ì—†ìŒ")
                logger.info("ğŸ”„ ê¸°ë³¸ ëª¨ë“œë¡œ í´ë°±")
                return await self._handle_basic_mode(user_message, streaming_callback)
            
            # ReAct ì—ì´ì „íŠ¸ ì´ˆê¸°í™” (í•„ìš”í•œ ê²½ìš°)
            if not self.react_agent:
                logger.info("ğŸ”„ ReAct ì—ì´ì „íŠ¸ ì´ˆê¸°í™” ì‹œì‘...")
                init_success = await self._initialize_react_agent()
                if not init_success:
                    logger.error("âŒ ReAct ì—ì´ì „íŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨")
                    logger.info("ğŸ”„ ê¸°ë³¸ ëª¨ë“œë¡œ í´ë°±")
                    return await self._handle_basic_mode(user_message, streaming_callback)
                else:
                    logger.info("âœ… ReAct ì—ì´ì „íŠ¸ ì´ˆê¸°í™” ì„±ê³µ")
            else:
                logger.info("âœ… ReAct ì—ì´ì „íŠ¸ ì´ë¯¸ ì´ˆê¸°í™”ë¨")
            
            # ReAct ì—ì´ì „íŠ¸ë¥¼ ì‚¬ìš©í•œ ì‘ë‹µ ìƒì„±
            logger.info("ğŸš€ ReAct ì—ì´ì „íŠ¸ ì‹¤í–‰ ì‹œì‘...")
            result = await self._run_react_agent(user_message, streaming_callback)
            
            logger.info(f"âœ… ReAct ì—ì´ì „íŠ¸ ì‘ë‹µ ì™„ë£Œ: {len(result.get('response', ''))} ë¬¸ì")
            
            return {
                "response": result.get("response", ""),
                "reasoning": "ReAct ì—ì´ì „íŠ¸ë¥¼ ì‚¬ìš©í•œ ì‘ë‹µ",
                "used_tools": result.get("used_tools", [])
            }
            
        except Exception as e:
            logger.error(f"ReAct ì—ì´ì „íŠ¸ ëª¨ë“œ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
            import traceback
            logger.error(f"ReAct ì—ì´ì „íŠ¸ ëª¨ë“œ ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()}")
            logger.info("ğŸ”„ ê¸°ë³¸ ëª¨ë“œë¡œ í´ë°±")
            return await self._handle_basic_mode(user_message, streaming_callback)

    async def _initialize_react_agent(self) -> bool:
        """ReAct ì—ì´ì „íŠ¸ ì´ˆê¸°í™”"""
        try:
            if not LANGGRAPH_AVAILABLE:
                logger.error("langgraphë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
                return False
            
            if not self.mcp_tool_manager:
                logger.error("MCP ë„êµ¬ ê´€ë¦¬ìê°€ ì—†ìŠµë‹ˆë‹¤")
                return False
            
            # MCP ë„êµ¬ ê°€ì ¸ì˜¤ê¸°
            tools = await self.mcp_tool_manager.get_langchain_tools()
            if not tools:
                logger.warning("ì‚¬ìš© ê°€ëŠ¥í•œ MCP ë„êµ¬ê°€ ì—†ìŠµë‹ˆë‹¤")
                return False
            
            # LLM ëª¨ë¸ ì´ˆê¸°í™”
            llm = await self._create_llm_model()
            if not llm:
                logger.error("LLM ëª¨ë¸ ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤")
                return False
            
            # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì„¤ì •
            system_prompt = self._get_system_prompt()
            
            # ReAct ì—ì´ì „íŠ¸ ìƒì„±
            if CreateReactAgentFunc is not None and self.checkpointer:
                self.react_agent = CreateReactAgentFunc(
                    llm,
                    tools,
                    checkpointer=self.checkpointer,
                    prompt=system_prompt
                )
                
                logger.info(f"ReAct ì—ì´ì „íŠ¸ ì´ˆê¸°í™” ì™„ë£Œ: {len(tools)}ê°œ ë„êµ¬")
                return True
            else:
                logger.error("create_react_agent ë˜ëŠ” checkpointerë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
                return False
            
        except Exception as e:
            logger.error(f"ReAct ì—ì´ì „íŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            import traceback
            logger.error(f"ReAct ì—ì´ì „íŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨ ìƒì„¸: {traceback.format_exc()}")
            return False

    async def _create_llm_model(self) -> Optional[ChatOpenAI]:
        """OpenAI Compatible LLM ëª¨ë¸ ìƒì„±"""
        try:
            model_name = str(self.llm_config.model)
            
            # ChatOpenAI ì´ˆê¸°í™” - ëª…ì‹œì  íŒŒë¼ë¯¸í„° ì „ë‹¬
            openai_params = {
                "model": model_name,
                "temperature": float(self.llm_config.temperature),
            }
            
            # API í‚¤ ì„¤ì •
            if self.llm_config.api_key:
                openai_params["api_key"] = str(self.llm_config.api_key)
            
            # base_urlì´ ìˆìœ¼ë©´ ì¶”ê°€
            if self.llm_config.base_url:
                openai_params["base_url"] = str(self.llm_config.base_url)
            
            # streaming ì„¤ì •
            if hasattr(self.llm_config, 'streaming') and self.llm_config.streaming is not None:
                openai_params["streaming"] = bool(self.llm_config.streaming)
            
            logger.debug(f"ChatOpenAI ì´ˆê¸°í™” íŒŒë¼ë¯¸í„°: model={model_name}, base_url={openai_params.get('base_url', 'None')}")
            
            # ëª…ì‹œì  ìƒì„±ì í˜¸ì¶œ
            return ChatOpenAI(
                model=str(openai_params["model"]),
                temperature=float(openai_params["temperature"]),
                api_key=str(openai_params["api_key"]) if openai_params.get("api_key") else None,
                base_url=str(openai_params["base_url"]) if openai_params.get("base_url") else None,
                streaming=bool(openai_params.get("streaming", True))
            )
                
        except Exception as e:
            logger.error(f"LLM ëª¨ë¸ ìƒì„± ì‹¤íŒ¨: {e}")
            import traceback
            logger.error(f"LLM ëª¨ë¸ ìƒì„± ì‹¤íŒ¨ ìƒì„¸: {traceback.format_exc()}")
            return None

    def _get_system_prompt(self) -> str:
        """ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ë°˜í™˜"""
        return """<ROLE>
You are a smart agent with an ability to use tools. 
You will be given a question and you will use the tools to answer the question.
Pick the most relevant tool to answer the question. 
If you are failed to answer the question, try different tools to get context.
Your answer should be very polite and professional.
</ROLE>

<INSTRUCTIONS>
Step 1: Analyze the question
- Analyze user's question and final goal.
- If the user's question is consist of multiple sub-questions, split them into smaller sub-questions.

Step 2: Pick the most relevant tool
- Pick the most relevant tool to answer the question.
- If you are failed to answer the question, try different tools to get context.

Step 3: Answer the question
- Answer the question in Korean.
- Your answer should be very polite and professional.

Step 4: Provide the source of the answer(if applicable)
- If you've used the tool, provide the source of the answer.
- Valid sources are either a website(URL) or a document(PDF, etc).

Guidelines:
- If you've used the tool, your answer should be based on the tool's output(tool's output is more important than your own knowledge).
- Answer in Korean.
- Answer should be concise and to the point.
</INSTRUCTIONS>
"""

    async def _run_react_agent(
        self,
        user_message: str,
        streaming_callback: Optional[Callable[[str], None]] = None,
    ) -> Dict[str, Any]:
        """ReAct ì—ì´ì „íŠ¸ ì‹¤í–‰"""
        try:
            logger.info(f"ğŸ”¥ ReAct ì—ì´ì „íŠ¸ ì‹¤í–‰ ë©”ì„œë“œ ì§„ì…: {user_message[:50]}...")
            
            if not self.react_agent:
                logger.error("âŒ ReAct ì—ì´ì „íŠ¸ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•ŠìŒ")
                return {"response": "ReAct ì—ì´ì „íŠ¸ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.", "used_tools": []}
            
            logger.info("âœ… ReAct ì—ì´ì „íŠ¸ í™•ì¸ ì™„ë£Œ")
            
            # ì„¤ì • ìƒì„± (configurable ì‚¬ìš©)
            config = RunnableConfig(
                recursion_limit=100,
                configurable={"thread_id": self.thread_id}
            )
            logger.info(f"âœ… ì„¤ì • ìƒì„± ì™„ë£Œ: thread_id={self.thread_id}")
            
            # ë©”ì‹œì§€ ìƒì„±
            messages = [HumanMessage(content=user_message)]
            inputs = {"messages": messages}
            logger.info(f"âœ… ì…ë ¥ ë©”ì‹œì§€ ìƒì„± ì™„ë£Œ: {len(messages)}ê°œ ë©”ì‹œì§€")
            
            # ìŠ¤íŠ¸ë¦¬ë°ì´ í™œì„±í™”ëœ ê²½ìš°
            if streaming_callback is not None:
                logger.info("ğŸ¬ ìŠ¤íŠ¸ë¦¬ë° ëª¨ë“œë¡œ ì‹¤í–‰")
                accumulated_response = ""
                used_tools: List[str] = []
                tool_results: Dict[str, str] = {}
                
                try:
                    # astream_graph í™œìš©í•˜ì—¬ ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬
                    from application.llm.workflow.workflow_utils import astream_graph
                    logger.info("âœ… astream_graph import ì„±ê³µ")
                    
                    def streaming_wrapper(chunk: Dict[str, Any]) -> None:
                        nonlocal accumulated_response, used_tools, tool_results
                        
                        logger.info(f"ğŸ“¦ ìŠ¤íŠ¸ë¦¬ë° ì²­í¬ ìˆ˜ì‹ : {type(chunk)}, keys={list(chunk.keys()) if isinstance(chunk, dict) else 'N/A'}")
                        
                        # langgraph ReAct ì—ì´ì „íŠ¸ì˜ ì‹¤ì œ ì‘ë‹µ êµ¬ì¡° ì²˜ë¦¬
                        if isinstance(chunk, dict):
                            # ì§ì ‘ messages í‚¤ê°€ ìˆëŠ” ê²½ìš°
                            if "messages" in chunk:
                                logger.info(f"ğŸ“¬ ì§ì ‘ ë©”ì‹œì§€ ì²­í¬ ì²˜ë¦¬: {len(chunk['messages'])}ê°œ ë©”ì‹œì§€")
                                for i, message in enumerate(chunk["messages"]):
                                    logger.info(f"ğŸ“¨ ì§ì ‘ ë©”ì‹œì§€ {i}: type={type(message)}, hasattr(content)={hasattr(message, 'content')}")
                                    # AIMessageë§Œ ì‚¬ìš©ìì—ê²Œ ì „ë‹¬ (ë„êµ¬ ê²°ê³¼ëŠ” ì œì™¸)
                                    if hasattr(message, 'content') and message.content and str(type(message)).find('AIMessage') != -1:
                                        content = str(message.content)
                                        accumulated_response += content
                                        logger.info(f"ğŸ“ ì§ì ‘ AI ì»¨í…ì¸  ì¶”ê°€: {len(content)}ì, ëˆ„ì ={len(accumulated_response)}ì")
                                        try:
                                            if streaming_callback is not None:
                                                streaming_callback(content)
                                                logger.info(f"âœ… ì§ì ‘ AI ìŠ¤íŠ¸ë¦¬ë° ì½œë°± ì‹¤í–‰ ì„±ê³µ")
                                        except Exception as e:
                                            logger.error(f"âŒ ì§ì ‘ AI ìŠ¤íŠ¸ë¦¬ë° ì½œë°± ì‹¤í–‰ ì˜¤ë¥˜: {e}")
                                    
                                    # ë„êµ¬ ì‚¬ìš© ì¶”ì  (ë¡œê¹…ìš©)
                                    if hasattr(message, 'tool_calls') and message.tool_calls:
                                        for tool_call in message.tool_calls:
                                            tool_name = tool_call.get('name', 'unknown_tool') if isinstance(tool_call, dict) else getattr(tool_call, 'name', 'unknown_tool')
                                            used_tools.append(tool_name)
                                            logger.info(f"ğŸ”§ ì§ì ‘ ë„êµ¬ ì‚¬ìš©: {tool_name}")
                            
                            # agent í‚¤ ì•ˆì— messagesê°€ ìˆëŠ” ê²½ìš° (ReAct ì—ì´ì „íŠ¸ êµ¬ì¡°)
                            elif "agent" in chunk and isinstance(chunk["agent"], dict) and "messages" in chunk["agent"]:
                                logger.info(f"ğŸ“¬ ì—ì´ì „íŠ¸ ë©”ì‹œì§€ ì²­í¬ ì²˜ë¦¬: {len(chunk['agent']['messages'])}ê°œ ë©”ì‹œì§€")
                                for i, message in enumerate(chunk["agent"]["messages"]):
                                    logger.info(f"ğŸ“¨ ì—ì´ì „íŠ¸ ë©”ì‹œì§€ {i}: type={type(message)}, hasattr(content)={hasattr(message, 'content')}")
                                    # AIMessageë§Œ ì‚¬ìš©ìì—ê²Œ ì „ë‹¬ (ë„êµ¬ ê²°ê³¼ëŠ” ì œì™¸)
                                    if hasattr(message, 'content') and message.content and str(type(message)).find('AIMessage') != -1:
                                        content = str(message.content)
                                        accumulated_response += content
                                        logger.info(f"ğŸ“ ì—ì´ì „íŠ¸ AI ì»¨í…ì¸  ì¶”ê°€: {len(content)}ì, ëˆ„ì ={len(accumulated_response)}ì")
                                        try:
                                            if streaming_callback is not None:
                                                streaming_callback(content)
                                                logger.info(f"âœ… ì—ì´ì „íŠ¸ AI ìŠ¤íŠ¸ë¦¬ë° ì½œë°± ì‹¤í–‰ ì„±ê³µ")
                                        except Exception as e:
                                            logger.error(f"âŒ ì—ì´ì „íŠ¸ AI ìŠ¤íŠ¸ë¦¬ë° ì½œë°± ì‹¤í–‰ ì˜¤ë¥˜: {e}")
                                    
                                    # ë„êµ¬ ì‚¬ìš© ì¶”ì  (ë¡œê¹…ìš©)
                                    if hasattr(message, 'tool_calls') and message.tool_calls:
                                        for tool_call in message.tool_calls:
                                            tool_name = tool_call.get('name', 'unknown_tool') if isinstance(tool_call, dict) else getattr(tool_call, 'name', 'unknown_tool')
                                            used_tools.append(tool_name)
                                            logger.info(f"ğŸ”§ ì—ì´ì „íŠ¸ ë„êµ¬ ì‚¬ìš©: {tool_name}")
                            
                            # tools í‚¤ ì•ˆì— messagesê°€ ìˆëŠ” ê²½ìš° (ë„êµ¬ ê²°ê³¼ - ì‚¬ìš©ìì—ê²Œ ì§ì ‘ ì „ë‹¬í•˜ì§€ ì•ŠìŒ)
                            elif "tools" in chunk and isinstance(chunk["tools"], dict) and "messages" in chunk["tools"]:
                                logger.info(f"ğŸ”§ ë„êµ¬ ê²°ê³¼ ì²­í¬ ì²˜ë¦¬: {len(chunk['tools']['messages'])}ê°œ ë©”ì‹œì§€ (ì‚¬ìš©ìì—ê²Œ ì§ì ‘ ì „ë‹¬í•˜ì§€ ì•ŠìŒ)")
                                # ë„êµ¬ ê²°ê³¼ëŠ” ë¡œê¹…ë§Œ í•˜ê³  ì‚¬ìš©ìì—ê²Œ ì§ì ‘ ì „ë‹¬í•˜ì§€ ì•ŠìŒ
                                for i, message in enumerate(chunk["tools"]["messages"]):
                                    logger.info(f"ğŸ”§ ë„êµ¬ ê²°ê³¼ {i}: type={type(message)}, content={getattr(message, 'content', 'No content')[:100]}...")
                                    
                                    # ë„êµ¬ ì‚¬ìš© ì¶”ì 
                                    if hasattr(message, 'name') and message.name:
                                        tool_name = str(message.name)
                                        if tool_name not in used_tools:
                                            used_tools.append(tool_name)
                                            logger.info(f"ğŸ”§ ë„êµ¬ ì‚¬ìš© ê¸°ë¡: {tool_name}")
                                        # ë©”ì‹œì§€ content ì €ì¥ (JSON ë¬¸ìì—´ì¼ ìˆ˜ ìˆìŒ)
                                        if hasattr(message, 'content') and message.content:
                                            tool_results[tool_name] = str(message.content)
                            
                            # ë‹¤ë¥¸ êµ¬ì¡°ì˜ ì²­í¬ë“¤
                            else:
                                logger.info(f"ğŸ“¦ ë‹¤ë¥¸ êµ¬ì¡°ì˜ ì²­í¬: {list(chunk.keys()) if isinstance(chunk, dict) else str(chunk)[:100]}")
                                # ì—ëŸ¬ ì²­í¬ëŠ” ê±´ë„ˆë›°ê¸°
                                if isinstance(chunk, dict) and "error" in chunk and "type" in chunk:
                                    logger.warning(f"âš ï¸ ì—ëŸ¬ ì²­í¬ ê±´ë„ˆë›°ê¸°: {str(chunk.get('error', ''))[:100]}...")
                                    return
                                
                                # í˜¹ì‹œ ë‹¤ë¥¸ êµ¬ì¡°ë¡œ AIMessageê°€ ìˆëŠ”ì§€ ì²´í¬
                                if isinstance(chunk, dict):
                                    for key, value in chunk.items():
                                        if isinstance(value, dict) and "messages" in value:
                                            logger.info(f"ğŸ“¬ {key} ì•ˆì—ì„œ ë©”ì‹œì§€ ë°œê²¬: {len(value['messages'])}ê°œ")
                                            for i, message in enumerate(value["messages"]):
                                                # AIMessageë§Œ ì²˜ë¦¬
                                                if hasattr(message, 'content') and message.content and str(type(message)).find('AIMessage') != -1:
                                                    content = str(message.content)
                                                    accumulated_response += content
                                                    logger.info(f"ğŸ“ {key} AI ì»¨í…ì¸  ì¶”ê°€: {len(content)}ì, ëˆ„ì ={len(accumulated_response)}ì")
                                                    try:
                                                        if streaming_callback is not None:
                                                            streaming_callback(content)
                                                            logger.info(f"âœ… {key} AI ìŠ¤íŠ¸ë¦¬ë° ì½œë°± ì‹¤í–‰ ì„±ê³µ")
                                                    except Exception as e:
                                                        logger.error(f"âŒ {key} AI ìŠ¤íŠ¸ë¦¬ë° ì½œë°± ì‹¤í–‰ ì˜¤ë¥˜: {e}")
                        else:
                            logger.info(f"ğŸ“¦ dictê°€ ì•„ë‹Œ ì²­í¬: {type(chunk)} - {str(chunk)[:100]}...")
                    
                    # astream_graphë¥¼ ì‚¬ìš©í•˜ì—¬ ìŠ¤íŠ¸ë¦¬ë° ì‹¤í–‰
                    logger.info("ğŸš€ astream_graph ì‹¤í–‰ ì‹œì‘...")
                    chunk_count = 0
                    final_response_found = False
                    
                    async for chunk_result in astream_graph(
                        self.react_agent,
                        inputs,
                        callback=streaming_wrapper,
                        config=config
                    ):
                        chunk_count += 1
                        logger.info(f"ğŸ“¦ ì²­í¬ {chunk_count} ì™„ë£Œ: {type(chunk_result)}")
                        
                        # ì²­í¬ì—ì„œ ìµœì¢… AI ì‘ë‹µ í™•ì¸
                        if isinstance(chunk_result, dict):
                            # agent í‚¤ì—ì„œ AIMessage í™•ì¸
                            if "agent" in chunk_result and isinstance(chunk_result["agent"], dict) and "messages" in chunk_result["agent"]:
                                for message in chunk_result["agent"]["messages"]:
                                    if hasattr(message, 'content') and message.content and str(type(message)).find('AIMessage') != -1:
                                        final_response_found = True
                                        logger.info(f"âœ… ìµœì¢… AI ì‘ë‹µ ë°œê²¬: {len(str(message.content))}ì")
                                        break
                    
                    logger.info(f"âœ… astream_graph ì™„ë£Œ: {chunk_count}ê°œ ì²­í¬ ì²˜ë¦¬, ìµœì¢…ì‘ë‹µ={final_response_found}")
                    
                    # ìµœì¢… ì‘ë‹µì´ ì—†ê³  ë„êµ¬ë¥¼ ì‚¬ìš©í–ˆë‹¤ë©´ ë¹„ìŠ¤íŠ¸ë¦¬ë° ëª¨ë“œë¡œ ì¬ì‹œë„
                    if not final_response_found and len(used_tools) > 0:
                        logger.warning("âš ï¸ ë„êµ¬ ì‚¬ìš© í›„ ìµœì¢… AI ì‘ë‹µì´ ì—†ìŒ - ë¹„ìŠ¤íŠ¸ë¦¬ë° ëª¨ë“œë¡œ ì¬ì‹œë„")
                        try:
                            # ìƒˆë¡œìš´ thread_idë¡œ ë¹„ìŠ¤íŠ¸ë¦¬ë° ì‹¤í–‰
                            fallback_thread_id = str(uuid.uuid4())
                            fallback_config = RunnableConfig(
                                recursion_limit=100,
                                configurable={"thread_id": fallback_thread_id}
                            )
                            
                            logger.info("ğŸ”„ ë¹„ìŠ¤íŠ¸ë¦¬ë° í´ë°± ì‹¤í–‰...")
                            result = await self.react_agent.ainvoke(inputs, config=fallback_config)
                            
                            if "messages" in result:
                                for message in result["messages"]:
                                    if hasattr(message, 'content') and message.content and str(type(message)).find('AIMessage') != -1:
                                        content = str(message.content)
                                        if content.strip() and content not in accumulated_response:
                                            accumulated_response += content
                                            logger.info(f"ğŸ“ í´ë°± AI ì‘ë‹µ ì¶”ê°€: {len(content)}ì")
                                            if streaming_callback is not None:
                                                streaming_callback(content)
                                            break
                        except Exception as e:
                            logger.error(f"âŒ ë¹„ìŠ¤íŠ¸ë¦¬ë° í´ë°± ì‹¤í–‰ ì‹¤íŒ¨: {e}")
                            
                            # ë„êµ¬ ê²°ê³¼ê°€ ìˆë‹¤ë©´ ì§ì ‘ ìì—°ìŠ¤ëŸ½ê²Œ í¬ë§·íŒ…
                            if len(used_tools) > 0 and not accumulated_response.strip():
                                logger.info("ğŸ”§ ìŠ¤íŠ¸ë¦¬ë° ë„êµ¬ ê²°ê³¼ ì§ì ‘ í¬ë§·íŒ… ì‹œë„...")
                                
                                formatted_response = self._format_tool_results(used_tools, tool_results)
                                accumulated_response = formatted_response
                                logger.info(f"ğŸ”§ ìŠ¤íŠ¸ë¦¬ë° ë„êµ¬ ê²°ê³¼ í¬ë§·íŒ… ì™„ë£Œ: {formatted_response}")
                                if streaming_callback is not None:
                                    streaming_callback(formatted_response)
                    
                    # ë„êµ¬ë¥¼ ì‚¬ìš©í•˜ì§€ ì•Šì•˜ë”ë¼ë„ íŠ¹ì • í‚¤ì›Œë“œê°€ í¬í•¨ëœ ì§ˆë¬¸ì—ì„œ ì‘ë‹µì´ ì—†ë‹¤ë©´ í´ë°±
                    elif not final_response_found and not accumulated_response.strip():
                        user_message_lower = user_message.lower()
                        logger.info(f"ğŸ” í‚¤ì›Œë“œ ê¸°ë°˜ í´ë°± ê²€ì‚¬: '{user_message_lower[:50]}...'")
                        
                        # ì‹œê°„ ê´€ë ¨ í‚¤ì›Œë“œ
                        time_keywords = ["ì‹œê°„", "ëª‡ì‹œ", "time", "í˜„ì¬ì‹œê°„", "ì§€ê¸ˆì‹œê°„"]
                        # ë‚ ì”¨ ê´€ë ¨ í‚¤ì›Œë“œ  
                        weather_keywords = ["ë‚ ì”¨", "weather", "ê¸°ì˜¨", "ì˜¨ë„", "ë¹„", "ëˆˆ", "ë§‘ìŒ", "íë¦¼"]
                        
                        if any(keyword in user_message_lower for keyword in time_keywords):
                            logger.info("ğŸ•’ ì‹œê°„ ê´€ë ¨ ì§ˆë¬¸ ê°ì§€ - ì‹œê°„ í´ë°± ì ìš©")
                            import datetime
                            now = datetime.datetime.now()
                            accumulated_response = f"í˜„ì¬ ì‹œê°„ì€ {now.strftime('%Yë…„ %mì›” %dì¼ %Hì‹œ %Më¶„')}ì…ë‹ˆë‹¤."
                            if streaming_callback is not None:
                                streaming_callback(accumulated_response)
                        
                        elif any(keyword in user_message_lower for keyword in weather_keywords):
                            logger.info("ğŸŒ¤ï¸ ë‚ ì”¨ ê´€ë ¨ ì§ˆë¬¸ ê°ì§€ - ë‚ ì”¨ í´ë°± ì ìš© (ë„êµ¬ ì§ì ‘ í˜¸ì¶œ)")
                            response_text = await self._get_weather_fallback(user_message)
                            if response_text:
                                if not response_text.endswith("ì…ë‹ˆë‹¤."):
                                    response_text += "ì…ë‹ˆë‹¤."
                                if streaming_callback is not None:
                                    streaming_callback(response_text)
                        else:
                            logger.info("ğŸ¤– ì¼ë°˜ ì§ˆë¬¸ - ê¸°ë³¸ í´ë°± ì ìš©")
                            accumulated_response = "ì£„ì†¡í•©ë‹ˆë‹¤. ìš”ì²­ì„ ì²˜ë¦¬í•˜ëŠ” ì¤‘ì— ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì‹œê±°ë‚˜ ë‹¤ë¥¸ ë°©ì‹ìœ¼ë¡œ ì§ˆë¬¸í•´ì£¼ì„¸ìš”."
                            if streaming_callback is not None:
                                streaming_callback(accumulated_response)
                except Exception as e:
                    logger.error(f"âŒ ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                    import traceback
                    logger.error(f"âŒ ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬ ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()}")
                    return {
                        "response": f"ReAct ìŠ¤íŠ¸ë¦¬ë° ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {str(e)}",
                        "used_tools": used_tools
                    }
                
                logger.info(f"âœ… ìŠ¤íŠ¸ë¦¬ë° ì™„ë£Œ: {len(accumulated_response)} ë¬¸ì, {len(used_tools)}ê°œ ë„êµ¬ ì‚¬ìš©")
                
                return {
                    "response": accumulated_response,
                    "used_tools": used_tools
                }
            else:
                # ë¹„ìŠ¤íŠ¸ë¦¬ë° ëª¨ë“œ
                logger.info("ğŸ“ ë¹„ìŠ¤íŠ¸ë¦¬ë° ëª¨ë“œë¡œ ì‹¤í–‰")
                try:
                    logger.info("ğŸš€ ReAct ainvoke ì‹œì‘...")
                    result = await self.react_agent.ainvoke(inputs, config=config)
                    logger.info(f"âœ… ReAct ainvoke ì™„ë£Œ: type={type(result)}, keys={list(result.keys()) if isinstance(result, dict) else 'N/A'}")
                except Exception as e:
                    logger.error(f"âŒ ReAct ainvoke ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
                    import traceback
                    logger.error(f"âŒ ReAct ainvoke ì˜¤ë¥˜ ìƒì„¸: {traceback.format_exc()}")
                    return {
                        "response": f"ReAct ainvoke ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {str(e)}",
                        "used_tools": []
                    }
                
                response_text = ""
                used_tools: List[str] = []
                tool_results: Dict[str, str] = {}
                
                if "messages" in result:
                    logger.info(f"ğŸ“¬ ê²°ê³¼ ë©”ì‹œì§€ ìˆ˜: {len(result['messages'])}")
                    ai_messages = []
                    tool_messages = []
                    
                    # ë©”ì‹œì§€ ë¶„ë¥˜
                    for i, message in enumerate(result["messages"]):
                        logger.info(f"ğŸ“¨ ê²°ê³¼ ë©”ì‹œì§€ {i}: type={type(message)}, hasattr(content)={hasattr(message, 'content')}")
                        
                        if str(type(message)).find('AIMessage') != -1:
                            ai_messages.append(message)
                            logger.info(f"ğŸ¤– AI ë©”ì‹œì§€ ë°œê²¬: {len(str(getattr(message, 'content', '')))}ì")
                        elif str(type(message)).find('ToolMessage') != -1:
                            tool_messages.append(message)
                            logger.info(f"ğŸ”§ ë„êµ¬ ë©”ì‹œì§€ ë°œê²¬: {getattr(message, 'name', 'unknown')}")
                            if hasattr(message, 'name') and message.name and hasattr(message, 'content') and message.content:
                                tool_results[str(message.name)] = str(message.content)
                    
                    # ë„êµ¬ ì‚¬ìš© ì¶”ì 
                    for tool_msg in tool_messages:
                        if hasattr(tool_msg, 'name') and tool_msg.name:
                            tool_name = str(tool_msg.name)
                            if tool_name not in used_tools:
                                used_tools.append(tool_name)
                                logger.info(f"ğŸ”§ ë„êµ¬ ì‚¬ìš© ê¸°ë¡: {tool_name}")
                    
                    # AI ë©”ì‹œì§€ì—ì„œ ìµœì¢… ì‘ë‹µ ì¶”ì¶œ (ë§ˆì§€ë§‰ AI ë©”ì‹œì§€ ìš°ì„ )
                    if ai_messages:
                        # ë§ˆì§€ë§‰ AI ë©”ì‹œì§€ê°€ ìµœì¢… ì‘ë‹µì¼ ê°€ëŠ¥ì„±ì´ ë†’ìŒ
                        for message in reversed(ai_messages):
                            if hasattr(message, 'content') and message.content:
                                content = str(message.content).strip()
                                if content:
                                    response_text = content
                                    logger.info(f"ğŸ“ ìµœì¢… AI ì‘ë‹µ ì„ íƒ: {len(content)}ì")
                                    break
                    
                    # ì‘ë‹µì´ ì—†ê³  ë„êµ¬ë¥¼ ì‚¬ìš©í–ˆë‹¤ë©´ ì¬ì‹œë„
                    if not response_text and used_tools:
                        logger.warning("âš ï¸ ë„êµ¬ ì‚¬ìš© í›„ AI ì‘ë‹µì´ ì—†ìŒ - ìƒˆ ì„¸ì…˜ìœ¼ë¡œ ì¬ì‹œë„")
                        try:
                            # ìƒˆë¡œìš´ thread_idë¡œ ì¬ì‹œë„
                            retry_thread_id = str(uuid.uuid4())
                            retry_config = RunnableConfig(
                                recursion_limit=100,
                                configurable={"thread_id": retry_thread_id}
                            )
                            
                            logger.info("ğŸ”„ ìƒˆ ì„¸ì…˜ìœ¼ë¡œ ì¬ì‹œë„...")
                            retry_result = await self.react_agent.ainvoke(inputs, config=retry_config)
                            
                            if "messages" in retry_result:
                                for message in reversed(retry_result["messages"]):
                                    if hasattr(message, 'content') and message.content and str(type(message)).find('AIMessage') != -1:
                                        content = str(message.content).strip()
                                        if content:
                                            response_text = content
                                            logger.info(f"ğŸ“ ì¬ì‹œë„ ì„±ê³µ: {len(content)}ì")
                                            break
                        except Exception as e:
                            logger.error(f"âŒ ì¬ì‹œë„ ì‹¤íŒ¨: {e}")
                            
                            # ë„êµ¬ ê²°ê³¼ê°€ ìˆë‹¤ë©´ ì§ì ‘ ìì—°ìŠ¤ëŸ½ê²Œ í¬ë§·íŒ…
                            if len(used_tools) > 0 and not response_text.strip():
                                logger.info("ğŸ”§ ë¹„ìŠ¤íŠ¸ë¦¬ë° ë„êµ¬ ê²°ê³¼ ì§ì ‘ í¬ë§·íŒ… ì‹œë„...")
                                
                                response_text = self._format_tool_results(used_tools, tool_results)
                                logger.info(f"ğŸ”§ ë¹„ìŠ¤íŠ¸ë¦¬ë° ë„êµ¬ ê²°ê³¼ í¬ë§·íŒ… ì™„ë£Œ: {response_text}")
                    
                    # ìµœì¢… í™•ì¸: ë„êµ¬ë¥¼ ì‚¬ìš©í–ˆëŠ”ë° ì—¬ì „íˆ ì‘ë‹µì´ ì—†ë‹¤ë©´ í´ë°±
                    if not response_text.strip() and len(used_tools) > 0:
                        logger.warning("âš ï¸ ëª¨ë“  ì‹œë„ í›„ì—ë„ ì‘ë‹µì´ ì—†ìŒ - ìµœì¢… í´ë°± ì ìš©")
                        if "get_current_time" in used_tools:
                            import datetime
                            now = datetime.datetime.now()
                            response_text = f"í˜„ì¬ ì‹œê°„ì€ {now.strftime('%Yë…„ %mì›” %dì¼ %Hì‹œ %Më¶„')}ì…ë‹ˆë‹¤."
                        else:
                            response_text = f"ìš”ì²­í•˜ì‹  ì‘ì—…ì„ ìˆ˜í–‰í–ˆì§€ë§Œ ì‘ë‹µì„ ìƒì„±í•˜ëŠ” ì¤‘ì— ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ì‚¬ìš©ëœ ë„êµ¬: {', '.join(used_tools)}"
                    
                    # ë„êµ¬ë¥¼ ì‚¬ìš©í•˜ì§€ ì•Šì•˜ë”ë¼ë„ íŠ¹ì • í‚¤ì›Œë“œê°€ í¬í•¨ëœ ì§ˆë¬¸ì—ì„œ ì‘ë‹µì´ ì—†ë‹¤ë©´ í´ë°±
                    elif not response_text.strip():
                        user_message_lower = user_message.lower()
                        logger.info(f"ğŸ” ë¹„ìŠ¤íŠ¸ë¦¬ë° í‚¤ì›Œë“œ ê¸°ë°˜ í´ë°± ê²€ì‚¬: '{user_message_lower[:50]}...'")
                        
                        # ì‹œê°„ ê´€ë ¨ í‚¤ì›Œë“œ
                        time_keywords = ["ì‹œê°„", "ëª‡ì‹œ", "time", "í˜„ì¬ì‹œê°„", "ì§€ê¸ˆì‹œê°„"]
                        # ë‚ ì”¨ ê´€ë ¨ í‚¤ì›Œë“œ  
                        weather_keywords = ["ë‚ ì”¨", "weather", "ê¸°ì˜¨", "ì˜¨ë„", "ë¹„", "ëˆˆ", "ë§‘ìŒ", "íë¦¼"]
                        
                        if any(keyword in user_message_lower for keyword in time_keywords):
                            logger.info("ğŸ•’ ë¹„ìŠ¤íŠ¸ë¦¬ë° ì‹œê°„ ê´€ë ¨ ì§ˆë¬¸ ê°ì§€ - ì‹œê°„ í´ë°± ì ìš©")
                            import datetime
                            now = datetime.datetime.now()
                            response_text = f"í˜„ì¬ ì‹œê°„ì€ {now.strftime('%Yë…„ %mì›” %dì¼ %Hì‹œ %Më¶„')}ì…ë‹ˆë‹¤."
                        
                        elif any(keyword in user_message_lower for keyword in weather_keywords):
                            logger.info("ğŸŒ¤ï¸ ë¹„ìŠ¤íŠ¸ë¦¬ë° ë‚ ì”¨ ê´€ë ¨ ì§ˆë¬¸ ê°ì§€ - ë‚ ì”¨ í´ë°± ì ìš© (ë„êµ¬ ì§ì ‘ í˜¸ì¶œ)")
                            response_text = await self._get_weather_fallback(user_message)
                        else:
                            logger.info("ğŸ¤– ë¹„ìŠ¤íŠ¸ë¦¬ë° ì¼ë°˜ ì§ˆë¬¸ - ê¸°ë³¸ í´ë°± ì ìš©")
                            response_text = "ì£„ì†¡í•©ë‹ˆë‹¤. ìš”ì²­ì„ ì²˜ë¦¬í•˜ëŠ” ì¤‘ì— ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì‹œê±°ë‚˜ ë‹¤ë¥¸ ë°©ì‹ìœ¼ë¡œ ì§ˆë¬¸í•´ì£¼ì„¸ìš”."
                else:
                    logger.warning("âŒ ê²°ê³¼ì— 'messages' í‚¤ê°€ ì—†ìŒ")
                
                # -------------  ìƒˆ í›„ì²˜ë¦¬: í”Œë ˆì´ìŠ¤í™€ë” ì¹˜í™˜ -------------
                if response_text and tool_results:
                    processed = self._substitute_tool_placeholders(response_text, tool_results)
                    if processed != response_text:
                        logger.info("ğŸ”§ í”Œë ˆì´ìŠ¤í™€ë” ì¹˜í™˜ ì™„ë£Œ")
                        response_text = processed
                
                logger.info(f"âœ… ë¹„ìŠ¤íŠ¸ë¦¬ë° ì™„ë£Œ: {len(response_text)} ë¬¸ì, {len(used_tools)}ê°œ ë„êµ¬ ì‚¬ìš©")
                
                return {
                    "response": response_text,
                    "used_tools": used_tools
                }
            
        except Exception as e:
            logger.error(f"âŒ ReAct ì—ì´ì „íŠ¸ ì‹¤í–‰ ìµœìƒìœ„ ì˜¤ë¥˜: {e}")
            import traceback
            logger.error(f"âŒ ReAct ì—ì´ì „íŠ¸ ì‹¤í–‰ ìµœìƒìœ„ ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()}")
            return {
                "response": f"ReAct ì—ì´ì „íŠ¸ ì‹¤í–‰ ì¤‘ ìµœìƒìœ„ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}",
                "used_tools": []
            }

    async def _generate_basic_response(
        self,
        user_message: str,
        streaming_callback: Optional[Callable[[str], None]] = None,
    ) -> str:
        """ê¸°ë³¸ ì‘ë‹µ ìƒì„±"""
        try:
            logger.debug("ê¸°ë³¸ LLM ì„œë¹„ìŠ¤ë¥¼ í†µí•œ ì‘ë‹µ ìƒì„± ì‹œì‘")
            messages = self.conversation_service.get_messages()
            response = await self.llm_service.generate_response(messages, streaming_callback)
            logger.debug(f"ê¸°ë³¸ ì‘ë‹µ ìƒì„± ì™„ë£Œ: {len(response.response)} ë¬¸ì")
            return response.response
        except Exception as e:
            logger.error(f"ê¸°ë³¸ ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
            import traceback
            logger.error(f"ê¸°ë³¸ ì‘ë‹µ ìƒì„± ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()}")
            return f"ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"

    def _get_llm_mode(self) -> str:
        """LLM ëª¨ë“œ ë°˜í™˜"""
        mode = getattr(self.llm_config, 'mode', 'basic')
        if mode and isinstance(mode, str):
            return mode.lower()
        return "basic"

    def _create_response_data(
        self,
        response: str,
        reasoning: str = "",
        used_tools: Optional[List[str]] = None
    ) -> Dict[str, Any]:
        """ì‘ë‹µ ë°ì´í„° ìƒì„±"""
        if used_tools is None:
            used_tools = []
            
        # ì–´ì‹œìŠ¤í„´íŠ¸ ë©”ì‹œì§€ ì¶”ê°€
        self.add_assistant_message(response)
        
        return {
            "response": response,
            "reasoning": reasoning,
            "used_tools": used_tools
        }

    def _create_error_response(self, error_msg: str, detail: str = "") -> Dict[str, Any]:
        """ì—ëŸ¬ ì‘ë‹µ ìƒì„±"""
        response = f"ì£„ì†¡í•©ë‹ˆë‹¤. {error_msg}"
        self.add_assistant_message(response)
        
        return {
            "response": response,
            "reasoning": detail,
            "used_tools": []
        }

    def add_user_message(self, message: str) -> None:
        """ì‚¬ìš©ì ë©”ì‹œì§€ë¥¼ ëŒ€í™” íˆìŠ¤í† ë¦¬ì— ì¶”ê°€"""
        self.conversation_service.add_user_message(message)
        # í•˜ìœ„ í˜¸í™˜ì„±
        self.history.append({"role": "user", "content": message})

    def add_assistant_message(self, message: str) -> None:
        """ì–´ì‹œìŠ¤í„´íŠ¸ ë©”ì‹œì§€ë¥¼ ëŒ€í™” íˆìŠ¤í† ë¦¬ì— ì¶”ê°€"""
        self.conversation_service.add_assistant_message(message)
        # í•˜ìœ„ í˜¸í™˜ì„±
        self.history.append({"role": "assistant", "content": message})

    def clear_conversation(self) -> None:
        """ëŒ€í™” íˆìŠ¤í† ë¦¬ ì´ˆê¸°í™”"""
        self.conversation_service.clear_conversation()
        # í•˜ìœ„ í˜¸í™˜ì„±
        self.history.clear()
        # ìƒˆ thread_id ìƒì„±
        self.thread_id = str(uuid.uuid4())
        logger.info("ëŒ€í™” íˆìŠ¤í† ë¦¬ ì´ˆê¸°í™”")

    def get_conversation_history(self) -> List[Dict[str, str]]:
        """ëŒ€í™” íˆìŠ¤í† ë¦¬ ë°˜í™˜"""
        messages = self.conversation_service.get_messages_as_dict()
        # íƒ€ì… ë³€í™˜ ë³´ì¥
        result: List[Dict[str, str]] = []
        for msg in messages:
            if isinstance(msg, dict):
                result.append({
                    "role": str(msg.get("role", "")),
                    "content": str(msg.get("content", ""))
                })
        return result

    async def cleanup(self) -> None:
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        await self.llm_service.cleanup()
        if self.react_agent:
            self.react_agent = None
        logger.info("LLM ì—ì´ì „íŠ¸ ì •ë¦¬ ì™„ë£Œ")

    def reinitialize_client(self) -> None:
        """í´ë¼ì´ì–¸íŠ¸ ì¬ì´ˆê¸°í™” - í”„ë¡œí•„ ë³€ê²½ ì‹œ ì‚¬ìš©"""
        try:
            logger.info("LLM ì—ì´ì „íŠ¸ í´ë¼ì´ì–¸íŠ¸ ì¬ì´ˆê¸°í™” ì‹œì‘")
            
            # ì„¤ì • ë‹¤ì‹œ ë¡œë“œ
            self._load_config()
            
            # LLM ì„œë¹„ìŠ¤ ì¬ì´ˆê¸°í™”
            self.llm_service = LLMService(self.llm_config)
            
            # ReAct ì—ì´ì „íŠ¸ ì¬ì´ˆê¸°í™” í•„ìš”
            self.react_agent = None
            
            # ëŒ€í™” ì„œë¹„ìŠ¤ ì¬ì´ˆê¸°í™” (íˆìŠ¤í† ë¦¬ëŠ” ìœ ì§€)
            # self.conversation_serviceëŠ” ê·¸ëŒ€ë¡œ ìœ ì§€í•˜ì—¬ ëŒ€í™” ë§¥ë½ ë³´ì¡´
            
            logger.info(f"LLM ì—ì´ì „íŠ¸ ì¬ì´ˆê¸°í™” ì™„ë£Œ: ëª¨ë¸={self.llm_config.model}, ëª¨ë“œ={self.llm_config.mode}")
            
        except Exception as e:
            logger.error(f"LLM ì—ì´ì „íŠ¸ ì¬ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")

    # ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì € ì§€ì›
    async def __aenter__(self) -> "LLMAgent":
        return self

    async def __aexit__(self, exc_type: Any, exc_val: Any, exc_tb: Any) -> None:
        await self.cleanup()

    def _format_tool_results(self, used_tools: List[str], tool_results: Dict[str, str]) -> str:
        """ë„êµ¬ ê²°ê³¼(JSON)ë¥¼ ì‚¬ëŒì´ ì½ì„ ìˆ˜ ìˆëŠ” í•œêµ­ì–´ ë¬¸ì¥ìœ¼ë¡œ ë³€í™˜"""
        try:
            if not used_tools:
                return "ë„êµ¬ë¥¼ ì‚¬ìš©í•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."

            # ì‹œê°„ ê´€ë ¨ ë³€í™˜
            if "get_current_time" in used_tools:
                import datetime
                import json
                raw = tool_results.get("get_current_time", "")
                timestamp = None
                try:
                    data = json.loads(raw)
                    result_str = data.get("result", "")
                    timestamp = result_str.split("í˜„ì¬ ì‹œê°„:")[-1].strip()
                except Exception:
                    pass
                if not timestamp:
                    timestamp = datetime.datetime.now().isoformat()
                # ISO í˜•ì‹ -> í•œêµ­ì–´ ë‚ ì§œì‹œê°„
                try:
                    dt = datetime.datetime.fromisoformat(timestamp.replace("Z", ""))
                    return f"í˜„ì¬ ì‹œê°„ì€ {dt.strftime('%Yë…„ %mì›” %dì¼ %Hì‹œ %Më¶„')}ì…ë‹ˆë‹¤."
                except Exception:
                    return f"í˜„ì¬ ì‹œê°„ì€ {timestamp} ì…ë‹ˆë‹¤."

            if "get_time_in_timezone" in used_tools:
                import json
                raw = tool_results.get("get_time_in_timezone", "")
                try:
                    data = json.loads(raw)
                    result_str = data.get("result", "")
                    return result_str.replace("ì‹œê°„:", "ì‹œê°„ì€").rstrip() + "ì…ë‹ˆë‹¤."
                except Exception:
                    return result_str if "result_str" in locals() else "ìš”ì²­í•˜ì‹  ì‹œê°„ ì •ë³´ë¥¼ ê°€ì ¸ì˜¤ëŠ” ì¤‘ ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."

            if "get_current_weather" in used_tools:
                import json
                raw = tool_results.get("get_current_weather", "")
                try:
                    data = json.loads(raw)
                    result_str = data.get("result", "")
                    return result_str
                except Exception:
                    return "í˜„ì¬ ë‚ ì”¨ ì •ë³´ë¥¼ ê°€ì ¸ì˜¤ëŠ” ì¤‘ ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."

            # ê¸°íƒ€ ë„êµ¬
            first_tool = used_tools[0]
            result_raw = tool_results.get(first_tool, "")
            if result_raw:
                return f"ë„êµ¬ {first_tool} ì‹¤í–‰ ê²°ê³¼:\n{result_raw}"

            return f"ë„êµ¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ì‘ì—…ì„ ìˆ˜í–‰í–ˆì§€ë§Œ, ê²°ê³¼ë¥¼ ì²˜ë¦¬í•˜ëŠ” ì¤‘ì— ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ì‚¬ìš©ëœ ë„êµ¬: {', '.join(used_tools)}"
        except Exception as e:
            logger.error(f"ë„êµ¬ ê²°ê³¼ í¬ë§·íŒ… ì˜¤ë¥˜: {e}")
            return f"ë„êµ¬ ê²°ê³¼ë¥¼ ì²˜ë¦¬í•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ì‚¬ìš©ëœ ë„êµ¬: {', '.join(used_tools)}"

    async def _get_weather_fallback(self, user_message: str) -> str:
        """ë‚ ì”¨ í‚¤ì›Œë“œ í´ë°±: MCP ë‚ ì”¨ ë„êµ¬ë¥¼ ì§ì ‘ í˜¸ì¶œí•˜ì—¬ ì‘ë‹µ ìƒì„±"""
        try:
            if not self.mcp_tool_manager:
                logger.warning("MCP ë„êµ¬ ê´€ë¦¬ìê°€ ì—†ì–´ ë‚ ì”¨ ë„êµ¬ë¥¼ ì§ì ‘ í˜¸ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
                return "ì£„ì†¡í•©ë‹ˆë‹¤. í˜„ì¬ ë‚ ì”¨ ì •ë³´ë¥¼ ê°€ì ¸ì˜¤ëŠ” ì¤‘ ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."

            # ë„ì‹œ ì¶”ì¶œ (ì•„ì£¼ ë‹¨ìˆœí•œ ì •ê·œì‹: ë’¤ì— 'ë‚ ì”¨'ê°€ ì˜¤ëŠ” í•œê¸€/ì˜ë¬¸ ë‹¨ì–´)
            import json
            import re
            city = "Seoul"
            match = re.search(r"([A-Za-zê°€-í£]+)\s*(?:ë‚ ì”¨|ê¸°ì˜¨|ì˜¨ë„)", user_message)
            if match:
                city = match.group(1)

            logger.info(f"ğŸŒ¤ï¸ ë‚ ì”¨ í´ë°± - get_current_weather í˜¸ì¶œ (city={city})")
            raw_result = await self.mcp_tool_manager.call_mcp_tool("get_current_weather", {"city": city})

            # ê²°ê³¼ íŒŒì‹±
            try:
                data = json.loads(raw_result)
                parsed = data.get("result", raw_result)
            except Exception:
                parsed = raw_result

            # ìµœì¢… ë¬¸ì¥ ë³´ì • (ì¢…ê²° ì–´ë¯¸ê°€ ì—†ì„ ë•Œ 'ì…ë‹ˆë‹¤.' ì¶”ê°€)
            if parsed and not parsed.strip().endswith("ë‹¤.") and not parsed.strip().endswith("ìš”."):
                parsed = parsed.rstrip('.') + "ì…ë‹ˆë‹¤."

            return parsed
        except Exception as e:
            logger.error(f"ë‚ ì”¨ í´ë°± ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
            return "ì£„ì†¡í•©ë‹ˆë‹¤. í˜„ì¬ ë‚ ì”¨ ì •ë³´ë¥¼ ê°€ì ¸ì˜¤ëŠ” ì¤‘ ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."

    def _substitute_tool_placeholders(self, text: str, tool_results: Dict[str, str]) -> str:
        """AI ì‘ë‹µ ì•ˆì˜ `default_api.xxx()` ì‹ í”Œë ˆì´ìŠ¤í™€ë”ë¥¼ ì‹¤ì œ ë„êµ¬ ê²°ê³¼ë¡œ ì¹˜í™˜"""
        import json
        import re
        out = str(text)
        for tool_name, raw in tool_results.items():
            try:
                data = json.loads(raw)
                result_str = data.get("result", raw)
            except Exception:
                result_str = raw

            # ë°±í‹± í¬í•¨ íŒ¨í„´ ë° í•¨ìˆ˜í˜¸ì¶œ íŒ¨í„´ ì¹˜í™˜
            patterns = [
                rf"`[^`]*{tool_name}\([^`]*`",  # `default_api.get_current_weather(...)`
                rf"{tool_name}\([^)]*\)"          # get_current_weather(...)
            ]
            for pat in patterns:
                out = re.sub(pat, result_str, out)

        return out 