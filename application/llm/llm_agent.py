"""
Langchain ê¸°ë°˜ LLM ì—ì´ì „íŠ¸ - create_react_agent ì‚¬ìš© (OpenAI Compatible)
"""

import logging
import uuid
from typing import Any, Callable, Dict, List, Optional

from langchain_core.messages import HumanMessage
from langchain_core.runnables import RunnableConfig
from langchain_openai import ChatOpenAI

# langgraph importë¥¼ try-catchë¡œ ì²˜ë¦¬
try:
    from langgraph.checkpoint.memory import MemorySaver
    from langgraph.prebuilt import create_react_agent

    LANGGRAPH_AVAILABLE = True
    CreateReactAgentFunc: Optional[Any] = create_react_agent
    MemorySaverClass: Optional[Any] = MemorySaver
except ImportError:
    LANGGRAPH_AVAILABLE = False
    CreateReactAgentFunc = None
    MemorySaverClass = None

from application.llm.interfaces.llm_interface import LLMInterface
from application.llm.models.llm_config import LLMConfig
from application.llm.processors.base_processor import ToolResultProcessorRegistry
from application.llm.processors.search_processor import SearchToolResultProcessor
from application.llm.services.conversation_service import ConversationService
from application.llm.services.llm_service import LLMService
from application.llm.workflow.workflow_utils import get_workflow
from application.util.logger import setup_logger

logger = setup_logger("llm_agent") or logging.getLogger("llm_agent")

if not LANGGRAPH_AVAILABLE:
    logger.warning("langgraphë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ReAct ëª¨ë“œê°€ ë¹„í™œì„±í™”ë©ë‹ˆë‹¤.")


class LLMAgent(LLMInterface):
    def __init__(self, config_manager: Any, mcp_tool_manager: Optional[Any] = None) -> None:
        self.config_manager = config_manager
        self.mcp_tool_manager = mcp_tool_manager

        # ì„¤ì • ë¡œë“œ
        self._load_config()

        # ì„œë¹„ìŠ¤ ì´ˆê¸°í™”
        self.llm_service = LLMService(self.llm_config)
        self.conversation_service = ConversationService()

        # í”„ë¡œì„¸ì„œ ë ˆì§€ìŠ¤íŠ¸ë¦¬ ì´ˆê¸°í™”
        self.processor_registry: Optional[ToolResultProcessorRegistry] = None

        # ReAct ì—ì´ì „íŠ¸ ê´€ë ¨ (langgraphê°€ ì‚¬ìš© ê°€ëŠ¥í•œ ê²½ìš°ë§Œ)
        self.react_agent: Optional[Any] = None
        self.checkpointer: Optional[Any] = None
        if LANGGRAPH_AVAILABLE and MemorySaverClass is not None:
            self.checkpointer = MemorySaverClass()

        self.thread_id = str(uuid.uuid4())

        # íˆìŠ¤í† ë¦¬ (í•˜ìœ„ í˜¸í™˜ì„±)
        self.history: List[Dict[str, str]] = []

        logger.info("LLM ì—ì´ì „íŠ¸ ì´ˆê¸°í™” ì™„ë£Œ")

    def _load_config(self) -> None:
        """ì„¤ì • ë¡œë“œ"""
        try:
            # í”„ë¡œí•„ ê¸°ë°˜ ì„¤ì • ë¡œë“œ (modeì™€ workflow í¬í•¨)
            llm_config_dict = self.config_manager.get_llm_config()

            self.llm_config = LLMConfig.from_dict(llm_config_dict)
            logger.debug(
                f"LLM ì„¤ì • ë¡œë“œ ì™„ë£Œ: {self.llm_config.model}, ëª¨ë“œ: {self.llm_config.mode}"
            )

        except Exception as e:
            logger.error(f"ì„¤ì • ë¡œë“œ ì‹¤íŒ¨: {e}")
            # ê¸°ë³¸ ì„¤ì •ìœ¼ë¡œ í´ë°± - ëª¨ë“  í•„ìˆ˜ í•„ë“œ í¬í•¨
            self.llm_config = LLMConfig(
                api_key="",
                base_url=None,
                model="gpt-4o-mini",
                max_tokens=1000,
                temperature=0.7,
                streaming=True,
                mode="basic",
                workflow=None,
            )

    async def generate_response(
        self,
        user_message: str,
        streaming_callback: Optional[Callable[[str], None]] = None,
    ) -> Dict[str, Any]:
        try:
            logger.info(f"LLM ì‘ë‹µ ìƒì„± ì‹œì‘: {user_message[:50]}...")

            # ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€
            self.add_user_message(user_message)

            # ëª¨ë“œì— ë”°ë¥¸ ì²˜ë¦¬
            mode = self._get_llm_mode()
            logger.info(f"ğŸ¯ í˜„ì¬ LLM ëª¨ë“œ: {mode}")
            logger.info(f"ğŸ”§ MCP ë„êµ¬ ê´€ë¦¬ì ì¡´ì¬: {self.mcp_tool_manager is not None}")
            logger.info(f"ğŸš€ LANGGRAPH ì‚¬ìš© ê°€ëŠ¥: {LANGGRAPH_AVAILABLE}")

            if mode == "workflow":
                logger.info("ğŸ“‹ ì›Œí¬í”Œë¡œìš° ëª¨ë“œë¡œ ì²˜ë¦¬")
                return await self._handle_workflow_mode(user_message, streaming_callback)
            elif mode == "mcp_tools" and self.mcp_tool_manager and LANGGRAPH_AVAILABLE:
                logger.info("ğŸ¤– ReAct ì—ì´ì „íŠ¸ ëª¨ë“œë¡œ ì²˜ë¦¬")
                return await self._handle_react_agent_mode(user_message, streaming_callback)
            else:
                logger.info("âš¡ ê¸°ë³¸ ëª¨ë“œë¡œ ì²˜ë¦¬")
                return await self._handle_basic_mode(user_message, streaming_callback)
        except Exception as e:
            logger.error(f"ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
            import traceback

            logger.error(f"ì‘ë‹µ ìƒì„± ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()}")
            return self._create_error_response("ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤", str(e))

    async def generate_response_streaming(
        self,
        user_message: str,
        streaming_callback: Optional[Callable[[str], None]] = None,
    ) -> Dict[str, Any]:
        """ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ìƒì„± (í•˜ìœ„ í˜¸í™˜ì„±)"""
        return await self.generate_response(user_message, streaming_callback)

    async def _handle_basic_mode(
        self,
        user_message: str,
        streaming_callback: Optional[Callable[[str], None]] = None,
    ) -> Dict[str, Any]:
        """ê¸°ë³¸ ëª¨ë“œ ì²˜ë¦¬"""
        try:
            logger.debug("ê¸°ë³¸ ëª¨ë“œë¡œ ì‘ë‹µ ìƒì„± ì¤‘...")
            response = await self._generate_basic_response(user_message, streaming_callback)
            logger.debug(f"ê¸°ë³¸ ì‘ë‹µ ìƒì„± ì™„ë£Œ: {len(response)} ë¬¸ì")
            return self._create_response_data(response)
        except Exception as e:
            logger.error(f"ê¸°ë³¸ ëª¨ë“œ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
            return self._create_error_response("ê¸°ë³¸ ëª¨ë“œ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤", str(e))

    async def _handle_workflow_mode(
        self,
        user_message: str,
        streaming_callback: Optional[Callable[[str], None]] = None,
    ) -> Dict[str, Any]:
        """ì›Œí¬í”Œë¡œìš° ëª¨ë“œ ì²˜ë¦¬"""
        try:
            workflow_name = self.llm_config.workflow or "basic_chat"
            workflow_class = get_workflow(workflow_name)
            workflow = workflow_class()

            result = await workflow.run(self, user_message, streaming_callback)

            return {
                "response": result,
                "workflow": workflow_name,
                "reasoning": "",
                "used_tools": [],
            }

        except Exception as e:
            logger.error(f"ì›Œí¬í”Œë¡œìš° ëª¨ë“œ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
            return {
                "response": "ì›Œí¬í”Œë¡œìš° ì²˜ë¦¬ ì¤‘ ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.",
                "workflow": self.llm_config.workflow or "basic_chat",
                "reasoning": str(e),
                "used_tools": [],
            }

    async def _handle_react_agent_mode(
        self,
        user_message: str,
        streaming_callback: Optional[Callable[[str], None]] = None,
    ) -> Dict[str, Any]:
        """ReAct ì—ì´ì „íŠ¸ ëª¨ë“œ ì²˜ë¦¬"""
        try:
            logger.info("ğŸ¤– ReAct ì—ì´ì „íŠ¸ ëª¨ë“œë¡œ ì‘ë‹µ ìƒì„± ì¤‘...")

            if not LANGGRAPH_AVAILABLE:
                logger.error("âŒ langgraphê°€ ì‚¬ìš©í•  ìˆ˜ ì—†ìŒ")
                logger.info("ğŸ”„ ê¸°ë³¸ ëª¨ë“œë¡œ í´ë°±")
                return await self._handle_basic_mode(user_message, streaming_callback)

            if not self.mcp_tool_manager:
                logger.error("âŒ MCP ë„êµ¬ ê´€ë¦¬ìê°€ ì—†ìŒ")
                logger.info("ğŸ”„ ê¸°ë³¸ ëª¨ë“œë¡œ í´ë°±")
                return await self._handle_basic_mode(user_message, streaming_callback)

            # ReAct ì—ì´ì „íŠ¸ ì´ˆê¸°í™” (í•„ìš”í•œ ê²½ìš°)
            if not self.react_agent:
                logger.info("ğŸ”„ ReAct ì—ì´ì „íŠ¸ ì´ˆê¸°í™” ì‹œì‘...")
                init_success = await self._initialize_react_agent()
                if not init_success:
                    logger.error("âŒ ReAct ì—ì´ì „íŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨")
                    logger.info("ğŸ”„ ê¸°ë³¸ ëª¨ë“œë¡œ í´ë°±")
                    return await self._handle_basic_mode(user_message, streaming_callback)
                else:
                    logger.info("âœ… ReAct ì—ì´ì „íŠ¸ ì´ˆê¸°í™” ì„±ê³µ")
            else:
                logger.info("âœ… ReAct ì—ì´ì „íŠ¸ ì´ë¯¸ ì´ˆê¸°í™”ë¨")

            # ReAct ì—ì´ì „íŠ¸ë¥¼ ì‚¬ìš©í•œ ì‘ë‹µ ìƒì„±
            logger.info("ğŸš€ ReAct ì—ì´ì „íŠ¸ ì‹¤í–‰ ì‹œì‘...")
            result = await self._run_react_agent(user_message, streaming_callback)

            logger.info(f"âœ… ReAct ì—ì´ì „íŠ¸ ì‘ë‹µ ì™„ë£Œ: {len(result.get('response', ''))} ë¬¸ì")

            return {
                "response": result.get("response", ""),
                "reasoning": "ReAct ì—ì´ì „íŠ¸ë¥¼ ì‚¬ìš©í•œ ì‘ë‹µ",
                "used_tools": result.get("used_tools", []),
            }

        except Exception as e:
            logger.error(f"ReAct ì—ì´ì „íŠ¸ ëª¨ë“œ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
            import traceback

            logger.error(f"ReAct ì—ì´ì „íŠ¸ ëª¨ë“œ ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()}")
            logger.info("ğŸ”„ ê¸°ë³¸ ëª¨ë“œë¡œ í´ë°±")
            return await self._handle_basic_mode(user_message, streaming_callback)

    async def _initialize_react_agent(self) -> bool:
        """ReAct ì—ì´ì „íŠ¸ ì´ˆê¸°í™”"""
        try:
            if not LANGGRAPH_AVAILABLE:
                logger.error("langgraphë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
                return False

            if not self.mcp_tool_manager:
                logger.error("MCP ë„êµ¬ ê´€ë¦¬ìê°€ ì—†ìŠµë‹ˆë‹¤")
                return False

            # MCP ë„êµ¬ ê°€ì ¸ì˜¤ê¸°
            tools = await self.mcp_tool_manager.get_langchain_tools()
            if not tools:
                logger.warning("ì‚¬ìš© ê°€ëŠ¥í•œ MCP ë„êµ¬ê°€ ì—†ìŠµë‹ˆë‹¤")
                return False

            # LLM ëª¨ë¸ ì´ˆê¸°í™”
            llm = await self._create_llm_model()
            if not llm:
                logger.error("LLM ëª¨ë¸ ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤")
                return False

            # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì„¤ì •
            system_prompt = self._get_system_prompt()

            # ReAct ì—ì´ì „íŠ¸ ìƒì„±
            if CreateReactAgentFunc is not None and self.checkpointer:
                self.react_agent = CreateReactAgentFunc(
                    llm, tools, checkpointer=self.checkpointer, prompt=system_prompt
                )

                logger.info(f"ReAct ì—ì´ì „íŠ¸ ì´ˆê¸°í™” ì™„ë£Œ: {len(tools)}ê°œ ë„êµ¬")
                return True
            else:
                logger.error("create_react_agent ë˜ëŠ” checkpointerë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
                return False

        except Exception as e:
            logger.error(f"ReAct ì—ì´ì „íŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            import traceback

            logger.error(f"ReAct ì—ì´ì „íŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨ ìƒì„¸: {traceback.format_exc()}")
            return False

    async def _create_llm_model(self) -> Optional[ChatOpenAI]:
        """OpenAI Compatible LLM ëª¨ë¸ ìƒì„±"""
        try:
            model_name = str(self.llm_config.model)

            # ChatOpenAI ì´ˆê¸°í™” - ëª…ì‹œì  íŒŒë¼ë¯¸í„° ì „ë‹¬
            openai_params = {
                "model": model_name,
                "temperature": float(self.llm_config.temperature),
            }

            # API í‚¤ ì„¤ì •
            if self.llm_config.api_key:
                openai_params["api_key"] = str(self.llm_config.api_key)

            # base_urlì´ ìˆìœ¼ë©´ ì¶”ê°€
            if self.llm_config.base_url:
                openai_params["base_url"] = str(self.llm_config.base_url)

            # streaming ì„¤ì •
            if hasattr(self.llm_config, "streaming") and self.llm_config.streaming is not None:
                openai_params["streaming"] = bool(self.llm_config.streaming)

            logger.debug(
                f"ChatOpenAI ì´ˆê¸°í™” íŒŒë¼ë¯¸í„°: model={model_name}, base_url={openai_params.get('base_url', 'None')}"
            )

            # ëª…ì‹œì  ìƒì„±ì í˜¸ì¶œ - ì•ˆì •ì„±ì„ ìœ„í•œ ì¶”ê°€ íŒŒë¼ë¯¸í„°
            return ChatOpenAI(
                model=str(openai_params["model"]),
                temperature=float(openai_params["temperature"]),
                api_key=str(openai_params["api_key"]) if openai_params.get("api_key") else None,
                base_url=str(openai_params["base_url"]) if openai_params.get("base_url") else None,
                streaming=bool(openai_params.get("streaming", True)),
                max_tokens=None,  # ìµœëŒ€ í† í° ì œí•œ ì—†ìŒ
                timeout=60,  # 60ì´ˆ íƒ€ì„ì•„ì›ƒ
                max_retries=3,  # ìµœëŒ€ 3íšŒ ì¬ì‹œë„
            )

        except Exception as e:
            logger.error(f"LLM ëª¨ë¸ ìƒì„± ì‹¤íŒ¨: {e}")
            import traceback

            logger.error(f"LLM ëª¨ë¸ ìƒì„± ì‹¤íŒ¨ ìƒì„¸: {traceback.format_exc()}")
            return None

    def _get_system_prompt(self) -> str:
        """ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ë°˜í™˜"""
        return """ë‹¹ì‹ ì€ ë‹¤ì–‘í•œ ë„êµ¬ë¥¼ í™œìš©í•˜ì—¬ ì •ë³´ë¥¼ ìˆ˜ì§‘í•˜ê³  ë¶„ì„í•˜ëŠ” ì§€ëŠ¥í˜• AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.

**í•µì‹¬ ì—­í• :**
- ì‚¬ìš©ìì˜ ìš”ì²­ì„ ì •í™•íˆ ì´í•´í•˜ê³  ì ì ˆí•œ ë„êµ¬ë¥¼ ì„ íƒí•˜ì—¬ ì •ë³´ë¥¼ ìˆ˜ì§‘
- ìˆ˜ì§‘ëœ ì •ë³´ë¥¼ ë¶„ì„í•˜ì—¬ ì‚¬ìš©ìì—ê²Œ ë§ì¶¤í™”ëœ ìœ ìš©í•œ ë‹µë³€ ì œê³µ
- ë‹¨ìˆœí•œ ë‚˜ì—´ì´ ì•„ë‹Œ ê¹Šì´ ìˆëŠ” ë¶„ì„ê³¼ ì¸ì‚¬ì´íŠ¸ ì œê³µ

**ì‘ì—… ì ˆì°¨:**

1. **ìš”ì²­ ë¶„ì„**: ì‚¬ìš©ìê°€ ì›í•˜ëŠ” ê²ƒì„ ì •í™•íˆ íŒŒì•…í•˜ê³  í•„ìš”í•œ ë„êµ¬ ê²°ì •

2. **ë„êµ¬ í™œìš©**: ì ì ˆí•œ ë„êµ¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ê´€ë ¨ ì •ë³´ ìˆ˜ì§‘

3. **ì •ë³´ ë¶„ì„ ë° ê°€ê³µ** (ë§¤ìš° ì¤‘ìš”):
   - ë„êµ¬ë¡œë¶€í„° ë°›ì€ ì›ì‹œ ë°ì´í„°ë¥¼ ì² ì €íˆ ë¶„ì„
   - í•µì‹¬ ì •ë³´ì™€ íŒ¨í„´ì„ ì¶”ì¶œí•˜ê³  ì˜ë¯¸ìˆëŠ” ì¸ì‚¬ì´íŠ¸ ë„ì¶œ
   - ì—¬ëŸ¬ ì†ŒìŠ¤ì˜ ì •ë³´ë¥¼ ì—°ê²°í•˜ê³  ë¹„êµ ë¶„ì„
   - ì‚¬ìš©ìì˜ ì›ë˜ ì§ˆë¬¸ì— ë§ê²Œ ì •ë³´ë¥¼ ì¬êµ¬ì„±

4. **ë§ì¶¤í˜• ì‘ë‹µ ìƒì„±**:
   - ìˆ˜ì§‘ëœ ì •ë³´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ìƒì„¸í•˜ê³  ìœ ìš©í•œ ë‹µë³€ ì‘ì„±
   - ê´€ë ¨ ì„¸ë¶€ì‚¬í•­, ì¸ìš©êµ¬, í†µê³„ ë“±ì„ í¬í•¨
   - ëª…í™•í•˜ê³  ë…¼ë¦¬ì ìœ¼ë¡œ ì •ë³´ êµ¬ì„±
   - ì¶œì²˜ ëª…ì‹œ ë° ë§¥ë½ ì œê³µ

**ì¤‘ìš” ì›ì¹™:**
- **ë„êµ¬ ê²°ê³¼ ìš°ì„ **: ë„êµ¬ë¡œ ìˆ˜ì§‘í•œ ë°ì´í„°ë¥¼ ì£¼ìš” ê·¼ê±°ë¡œ ì‚¬ìš©
- **ë¶„ì„ì  ì ‘ê·¼**: ë‹¨ìˆœ ìš”ì•½ì´ ì•„ë‹Œ í•´ì„ê³¼ ë§¥ë½ ì œê³µ
- **í•œêµ­ì–´ ì‘ë‹µ**: ëª¨ë“  ì‘ë‹µì€ ìì—°ìŠ¤ëŸ¬ìš´ í•œêµ­ì–´ë¡œ ì‘ì„±
- **ì „ë¬¸ì ì´ê³  ë„ì›€ì´ ë˜ëŠ” í†¤**: ëª…í™•í•˜ê³  ìœ ìµí•œ ì •ë³´ ì „ë‹¬
- **í’ë¶€í•œ ì½˜í…ì¸ **: ë„êµ¬ê°€ ì œê³µí•˜ëŠ” ìƒì„¸ ì •ë³´ë¥¼ ìµœëŒ€í•œ í™œìš©

**íŠ¹ë³„ ì§€ì¹¨:**
ë„êµ¬ë¥¼ ì‚¬ìš©í•œ í›„ì—ëŠ” ë°˜ë“œì‹œ ê²°ê³¼ë¥¼ ë¶„ì„í•˜ì—¬ ì‚¬ìš©ì ìš”ì²­ì— ë§ëŠ” ìœ ìš©í•œ ë‹µë³€ì„ ìƒì„±í•´ì•¼ í•©ë‹ˆë‹¤.
ì›ì‹œ ë°ì´í„°ë¥¼ ê·¸ëŒ€ë¡œ ë‚˜ì—´í•˜ì§€ ë§ê³ , ì‚¬ìš©ìê°€ ì´í•´í•˜ê¸° ì‰½ê³  ì‹¤ìš©ì ì¸ í˜•íƒœë¡œ ê°€ê³µí•˜ì—¬ ì œê³µí•˜ì„¸ìš”."""

    async def _run_react_agent(
        self,
        user_message: str,
        streaming_callback: Optional[Callable[[str], None]] = None,
    ) -> Dict[str, Any]:
        """ReAct ì—ì´ì „íŠ¸ ì‹¤í–‰"""
        try:
            logger.info(f"ğŸ”¥ ReAct ì—ì´ì „íŠ¸ ì‹¤í–‰ ë©”ì„œë“œ ì§„ì…: {user_message[:50]}...")

            if not self.react_agent:
                logger.error("âŒ ReAct ì—ì´ì „íŠ¸ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•ŠìŒ")
                return {"response": "ReAct ì—ì´ì „íŠ¸ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.", "used_tools": []}

            logger.info("âœ… ReAct ì—ì´ì „íŠ¸ í™•ì¸ ì™„ë£Œ")

            # ì„¤ì • ìƒì„± (configurable ì‚¬ìš©)
            config = RunnableConfig(recursion_limit=100, configurable={"thread_id": self.thread_id})
            logger.info(f"âœ… ì„¤ì • ìƒì„± ì™„ë£Œ: thread_id={self.thread_id}")

            # ë©”ì‹œì§€ ìƒì„±
            messages = [HumanMessage(content=user_message)]
            inputs = {"messages": messages}
            logger.info(f"âœ… ì…ë ¥ ë©”ì‹œì§€ ìƒì„± ì™„ë£Œ: {len(messages)}ê°œ ë©”ì‹œì§€")

            # ìŠ¤íŠ¸ë¦¬ë°ì´ í™œì„±í™”ëœ ê²½ìš°
            if streaming_callback is not None:
                logger.info("ğŸ¬ ìŠ¤íŠ¸ë¦¬ë° ëª¨ë“œë¡œ ì‹¤í–‰")
                accumulated_response = ""
                used_tools: List[str] = []
                tool_results: Dict[str, str] = {}

                try:
                    # astream_graph í™œìš©í•˜ì—¬ ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬
                    from application.llm.workflow.workflow_utils import astream_graph

                    logger.info("âœ… astream_graph import ì„±ê³µ")

                    def streaming_wrapper(chunk: Dict[str, Any]) -> None:
                        nonlocal accumulated_response, used_tools, tool_results

                        logger.info(
                            f"ğŸ“¦ ìŠ¤íŠ¸ë¦¬ë° ì²­í¬ ìˆ˜ì‹ : {type(chunk)}, keys={list(chunk.keys()) if isinstance(chunk, dict) else 'N/A'}"
                        )

                        # langgraph ReAct ì—ì´ì „íŠ¸ì˜ ì‹¤ì œ ì‘ë‹µ êµ¬ì¡° ì²˜ë¦¬
                        if isinstance(chunk, dict):
                            # ì§ì ‘ messages í‚¤ê°€ ìˆëŠ” ê²½ìš°
                            if "messages" in chunk:
                                logger.info(
                                    f"ğŸ“¬ ì§ì ‘ ë©”ì‹œì§€ ì²­í¬ ì²˜ë¦¬: {len(chunk['messages'])}ê°œ ë©”ì‹œì§€"
                                )
                                for i, message in enumerate(chunk["messages"]):
                                    logger.info(
                                        f"ğŸ“¨ ì§ì ‘ ë©”ì‹œì§€ {i}: type={type(message)}, hasattr(content)={hasattr(message, 'content')}"
                                    )
                                    # AIMessageë§Œ ì‚¬ìš©ìì—ê²Œ ì „ë‹¬ (ë„êµ¬ ê²°ê³¼ëŠ” ì œì™¸)
                                    if (
                                        hasattr(message, "content")
                                        and message.content
                                        and str(type(message)).find("AIMessage") != -1
                                    ):
                                        content = str(message.content)
                                        accumulated_response += content
                                        logger.info(
                                            f"ğŸ“ ì§ì ‘ AI ì»¨í…ì¸  ì¶”ê°€: {len(content)}ì, ëˆ„ì ={len(accumulated_response)}ì"
                                        )
                                        try:
                                            if streaming_callback is not None:
                                                streaming_callback(content)
                                                logger.info(f"âœ… ì§ì ‘ AI ìŠ¤íŠ¸ë¦¬ë° ì½œë°± ì‹¤í–‰ ì„±ê³µ")
                                        except Exception as e:
                                            logger.error(f"âŒ ì§ì ‘ AI ìŠ¤íŠ¸ë¦¬ë° ì½œë°± ì‹¤í–‰ ì˜¤ë¥˜: {e}")

                                    # ë„êµ¬ ì‚¬ìš© ì¶”ì  (ë¡œê¹…ìš©)
                                    if hasattr(message, "tool_calls") and message.tool_calls:
                                        for tool_call in message.tool_calls:
                                            tool_name = (
                                                tool_call.get("name", "unknown_tool")
                                                if isinstance(tool_call, dict)
                                                else getattr(tool_call, "name", "unknown_tool")
                                            )
                                            used_tools.append(tool_name)
                                            logger.info(f"ğŸ”§ ì§ì ‘ ë„êµ¬ ì‚¬ìš©: {tool_name}")

                                            # --- ì‚¬ìš©ìì—ê²Œ ì§„í–‰ ìƒí™© ì•Œë¦¼ ---
                                            try:
                                                if streaming_callback is not None:
                                                    streaming_callback(f"\nâ³ '{tool_name}' ë„êµ¬ ì‹¤í–‰ ì¤‘...<br/>")
                                            except Exception as cb_err:
                                                logger.error(f"ì§„í–‰ ìƒí™© ì½œë°± ì˜¤ë¥˜: {cb_err}")

                            # agent í‚¤ ì•ˆì— messagesê°€ ìˆëŠ” ê²½ìš° (ReAct ì—ì´ì „íŠ¸ êµ¬ì¡°)
                            elif (
                                "agent" in chunk
                                and isinstance(chunk["agent"], dict)
                                and "messages" in chunk["agent"]
                            ):
                                logger.info(
                                    f"ğŸ“¬ ì—ì´ì „íŠ¸ ë©”ì‹œì§€ ì²­í¬ ì²˜ë¦¬: {len(chunk['agent']['messages'])}ê°œ ë©”ì‹œì§€"
                                )
                                for i, message in enumerate(chunk["agent"]["messages"]):
                                    logger.info(
                                        f"ğŸ“¨ ì—ì´ì „íŠ¸ ë©”ì‹œì§€ {i}: type={type(message)}, hasattr(content)={hasattr(message, 'content')}"
                                    )
                                    # AIMessageë§Œ ì‚¬ìš©ìì—ê²Œ ì „ë‹¬ (ë„êµ¬ ê²°ê³¼ëŠ” ì œì™¸)
                                    if (
                                        hasattr(message, "content")
                                        and message.content
                                        and str(type(message)).find("AIMessage") != -1
                                    ):
                                        content = str(message.content)
                                        accumulated_response += content
                                        logger.info(
                                            f"ğŸ“ ì—ì´ì „íŠ¸ AI ì»¨í…ì¸  ì¶”ê°€: {len(content)}ì, ëˆ„ì ={len(accumulated_response)}ì"
                                        )
                                        try:
                                            if streaming_callback is not None:
                                                streaming_callback(content)
                                                logger.info(
                                                    f"âœ… ì—ì´ì „íŠ¸ AI ìŠ¤íŠ¸ë¦¬ë° ì½œë°± ì‹¤í–‰ ì„±ê³µ"
                                                )
                                        except Exception as e:
                                            logger.error(
                                                f"âŒ ì—ì´ì „íŠ¸ AI ìŠ¤íŠ¸ë¦¬ë° ì½œë°± ì‹¤í–‰ ì˜¤ë¥˜: {e}"
                                            )

                                    # ë„êµ¬ ì‚¬ìš© ì¶”ì  (ë¡œê¹…ìš©)
                                    if hasattr(message, "tool_calls") and message.tool_calls:
                                        for tool_call in message.tool_calls:
                                            tool_name = (
                                                tool_call.get("name", "unknown_tool")
                                                if isinstance(tool_call, dict)
                                                else getattr(tool_call, "name", "unknown_tool")
                                            )
                                            used_tools.append(tool_name)
                                            logger.info(f"ğŸ”§ ì—ì´ì „íŠ¸ ë„êµ¬ ì‚¬ìš©: {tool_name}")

                                            # --- ì‚¬ìš©ìì—ê²Œ ì§„í–‰ ìƒí™© ì•Œë¦¼ ---
                                            try:
                                                if streaming_callback is not None:
                                                    streaming_callback(f"\nâ³ '{tool_name}' ë„êµ¬ ì‹¤í–‰ ì¤‘...<br/>")
                                            except Exception as cb_err:
                                                logger.error(f"ì§„í–‰ ìƒí™© ì½œë°± ì˜¤ë¥˜: {cb_err}")

                            # tools í‚¤ ì•ˆì— messagesê°€ ìˆëŠ” ê²½ìš° (ë„êµ¬ ê²°ê³¼ - ì‚¬ìš©ìì—ê²Œ ì§ì ‘ ì „ë‹¬í•˜ì§€ ì•ŠìŒ)
                            elif (
                                "tools" in chunk
                                and isinstance(chunk["tools"], dict)
                                and "messages" in chunk["tools"]
                            ):
                                logger.info(
                                    f"ğŸ”§ ë„êµ¬ ê²°ê³¼ ì²­í¬ ì²˜ë¦¬: {len(chunk['tools']['messages'])}ê°œ ë©”ì‹œì§€ (ì‚¬ìš©ìì—ê²Œ ì§ì ‘ ì „ë‹¬í•˜ì§€ ì•ŠìŒ)"
                                )
                                # ë„êµ¬ ê²°ê³¼ëŠ” ë¡œê¹…ë§Œ í•˜ê³  ì‚¬ìš©ìì—ê²Œ ì§ì ‘ ì „ë‹¬í•˜ì§€ ì•ŠìŒ
                                for i, message in enumerate(chunk["tools"]["messages"]):
                                    logger.info(
                                        f"ğŸ”§ ë„êµ¬ ê²°ê³¼ {i}: type={type(message)}, content={getattr(message, 'content', 'No content')[:100]}..."
                                    )

                                    # ë„êµ¬ ì‚¬ìš© ì¶”ì 
                                    if hasattr(message, "name") and message.name:
                                        tool_name = str(message.name)
                                        if tool_name not in used_tools:
                                            used_tools.append(tool_name)
                                            logger.info(f"ğŸ”§ ë„êµ¬ ì‚¬ìš© ê¸°ë¡: {tool_name}")
                                        # ë©”ì‹œì§€ content ì €ì¥ (JSON ë¬¸ìì—´ì¼ ìˆ˜ ìˆìŒ)
                                        if hasattr(message, "content") and message.content:
                                            tool_results[tool_name] = str(message.content)

                                            # --- ì‚¬ìš©ìì—ê²Œ ê²°ê³¼ ìˆ˜ì‹  ì•Œë¦¼ ---
                                            try:
                                                if streaming_callback is not None:
                                                    streaming_callback(f"\nâœ… '{tool_name}' ê²°ê³¼ ìˆ˜ì‹  ì™„ë£Œ<br/>")
                                            except Exception as cb_err:
                                                logger.error(f"ê²°ê³¼ ìˆ˜ì‹  ì½œë°± ì˜¤ë¥˜: {cb_err}")

                            # ë‹¤ë¥¸ êµ¬ì¡°ì˜ ì²­í¬ë“¤
                            else:
                                logger.info(
                                    f"ğŸ“¦ ë‹¤ë¥¸ êµ¬ì¡°ì˜ ì²­í¬: {list(chunk.keys()) if isinstance(chunk, dict) else str(chunk)[:100]}"
                                )
                                # ì—ëŸ¬ ì²­í¬ëŠ” ê±´ë„ˆë›°ê¸°
                                if isinstance(chunk, dict) and "error" in chunk and "type" in chunk:
                                    logger.warning(
                                        f"âš ï¸ ì—ëŸ¬ ì²­í¬ ê±´ë„ˆë›°ê¸°: {str(chunk.get('error', ''))[:100]}..."
                                    )
                                    return

                                # í˜¹ì‹œ ë‹¤ë¥¸ êµ¬ì¡°ë¡œ AIMessageê°€ ìˆëŠ”ì§€ ì²´í¬
                                if isinstance(chunk, dict):
                                    for key, value in chunk.items():
                                        if isinstance(value, dict) and "messages" in value:
                                            logger.info(
                                                f"ğŸ“¬ {key} ì•ˆì—ì„œ ë©”ì‹œì§€ ë°œê²¬: {len(value['messages'])}ê°œ"
                                            )
                                            for i, message in enumerate(value["messages"]):
                                                # AIMessageë§Œ ì²˜ë¦¬
                                                if (
                                                    hasattr(message, "content")
                                                    and message.content
                                                    and str(type(message)).find("AIMessage") != -1
                                                ):
                                                    content = str(message.content)
                                                    accumulated_response += content
                                                    logger.info(
                                                        f"ğŸ“ {key} AI ì»¨í…ì¸  ì¶”ê°€: {len(content)}ì, ëˆ„ì ={len(accumulated_response)}ì"
                                                    )
                                                    try:
                                                        if streaming_callback is not None:
                                                            streaming_callback(content)
                                                            logger.info(
                                                                f"âœ… {key} AI ìŠ¤íŠ¸ë¦¬ë° ì½œë°± ì‹¤í–‰ ì„±ê³µ"
                                                            )
                                                    except Exception as e:
                                                        logger.error(
                                                            f"âŒ {key} AI ìŠ¤íŠ¸ë¦¬ë° ì½œë°± ì‹¤í–‰ ì˜¤ë¥˜: {e}"
                                                        )
                        else:
                            logger.info(
                                f"ğŸ“¦ dictê°€ ì•„ë‹Œ ì²­í¬: {type(chunk)} - {str(chunk)[:100]}..."
                            )

                    # astream_graphë¥¼ ì‚¬ìš©í•˜ì—¬ ìŠ¤íŠ¸ë¦¬ë° ì‹¤í–‰
                    logger.info("ğŸš€ astream_graph ì‹¤í–‰ ì‹œì‘...")
                    chunk_count = 0
                    final_response_found = False

                    async for chunk_result in astream_graph(
                        self.react_agent, inputs, callback=streaming_wrapper, config=config
                    ):
                        chunk_count += 1
                        logger.info(f"ğŸ“¦ ì²­í¬ {chunk_count} ì™„ë£Œ: {type(chunk_result)}")

                        # ì²­í¬ì—ì„œ ìµœì¢… AI ì‘ë‹µ í™•ì¸
                        if isinstance(chunk_result, dict):
                            # agent í‚¤ì—ì„œ AIMessage í™•ì¸
                            if (
                                "agent" in chunk_result
                                and isinstance(chunk_result["agent"], dict)
                                and "messages" in chunk_result["agent"]
                            ):
                                for message in chunk_result["agent"]["messages"]:
                                    if (
                                        hasattr(message, "content")
                                        and message.content
                                        and str(type(message)).find("AIMessage") != -1
                                    ):
                                        final_response_found = True
                                        logger.info(
                                            f"âœ… ìµœì¢… AI ì‘ë‹µ ë°œê²¬: {len(str(message.content))}ì"
                                        )
                                        break

                    logger.info(
                        f"âœ… astream_graph ì™„ë£Œ: {chunk_count}ê°œ ì²­í¬ ì²˜ë¦¬, ìµœì¢…ì‘ë‹µ={final_response_found}"
                    )

                    # ìµœì¢… ì‘ë‹µì´ ì—†ê³  ë„êµ¬ë¥¼ ì‚¬ìš©í–ˆë‹¤ë©´ ë¹„ìŠ¤íŠ¸ë¦¬ë° ëª¨ë“œë¡œ ì¬ì‹œë„
                    if not final_response_found and len(used_tools) > 0:
                        logger.warning(
                            "âš ï¸ ë„êµ¬ ì‚¬ìš© í›„ ìµœì¢… AI ì‘ë‹µì´ ì—†ìŒ - ë¹„ìŠ¤íŠ¸ë¦¬ë° ëª¨ë“œë¡œ ì¬ì‹œë„"
                        )
                        try:
                            # ìƒˆë¡œìš´ thread_idë¡œ ë¹„ìŠ¤íŠ¸ë¦¬ë° ì‹¤í–‰
                            fallback_thread_id = str(uuid.uuid4())
                            fallback_config = RunnableConfig(
                                recursion_limit=100, configurable={"thread_id": fallback_thread_id}
                            )

                            logger.info("ğŸ”„ ë¹„ìŠ¤íŠ¸ë¦¬ë° í´ë°± ì‹¤í–‰...")
                            result = await self.react_agent.ainvoke(inputs, config=fallback_config)

                            if "messages" in result:
                                for message in result["messages"]:
                                    if (
                                        hasattr(message, "content")
                                        and message.content
                                        and str(type(message)).find("AIMessage") != -1
                                    ):
                                        content = str(message.content)
                                        if content.strip() and content not in accumulated_response:
                                            accumulated_response += content
                                            logger.info(f"ğŸ“ í´ë°± AI ì‘ë‹µ ì¶”ê°€: {len(content)}ì")
                                            if streaming_callback is not None:
                                                streaming_callback(content)
                                            break
                        except Exception as e:
                            logger.error(f"âŒ ë¹„ìŠ¤íŠ¸ë¦¬ë° í´ë°± ì‹¤í–‰ ì‹¤íŒ¨: {e}")

                            # ë„êµ¬ ì‚¬ìš© í›„ AI ì‘ë‹µì´ ì—†ìœ¼ë©´ LLMìœ¼ë¡œ ë¶„ì„ ì‹œë„
                            if len(used_tools) > 0 and not accumulated_response.strip():
                                logger.info("ğŸ”§ ìŠ¤íŠ¸ë¦¬ë°: LLM ë¶„ì„ìœ¼ë¡œ í´ë°±...")
                                try:
                                    analyzed_response = await self._analyze_tool_results_with_llm(
                                        user_message, used_tools, tool_results, streaming_callback
                                    )
                                    if analyzed_response and analyzed_response.strip():
                                        accumulated_response = analyzed_response
                                        logger.info(f"âœ… ìŠ¤íŠ¸ë¦¬ë° LLM ë¶„ì„ ì™„ë£Œ: {len(analyzed_response)}ì")
                                    else:
                                        # LLM ë¶„ì„ë„ ì‹¤íŒ¨í•˜ë©´ í¬ë§·íŒ…ëœ ê²°ê³¼ ì‚¬ìš©
                                        formatted_response = self._format_tool_results(used_tools, tool_results)
                                        accumulated_response = formatted_response
                                        if streaming_callback is not None:
                                            streaming_callback(formatted_response)
                                        logger.info(f"âœ… ìŠ¤íŠ¸ë¦¬ë° í¬ë§·íŒ… í´ë°± ì™„ë£Œ: {len(formatted_response)}ì")
                                except Exception as analysis_error:
                                    logger.error(f"âŒ LLM ë¶„ì„ ì‹¤íŒ¨: {analysis_error}")
                                    # ìµœì¢… í´ë°±: í¬ë§·íŒ…ëœ ê²°ê³¼ ì‚¬ìš©
                                    formatted_response = self._format_tool_results(used_tools, tool_results)
                                    accumulated_response = formatted_response
                                    if streaming_callback is not None:
                                        streaming_callback(formatted_response)
                                    logger.info(f"âœ… ìŠ¤íŠ¸ë¦¬ë° ìµœì¢… í´ë°± ì™„ë£Œ: {len(formatted_response)}ì")

                    # ë„êµ¬ë¥¼ ì‚¬ìš©í•˜ì§€ ì•Šì•˜ë”ë¼ë„ íŠ¹ì • í‚¤ì›Œë“œê°€ í¬í•¨ëœ ì§ˆë¬¸ì—ì„œ ì‘ë‹µì´ ì—†ë‹¤ë©´ í´ë°±
                    elif not final_response_found and not accumulated_response.strip():
                        # ë„êµ¬ ê²°ê³¼ê°€ ìˆìœ¼ë©´ í¬ë§·íŒ…í•´ì„œ ì‚¬ìš©
                        if tool_results:
                            logger.info("ğŸ”§ ìŠ¤íŠ¸ë¦¬ë°: ì¼ë°˜ í´ë°±ì—ì„œ ë„êµ¬ ê²°ê³¼ í¬ë§·íŒ…")
                            accumulated_response = self._format_tool_results(used_tools, tool_results)
                            if streaming_callback is not None:
                                streaming_callback(accumulated_response)
                        else:
                            accumulated_response = "ì£„ì†¡í•©ë‹ˆë‹¤. ìš”ì²­ì„ ì²˜ë¦¬í•˜ëŠ” ì¤‘ ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
                            if streaming_callback is not None:
                                streaming_callback(accumulated_response)
                except Exception as e:
                    logger.error(f"âŒ ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                    import traceback

                    logger.error(f"âŒ ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬ ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()}")
                    return {
                        "response": f"ReAct ìŠ¤íŠ¸ë¦¬ë° ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {str(e)}",
                        "used_tools": used_tools,
                    }

                logger.info(
                    f"âœ… ìŠ¤íŠ¸ë¦¬ë° ì™„ë£Œ: {len(accumulated_response)} ë¬¸ì, {len(used_tools)}ê°œ ë„êµ¬ ì‚¬ìš©"
                )

                return {"response": accumulated_response, "used_tools": used_tools}
            else:
                # ë¹„ìŠ¤íŠ¸ë¦¬ë° ëª¨ë“œ
                logger.info("ğŸ“ ë¹„ìŠ¤íŠ¸ë¦¬ë° ëª¨ë“œë¡œ ì‹¤í–‰")
                try:
                    logger.info("ğŸš€ ReAct ainvoke ì‹œì‘...")
                    result = await self.react_agent.ainvoke(inputs, config=config)
                    logger.info(
                        f"âœ… ReAct ainvoke ì™„ë£Œ: type={type(result)}, keys={list(result.keys()) if isinstance(result, dict) else 'N/A'}"
                    )
                except Exception as e:
                    logger.error(f"âŒ ReAct ainvoke ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
                    import traceback

                    logger.error(f"âŒ ReAct ainvoke ì˜¤ë¥˜ ìƒì„¸: {traceback.format_exc()}")
                    return {"response": f"ReAct ainvoke ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {str(e)}", "used_tools": []}

                response_text = ""
                used_tools: List[str] = []
                tool_results: Dict[str, str] = {}

                if "messages" in result:
                    logger.info(f"ğŸ“¬ ê²°ê³¼ ë©”ì‹œì§€ ìˆ˜: {len(result['messages'])}")
                    ai_messages = []
                    tool_messages = []

                    # ë©”ì‹œì§€ ë¶„ë¥˜
                    for i, message in enumerate(result["messages"]):
                        logger.info(
                            f"ğŸ“¨ ê²°ê³¼ ë©”ì‹œì§€ {i}: type={type(message)}, hasattr(content)={hasattr(message, 'content')}"
                        )

                        if str(type(message)).find("AIMessage") != -1:
                            ai_messages.append(message)
                            logger.info(
                                f"ğŸ¤– AI ë©”ì‹œì§€ ë°œê²¬: {len(str(getattr(message, 'content', '')))}ì"
                            )
                        elif str(type(message)).find("ToolMessage") != -1:
                            tool_messages.append(message)
                            logger.info(
                                f"ğŸ”§ ë„êµ¬ ë©”ì‹œì§€ ë°œê²¬: {getattr(message, 'name', 'unknown')}"
                            )
                            if (
                                hasattr(message, "name")
                                and message.name
                                and hasattr(message, "content")
                                and message.content
                            ):
                                tool_results[str(message.name)] = str(message.content)

                    # ë„êµ¬ ì‚¬ìš© ì¶”ì 
                    for tool_msg in tool_messages:
                        if hasattr(tool_msg, "name") and tool_msg.name:
                            tool_name = str(tool_msg.name)
                            if tool_name not in used_tools:
                                used_tools.append(tool_name)
                                logger.info(f"ğŸ”§ ë„êµ¬ ì‚¬ìš© ê¸°ë¡: {tool_name}")

                    # AI ë©”ì‹œì§€ì—ì„œ ìµœì¢… ì‘ë‹µ ì¶”ì¶œ (ë§ˆì§€ë§‰ AI ë©”ì‹œì§€ ìš°ì„ )
                    if ai_messages:
                        # ë§ˆì§€ë§‰ AI ë©”ì‹œì§€ê°€ ìµœì¢… ì‘ë‹µì¼ ê°€ëŠ¥ì„±ì´ ë†’ìŒ
                        for message in reversed(ai_messages):
                            if hasattr(message, "content") and message.content:
                                content = str(message.content).strip()
                                if content:
                                    response_text = content
                                    logger.info(f"ğŸ“ ìµœì¢… AI ì‘ë‹µ ì„ íƒ: {len(content)}ì")
                                    break

                    # ì‘ë‹µì´ ì—†ê³  ë„êµ¬ë¥¼ ì‚¬ìš©í–ˆë‹¤ë©´ ì¬ì‹œë„
                    if not response_text and used_tools:
                        logger.warning("âš ï¸ ë„êµ¬ ì‚¬ìš© í›„ AI ì‘ë‹µì´ ì—†ìŒ - ìƒˆ ì„¸ì…˜ìœ¼ë¡œ ì¬ì‹œë„")
                        try:
                            # ìƒˆë¡œìš´ thread_idë¡œ ì¬ì‹œë„
                            retry_thread_id = str(uuid.uuid4())
                            retry_config = RunnableConfig(
                                recursion_limit=100, configurable={"thread_id": retry_thread_id}
                            )

                            logger.info("ğŸ”„ ìƒˆ ì„¸ì…˜ìœ¼ë¡œ ì¬ì‹œë„...")
                            retry_result = await self.react_agent.ainvoke(
                                inputs, config=retry_config
                            )

                            if "messages" in retry_result:
                                for message in reversed(retry_result["messages"]):
                                    if (
                                        hasattr(message, "content")
                                        and message.content
                                        and str(type(message)).find("AIMessage") != -1
                                    ):
                                        content = str(message.content).strip()
                                        if content:
                                            response_text = content
                                            logger.info(f"ğŸ“ ì¬ì‹œë„ ì„±ê³µ: {len(content)}ì")
                                            break
                        except Exception as e:
                            logger.error(f"âŒ ì¬ì‹œë„ ì‹¤íŒ¨: {e}")

                    # ë„êµ¬ ì‚¬ìš© í›„ AI ì‘ë‹µì´ ì—†ìœ¼ë©´ LLMìœ¼ë¡œ ë¶„ì„ ì‹œë„
                    if not response_text.strip() and len(used_tools) > 0:
                        logger.info("ğŸ”§ LLM ë¶„ì„ìœ¼ë¡œ í´ë°±...")
                        try:
                            analyzed_response = await self._analyze_tool_results_with_llm(
                                user_message, used_tools, tool_results
                            )
                            if analyzed_response and analyzed_response.strip():
                                response_text = analyzed_response
                                logger.info(f"âœ… LLM ë¶„ì„ ì™„ë£Œ: {len(response_text)}ì")
                            else:
                                # LLM ë¶„ì„ë„ ì‹¤íŒ¨í•˜ë©´ í¬ë§·íŒ…ëœ ê²°ê³¼ ì‚¬ìš©
                                response_text = self._format_tool_results(used_tools, tool_results)
                                logger.info(f"âœ… í¬ë§·íŒ… í´ë°± ì™„ë£Œ: {len(response_text)}ì")
                        except Exception as analysis_error:
                            logger.error(f"âŒ LLM ë¶„ì„ ì‹¤íŒ¨: {analysis_error}")
                            # ìµœì¢… í´ë°±: í¬ë§·íŒ…ëœ ê²°ê³¼ ì‚¬ìš©
                            response_text = self._format_tool_results(used_tools, tool_results)
                            logger.info(f"âœ… ìµœì¢… í´ë°± ì™„ë£Œ: {len(response_text)}ì")

                else:
                    logger.warning("âŒ ê²°ê³¼ì— 'messages' í‚¤ê°€ ì—†ìŒ")

                # -------------  ìƒˆ í›„ì²˜ë¦¬: í”Œë ˆì´ìŠ¤í™€ë” ì¹˜í™˜ -------------
                if response_text and tool_results:
                    processed = self._substitute_tool_placeholders(response_text, tool_results)
                    if processed != response_text:
                        logger.info("ğŸ”§ í”Œë ˆì´ìŠ¤í™€ë” ì¹˜í™˜ ì™„ë£Œ")
                        response_text = processed

                logger.info(
                    f"âœ… ë¹„ìŠ¤íŠ¸ë¦¬ë° ì™„ë£Œ: {len(response_text)} ë¬¸ì, {len(used_tools)}ê°œ ë„êµ¬ ì‚¬ìš©"
                )

                return {"response": response_text, "used_tools": used_tools}

        except Exception as e:
            logger.error(f"âŒ ReAct ì—ì´ì „íŠ¸ ì‹¤í–‰ ìµœìƒìœ„ ì˜¤ë¥˜: {e}")
            import traceback

            logger.error(f"âŒ ReAct ì—ì´ì „íŠ¸ ì‹¤í–‰ ìµœìƒìœ„ ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()}")
            return {
                "response": f"ReAct ì—ì´ì „íŠ¸ ì‹¤í–‰ ì¤‘ ìµœìƒìœ„ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}",
                "used_tools": [],
            }

    async def _generate_basic_response(
        self,
        user_message: str,
        streaming_callback: Optional[Callable[[str], None]] = None,
    ) -> str:
        """ê¸°ë³¸ ì‘ë‹µ ìƒì„±"""
        try:
            logger.debug("ê¸°ë³¸ LLM ì„œë¹„ìŠ¤ë¥¼ í†µí•œ ì‘ë‹µ ìƒì„± ì‹œì‘")
            messages = self.conversation_service.get_messages()
            response = await self.llm_service.generate_response(messages, streaming_callback)
            logger.debug(f"ê¸°ë³¸ ì‘ë‹µ ìƒì„± ì™„ë£Œ: {len(response.response)} ë¬¸ì")
            return response.response
        except Exception as e:
            logger.error(f"ê¸°ë³¸ ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
            import traceback

            logger.error(f"ê¸°ë³¸ ì‘ë‹µ ìƒì„± ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()}")
            return f"ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"

    def _get_llm_mode(self) -> str:
        """LLM ëª¨ë“œ ë°˜í™˜"""
        mode = getattr(self.llm_config, "mode", "basic")
        if mode and isinstance(mode, str):
            return mode.lower()
        return "basic"

    def _create_response_data(
        self, response: str, reasoning: str = "", used_tools: Optional[List[str]] = None
    ) -> Dict[str, Any]:
        """ì‘ë‹µ ë°ì´í„° ìƒì„±"""
        if used_tools is None:
            used_tools = []

        # ì–´ì‹œìŠ¤í„´íŠ¸ ë©”ì‹œì§€ ì¶”ê°€
        self.add_assistant_message(response)

        return {"response": response, "reasoning": reasoning, "used_tools": used_tools}

    def _create_error_response(self, error_msg: str, detail: str = "") -> Dict[str, Any]:
        """ì—ëŸ¬ ì‘ë‹µ ìƒì„±"""
        response = f"ì£„ì†¡í•©ë‹ˆë‹¤. {error_msg}"
        self.add_assistant_message(response)

        return {"response": response, "reasoning": detail, "used_tools": []}

    def add_user_message(self, message: str) -> None:
        """ì‚¬ìš©ì ë©”ì‹œì§€ë¥¼ ëŒ€í™” íˆìŠ¤í† ë¦¬ì— ì¶”ê°€"""
        self.conversation_service.add_user_message(message)
        # í•˜ìœ„ í˜¸í™˜ì„±
        self.history.append({"role": "user", "content": message})

    def add_assistant_message(self, message: str) -> None:
        """ì–´ì‹œìŠ¤í„´íŠ¸ ë©”ì‹œì§€ë¥¼ ëŒ€í™” íˆìŠ¤í† ë¦¬ì— ì¶”ê°€"""
        self.conversation_service.add_assistant_message(message)
        # í•˜ìœ„ í˜¸í™˜ì„±
        self.history.append({"role": "assistant", "content": message})

    def clear_conversation(self) -> None:
        """ëŒ€í™” íˆìŠ¤í† ë¦¬ ì´ˆê¸°í™”"""
        self.conversation_service.clear_conversation()
        # í•˜ìœ„ í˜¸í™˜ì„±
        self.history.clear()
        # ìƒˆ thread_id ìƒì„±
        self.thread_id = str(uuid.uuid4())
        logger.info("ëŒ€í™” íˆìŠ¤í† ë¦¬ ì´ˆê¸°í™”")

    def get_conversation_history(self) -> List[Dict[str, str]]:
        """ëŒ€í™” íˆìŠ¤í† ë¦¬ ë°˜í™˜"""
        messages = self.conversation_service.get_messages_as_dict()
        # íƒ€ì… ë³€í™˜ ë³´ì¥
        result: List[Dict[str, str]] = []
        for msg in messages:
            if isinstance(msg, dict):
                result.append(
                    {"role": str(msg.get("role", "")), "content": str(msg.get("content", ""))}
                )
        return result

    async def cleanup(self) -> None:
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        await self.llm_service.cleanup()
        if self.react_agent:
            self.react_agent = None
        logger.info("LLM ì—ì´ì „íŠ¸ ì •ë¦¬ ì™„ë£Œ")

    def reinitialize_client(self) -> None:
        """í´ë¼ì´ì–¸íŠ¸ ì¬ì´ˆê¸°í™” - í”„ë¡œí•„ ë³€ê²½ ì‹œ ì‚¬ìš©"""
        try:
            logger.info("LLM ì—ì´ì „íŠ¸ í´ë¼ì´ì–¸íŠ¸ ì¬ì´ˆê¸°í™” ì‹œì‘")

            # ì„¤ì • ë‹¤ì‹œ ë¡œë“œ
            self._load_config()

            # LLM ì„œë¹„ìŠ¤ ì¬ì´ˆê¸°í™”
            self.llm_service = LLMService(self.llm_config)

            # ReAct ì—ì´ì „íŠ¸ ì¬ì´ˆê¸°í™” í•„ìš”
            self.react_agent = None

            # ëŒ€í™” ì„œë¹„ìŠ¤ ì¬ì´ˆê¸°í™” (íˆìŠ¤í† ë¦¬ëŠ” ìœ ì§€)
            # self.conversation_serviceëŠ” ê·¸ëŒ€ë¡œ ìœ ì§€í•˜ì—¬ ëŒ€í™” ë§¥ë½ ë³´ì¡´

            logger.info(
                f"LLM ì—ì´ì „íŠ¸ ì¬ì´ˆê¸°í™” ì™„ë£Œ: ëª¨ë¸={self.llm_config.model}, ëª¨ë“œ={self.llm_config.mode}"
            )

        except Exception as e:
            logger.error(f"LLM ì—ì´ì „íŠ¸ ì¬ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")

    # ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì € ì§€ì›
    async def __aenter__(self) -> "LLMAgent":
        return self

    async def __aexit__(self, exc_type: Any, exc_val: Any, exc_tb: Any) -> None:
        await self.cleanup()

    def _format_tool_results(self, used_tools: List[str], tool_results: Dict[str, str]) -> str:
        """ë„êµ¬ ê²°ê³¼ë¥¼ LLMì´ ë¶„ì„í•  ìˆ˜ ìˆëŠ” í˜•íƒœë¡œ í¬ë§·íŒ… (ë²”ìš©ì )"""
        try:
            # í”„ë¡œì„¸ì„œ ë ˆì§€ìŠ¤íŠ¸ë¦¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ì²˜ë¦¬
            return self._get_processor_registry().process_tool_results(used_tools, tool_results)
        except Exception as e:
            logger.error(f"ë„êµ¬ ê²°ê³¼ í¬ë§·íŒ… ì˜¤ë¥˜: {e}")
            return "ë„êµ¬ ê²°ê³¼ë¥¼ ì²˜ë¦¬í•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."

    async def _analyze_tool_results_with_llm(
        self,
        user_message: str,
        used_tools: List[str],
        tool_results: Dict[str, str],
        streaming_callback: Optional[Callable[[str], None]] = None,
    ) -> str:
        """ë„êµ¬ ê²°ê³¼ë¥¼ LLMì´ ë¶„ì„í•˜ì—¬ ì‚¬ìš©ì ìš”ì²­ì— ë§ëŠ” ì‘ë‹µ ìƒì„±"""
        try:
            logger.info(f"ğŸ§  LLM ë¶„ì„ ì‹œì‘: {len(used_tools)}ê°œ ë„êµ¬, {user_message[:50]}...")

            # ë„êµ¬ ê²°ê³¼ë¥¼ ë¶„ì„ìš© í”„ë¡¬í”„íŠ¸ì— í¬í•¨
            formatted_results = self._format_tool_results(used_tools, tool_results)
            
            analysis_prompt = f"""ì‚¬ìš©ì ìš”ì²­: {user_message}

ë‹¤ìŒ ë„êµ¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ì •ë³´ë¥¼ ìˆ˜ì§‘í–ˆìŠµë‹ˆë‹¤:
{formatted_results}

ìœ„ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì˜ ìš”ì²­ì— ëŒ€í•´ ì¢…í•©ì ì´ê³  ìœ ìš©í•œ ë‹µë³€ì„ ì œê³µí•´ì£¼ì„¸ìš”:

1. **í•µì‹¬ ë‚´ìš© ì •ë¦¬**: ìˆ˜ì§‘ëœ ì •ë³´ì˜ ì£¼ìš” í¬ì¸íŠ¸
2. **ë¶„ì„ ë° í•´ì„**: ë°ì´í„°ì—ì„œ ë°œê²¬í•œ ì¸ì‚¬ì´íŠ¸ë‚˜ íŒ¨í„´
3. **ë§¥ë½ê³¼ ë°°ê²½**: í•„ìš”ì‹œ ì¶”ê°€ ì„¤ëª…ì´ë‚˜ ì—°ê´€ì„± ì œê³µ
4. **ì¶œì²˜ ë° ì‹ ë¢°ì„±**: ì¤‘ìš”í•œ ì •ë³´ì˜ ì¶œì²˜ ëª…ì‹œ

ì‚¬ìš©ìê°€ ì´í•´í•˜ê¸° ì‰½ê³  ì‹¤ìš©ì ì¸ í•œêµ­ì–´ ë‹µë³€ì„ ì œê³µí•´ì£¼ì„¸ìš”."""

            # ìƒˆë¡œìš´ ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ë¡œ LLM í˜¸ì¶œ
            from application.llm.models.conversation_message import ConversationMessage
            
            temp_messages = [
                ConversationMessage(role="user", content=analysis_prompt)
            ]
            
            logger.info(f"ğŸ”„ LLM ë¶„ì„ ìš”ì²­: {len(analysis_prompt)}ì í”„ë¡¬í”„íŠ¸")
            
            response = await self.llm_service.generate_response(temp_messages, streaming_callback)
            analysis_result = response.response.strip()
            
            if analysis_result:
                logger.info(f"âœ… LLM ë¶„ì„ ì„±ê³µ: {len(analysis_result)}ì")
                return analysis_result
            else:
                logger.warning("âš ï¸ LLM ë¶„ì„ ê²°ê³¼ê°€ ë¹„ì–´ìˆìŒ")
                return self._format_tool_results(used_tools, tool_results)
                
        except Exception as e:
            logger.error(f"âŒ LLM ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {e}")
            import traceback
            logger.error(f"âŒ LLM ë¶„ì„ ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()}")
            # ì˜¤ë¥˜ ì‹œ ê¸°ë³¸ í¬ë§·íŒ… ë°˜í™˜
            return self._format_tool_results(used_tools, tool_results)

    def _substitute_tool_placeholders(self, text: str, tool_results: Dict[str, str]) -> str:
        """AI ì‘ë‹µ ì•ˆì˜ `default_api.xxx()` ì‹ í”Œë ˆì´ìŠ¤í™€ë”ë¥¼ ì‹¤ì œ ë„êµ¬ ê²°ê³¼ë¡œ ì¹˜í™˜"""
        import json
        import re

        out = str(text)
        for tool_name, raw in tool_results.items():
            try:
                data = json.loads(raw)
                result_str = data.get("result", raw)
            except Exception:
                result_str = raw

            # ë°±í‹± í¬í•¨ íŒ¨í„´ ë° í•¨ìˆ˜í˜¸ì¶œ íŒ¨í„´ ì¹˜í™˜
            patterns = [
                rf"`[^`]*{tool_name}\([^`]*`",  # `default_api.get_current_weather(...)`
                rf"{tool_name}\([^)]*\)",  # get_current_weather(...)
            ]
            for pat in patterns:
                out = re.sub(pat, result_str, out)

        return out

    def _get_processor_registry(self) -> ToolResultProcessorRegistry:
        """í”„ë¡œì„¸ì„œ ë ˆì§€ìŠ¤íŠ¸ë¦¬ë¥¼ ê°€ì ¸ì˜¤ê±°ë‚˜ ìƒì„±í•©ë‹ˆë‹¤ (ì§€ì—° ì´ˆê¸°í™”)"""
        if self.processor_registry is None:
            self.processor_registry = ToolResultProcessorRegistry()
            # ê²€ìƒ‰ í”„ë¡œì„¸ì„œ ë“±ë¡
            self.processor_registry.register(SearchToolResultProcessor())
            logger.debug("ë„êµ¬ ê²°ê³¼ í”„ë¡œì„¸ì„œ ë ˆì§€ìŠ¤íŠ¸ë¦¬ ì´ˆê¸°í™” ì™„ë£Œ")
        return self.processor_registry
