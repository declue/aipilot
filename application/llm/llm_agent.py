"""
Langchain ê¸°ë°˜ LLM ì—ì´ì „íŠ¸
"""

import logging
from typing import Any, Callable, Dict, List, Optional

from application.llm.interfaces.llm_interface import LLMInterface
from application.llm.models.llm_config import LLMConfig
from application.llm.services.conversation_service import ConversationService
from application.llm.services.llm_service import LLMService
from application.llm.workflow.workflow_utils import get_workflow
from application.util.logger import setup_logger

logger = setup_logger("llm_agent") or logging.getLogger("llm_agent")


class LLMAgent(LLMInterface):
    """Langchain ê¸°ë°˜ LLM ì—ì´ì „íŠ¸"""

    def __init__(self, config_manager, mcp_tool_manager=None):
        """
        LLM ì—ì´ì „íŠ¸ ì´ˆê¸°í™”
        
        Args:
            config_manager: ì„¤ì • ê´€ë¦¬ì
            mcp_tool_manager: MCP ë„êµ¬ ê´€ë¦¬ì (ì„ íƒì‚¬í•­)
        """
        self.config_manager = config_manager
        self.mcp_tool_manager = mcp_tool_manager
        
        # ì„¤ì • ë¡œë“œ
        self._load_config()
        
        # ì„œë¹„ìŠ¤ ì´ˆê¸°í™”
        self.llm_service = LLMService(self.llm_config)
        self.conversation_service = ConversationService()
        
        # íˆìŠ¤í† ë¦¬ (í•˜ìœ„ í˜¸í™˜ì„±)
        self.history = []
        
        logger.info("LLM ì—ì´ì „íŠ¸ ì´ˆê¸°í™” ì™„ë£Œ")

    def _load_config(self) -> None:
        """ì„¤ì • ë¡œë“œ"""
        try:
            # í”„ë¡œí•„ ê¸°ë°˜ ì„¤ì • ë¡œë“œ (modeì™€ workflow í¬í•¨)
            llm_config_dict = self.config_manager.get_llm_config()
            
            self.llm_config = LLMConfig.from_dict(llm_config_dict)
            logger.debug(f"LLM ì„¤ì • ë¡œë“œ ì™„ë£Œ: {self.llm_config.model}, ëª¨ë“œ: {self.llm_config.mode}")
            
        except Exception as e:
            logger.error(f"ì„¤ì • ë¡œë“œ ì‹¤íŒ¨: {e}")
            # ê¸°ë³¸ ì„¤ì •ìœ¼ë¡œ í´ë°±
            self.llm_config = LLMConfig(
                api_key="",
                model="gpt-3.5-turbo",
                mode="basic"
            )

    async def generate_response(
        self,
        user_message: str,
        streaming_callback: Optional[Callable[[str], None]] = None,
    ) -> Dict[str, Any]:
        """
        ì‚¬ìš©ì ë©”ì‹œì§€ì— ëŒ€í•œ ì‘ë‹µ ìƒì„±
        
        Args:
            user_message: ì‚¬ìš©ì ì…ë ¥ ë©”ì‹œì§€
            streaming_callback: ìŠ¤íŠ¸ë¦¬ë° ì½œë°± í•¨ìˆ˜
            
        Returns:
            Dict[str, Any]: ì‘ë‹µ ë°ì´í„°
        """
        try:
            logger.info(f"LLM ì‘ë‹µ ìƒì„± ì‹œì‘: {user_message[:50]}...")
            
            # ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€
            self.add_user_message(user_message)
            
            # ëª¨ë“œì— ë”°ë¥¸ ì²˜ë¦¬
            mode = self._get_llm_mode()
            
            if mode == "workflow":
                return await self._handle_workflow_mode(user_message, streaming_callback)
            elif mode == "mcp_tools" and self.mcp_tool_manager:
                return await self._handle_mcp_tools_mode(user_message, streaming_callback)
            else:
                return await self._handle_basic_mode(user_message, streaming_callback)
                
        except Exception as e:
            logger.error(f"ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
            return self._create_error_response("ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤", str(e))

    async def generate_response_streaming(
        self,
        user_message: str,
        streaming_callback: Optional[Callable[[str], None]] = None,
    ) -> Dict[str, Any]:
        """ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ìƒì„± (í•˜ìœ„ í˜¸í™˜ì„±)"""
        return await self.generate_response(user_message, streaming_callback)

    async def _handle_basic_mode(
        self,
        user_message: str,
        streaming_callback: Optional[Callable[[str], None]] = None,
    ) -> Dict[str, Any]:
        """ê¸°ë³¸ ëª¨ë“œ ì²˜ë¦¬"""
        try:
            response = await self._generate_basic_response(user_message, streaming_callback)
            return self._create_response_data(response)
        except Exception as e:
            logger.error(f"ê¸°ë³¸ ëª¨ë“œ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
            return self._create_error_response("ê¸°ë³¸ ëª¨ë“œ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤", str(e))

    async def _handle_workflow_mode(
        self,
        user_message: str,
        streaming_callback: Optional[Callable[[str], None]] = None,
    ) -> Dict[str, Any]:
        """ì›Œí¬í”Œë¡œìš° ëª¨ë“œ ì²˜ë¦¬"""
        try:
            workflow_name = self.llm_config.workflow or "basic_chat"
            workflow_class = get_workflow(workflow_name)
            workflow = workflow_class()
            
            result = await workflow.run(self, user_message, streaming_callback)
            
            return {
                "response": result,
                "workflow": workflow_name,
                "reasoning": "",
                "used_tools": []
            }
            
        except Exception as e:
            logger.error(f"ì›Œí¬í”Œë¡œìš° ëª¨ë“œ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
            return {
                "response": "ì›Œí¬í”Œë¡œìš° ì²˜ë¦¬ ì¤‘ ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.",
                "workflow": self.llm_config.workflow or "basic_chat",
                "reasoning": str(e),
                "used_tools": []
            }

    async def _handle_mcp_tools_mode(
        self,
        user_message: str,
        streaming_callback: Optional[Callable[[str], None]] = None,
    ) -> Dict[str, Any]:
        """MCP ë„êµ¬ ëª¨ë“œ ì²˜ë¦¬ - Langchain Agent íŒ¨í„´ ì‚¬ìš©"""
        try:
            if not self.mcp_tool_manager:
                return self._create_error_response("MCP ë„êµ¬ ê´€ë¦¬ìê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
            
            # Langchain Agentë¥¼ ì‚¬ìš©í•œ ë„êµ¬ ê¸°ë°˜ ì‘ë‹µ ìƒì„±
            result = await self._run_langchain_agent_with_tools(user_message, streaming_callback)
            
            return {
                "response": result.get("response", ""),
                "reasoning": result.get("reasoning", ""),
                "used_tools": result.get("used_tools", [])
            }
            
        except Exception as e:
            logger.error(f"MCP ë„êµ¬ ëª¨ë“œ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
            return self._create_error_response("MCP ë„êµ¬ ëª¨ë“œ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤", str(e))

    async def _run_langchain_agent_with_tools(
        self,
        user_message: str,
        streaming_callback: Optional[Callable[[str], None]] = None,
    ) -> Dict[str, Any]:
        """ì§„ì •í•œ Langchain MCP Agentë¥¼ ì‚¬ìš©í•œ ë„êµ¬ ê¸°ë°˜ ì‘ë‹µ ìƒì„±"""
        # JSON ìŠ¤í‚¤ë§ˆ ì´ìŠˆë¡œ ì¸í•´ ì¼ì‹œì ìœ¼ë¡œ ê°„ë‹¨í•œ ë°©ì‹ ì‚¬ìš©
        logger.info("ğŸ”§ Langchain Agent JSON ìŠ¤í‚¤ë§ˆ ì´ìŠˆë¡œ ì¸í•´ ê°„ë‹¨í•œ MCP ë°©ì‹ ì‚¬ìš©")
        return await self._fallback_to_simple_mcp_approach(user_message, streaming_callback)

    async def _fallback_to_simple_mcp_approach(
        self,
        user_message: str,
        streaming_callback: Optional[Callable[[str], None]] = None,
    ) -> Dict[str, Any]:
        """ê°„ë‹¨í•œ MCP ì ‘ê·¼ë²•ìœ¼ë¡œ í´ë°±"""
        try:
            # ì‚¬ìš©ì ë©”ì‹œì§€ì—ì„œ í•„ìš”í•œ ë„êµ¬ íŒŒì•… ë° ì‹¤í–‰
            tool_results = await self._execute_relevant_tools(user_message)
            
            # ë„êµ¬ ê²°ê³¼ë¥¼ í¬í•¨í•œ í”„ë¡¬í”„íŠ¸ë¡œ LLMì—ê²Œ ìµœì¢… ë‹µë³€ ìš”ì²­
            if tool_results["used_tools"]:
                enhanced_prompt = self._create_enhanced_prompt_with_tools(user_message, tool_results)
                final_response = await self._generate_basic_response(enhanced_prompt, streaming_callback)
                
                return {
                    "response": final_response,
                    "reasoning": "Simple MCP approach with tool results",
                    "used_tools": tool_results["used_tools"]
                }
            else:
                return await self._fallback_to_basic_response(user_message, streaming_callback)
            
        except Exception as e:
            logger.error(f"Simple MCP approach ì‹¤íŒ¨: {e}")
            return await self._fallback_to_basic_response(user_message, streaming_callback)

    async def _execute_relevant_tools(self, user_message: str) -> Dict[str, Any]:
        """ì‚¬ìš©ì ë©”ì‹œì§€ì—ì„œ ê´€ë ¨ ë„êµ¬ ì‹¤í–‰"""
        try:
            # Langchain ë„êµ¬ ê°€ì ¸ì˜¤ê¸°
            langchain_tools = await self.mcp_tool_manager.get_langchain_tools()
            
            if not langchain_tools:
                logger.warning("ì‚¬ìš© ê°€ëŠ¥í•œ MCP ë„êµ¬ê°€ ì—†ìŠµë‹ˆë‹¤")
                return {"response": "", "reasoning": "ì‚¬ìš© ê°€ëŠ¥í•œ ë„êµ¬ ì—†ìŒ", "used_tools": []}
            
            logger.info(f"ğŸ” ë„êµ¬ ì‹¤í–‰ ë¶„ì„: '{user_message}' -> {len(langchain_tools)}ê°œ ë„êµ¬ ì‚¬ìš© ê°€ëŠ¥")
            
            # ê°„ë‹¨í•œ í‚¤ì›Œë“œ ê¸°ë°˜ ë„êµ¬ ì‹¤í–‰
            message_lower = user_message.lower()
            used_tools = []
            responses = []
            
            # ì‹œê°„ ê´€ë ¨ ìš”ì²­
            time_keywords = ["ì‹œê°„", "time", "í˜„ì¬", "ì§€ê¸ˆ"]
            time_match = any(keyword in message_lower for keyword in time_keywords)
            logger.info(f"ğŸ• ì‹œê°„ í‚¤ì›Œë“œ ë§¤ì¹­: {time_match} (í‚¤ì›Œë“œ: {time_keywords})")
            
            if time_match:
                logger.info("ğŸ• ì‹œê°„ ê´€ë ¨ ë„êµ¬ ê²€ìƒ‰ ì¤‘...")
                for tool in langchain_tools:
                    logger.debug(f"  - ë„êµ¬ í™•ì¸: {tool.name}")
                    if "time" in tool.name.lower() and "current" in tool.name.lower():
                        try:
                            logger.info(f"ğŸ”§ ì‹œê°„ ë„êµ¬ ì‹¤í–‰: {tool.name}")
                            result = await tool.ainvoke({})
                            logger.info(f"âœ… ì‹œê°„ ë„êµ¬ ê²°ê³¼: {result}")
                            responses.append(str(result))
                            used_tools.append(tool.name)
                            break
                        except Exception as e:
                            logger.error(f"âŒ ë„êµ¬ {tool.name} ì‹¤í–‰ ì‹¤íŒ¨: {e}")
            
            # ë‚ ì”¨ ê´€ë ¨ ìš”ì²­
            weather_keywords = ["ë‚ ì”¨", "weather", "ê¸°ì˜¨", "ì˜¨ë„"]
            weather_match = any(keyword in message_lower for keyword in weather_keywords)
            logger.info(f"ğŸŒ¤ï¸ ë‚ ì”¨ í‚¤ì›Œë“œ ë§¤ì¹­: {weather_match} (í‚¤ì›Œë“œ: {weather_keywords})")
            
            if weather_match:
                city = "Seoul"  # ê¸°ë³¸ê°’
                # ë„ì‹œëª… ì¶”ì¶œ (ê°„ë‹¨í•œ ë°©ì‹)
                for word in user_message.split():
                    if word in ["ì„œìš¸", "Seoul", "ë¶€ì‚°", "Busan", "ë„ì¿„", "Tokyo"]:
                        city = word
                        break
                
                logger.info(f"ğŸŒ¤ï¸ ë‚ ì”¨ ê´€ë ¨ ë„êµ¬ ê²€ìƒ‰ ì¤‘... (ë„ì‹œ: {city})")
                for tool in langchain_tools:
                    logger.debug(f"  - ë„êµ¬ í™•ì¸: {tool.name}")
                    if "weather" in tool.name.lower() and "current" in tool.name.lower():
                        try:
                            logger.info(f"ğŸ”§ ë‚ ì”¨ ë„êµ¬ ì‹¤í–‰: {tool.name} (city={city})")
                            result = await tool.ainvoke({"city": city})
                            logger.info(f"âœ… ë‚ ì”¨ ë„êµ¬ ê²°ê³¼: {result}")
                            responses.append(str(result))
                            used_tools.append(tool.name)
                            break
                        except Exception as e:
                            logger.error(f"âŒ ë„êµ¬ {tool.name} ì‹¤í–‰ ì‹¤íŒ¨: {e}")
            
            if responses:
                logger.info(f"âœ… ë„êµ¬ ì‹¤í–‰ ì™„ë£Œ: {len(used_tools)}ê°œ ë„êµ¬ ì‚¬ìš©")
                return {
                    "response": "\n\n".join(responses),
                    "reasoning": f"ë„êµ¬ {len(used_tools)}ê°œ ì‹¤í–‰",
                    "used_tools": used_tools
                }
            else:
                logger.warning("âš ï¸ ì‹¤í–‰ëœ ë„êµ¬ê°€ ì—†ìŠµë‹ˆë‹¤")
                return {
                    "response": "",
                    "reasoning": "ì‹¤í–‰í•  ë„êµ¬ ì—†ìŒ",
                    "used_tools": []
                }
                
        except Exception as e:
            logger.error(f"âŒ ê´€ë ¨ ë„êµ¬ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
            return {"response": "", "reasoning": str(e), "used_tools": []}

    def _create_enhanced_prompt_with_tools(self, user_message: str, tool_results: Dict[str, Any]) -> str:
        """ë„êµ¬ ê²°ê³¼ë¥¼ í¬í•¨í•œ í–¥ìƒëœ í”„ë¡¬í”„íŠ¸ ìƒì„±"""
        tool_info = ""
        if tool_results.get("used_tools"):
            tool_info = f"\n\në„êµ¬ ì‹¤í–‰ ê²°ê³¼:\n{tool_results.get('response', '')}\n"
        
        enhanced_prompt = f"""ì‚¬ìš©ì ì§ˆë¬¸: {user_message}
{tool_info}
ìœ„ì˜ ë„êµ¬ ì‹¤í–‰ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ ì •í™•í•˜ê³  ìœ ìš©í•œ ë‹µë³€ì„ ì œê³µí•´ì£¼ì„¸ìš”.

íŠ¹ë³„ ìš”ì²­ì‚¬í•­:
- ì‹œê°„ ê´€ë ¨ ì§ˆë¬¸ì˜ ê²½ìš°: ì‹œê°„ ê³„ì‚°, í¬ë§·íŒ…, ì¶”ê°€ì ì¸ ì •ë³´ ì œê³µ
- ë‚ ì”¨ ê´€ë ¨ ì§ˆë¬¸ì˜ ê²½ìš°: í‘œ í˜•íƒœë‚˜ êµ¬ì¡°í™”ëœ í˜•íƒœë¡œ ì •ë³´ ì •ë¦¬
- ë³µí•© ì§ˆë¬¸ì˜ ê²½ìš°: ì—¬ëŸ¬ ì •ë³´ë¥¼ ì¢…í•©í•˜ì—¬ ì™„ì „í•œ ë‹µë³€ ì œê³µ

í•­ìƒ í•œêµ­ì–´ë¡œ ì¹œì ˆí•˜ê³  ìì„¸í•˜ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”."""

        return enhanced_prompt

    async def _fallback_to_basic_response(
        self,
        user_message: str,
        streaming_callback: Optional[Callable[[str], None]] = None,
    ) -> Dict[str, Any]:
        """ë„êµ¬ê°€ ì—†ì„ ë•Œ ê¸°ë³¸ ì‘ë‹µìœ¼ë¡œ í´ë°±"""
        try:
            response = await self._generate_basic_response(user_message, streaming_callback)
            return {
                "response": response,
                "reasoning": "ë„êµ¬ ì—†ì´ ê¸°ë³¸ ì‘ë‹µ ìƒì„±",
                "used_tools": []
            }
        except Exception as e:
            logger.error(f"ê¸°ë³¸ ì‘ë‹µ í´ë°± ì‹¤íŒ¨: {e}")
            return {
                "response": "ì£„ì†¡í•©ë‹ˆë‹¤. ì‘ë‹µ ìƒì„± ì¤‘ ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.",
                "reasoning": str(e),
                "used_tools": []
            }

    async def _fallback_to_mcp_tools(self, user_message: str) -> Dict[str, Any]:
        """Langchain Agent ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ MCP ë„êµ¬ ì‚¬ìš©ìœ¼ë¡œ í´ë°±"""
        try:
            result = await self.mcp_tool_manager.run_agent_with_tools(user_message)
            return {
                "response": result.get("response", ""),
                "reasoning": "Langchain Agent ì‹¤íŒ¨ë¡œ ê¸°ë³¸ MCP ë„êµ¬ ì‚¬ìš©",
                "used_tools": result.get("used_tools", [])
            }
        except Exception as e:
            logger.error(f"MCP ë„êµ¬ í´ë°±ë„ ì‹¤íŒ¨: {e}")
            return {
                "response": "ì£„ì†¡í•©ë‹ˆë‹¤. ë„êµ¬ ì‚¬ìš© ì¤‘ ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.",
                "reasoning": str(e),
                "used_tools": []
            }

    async def _generate_basic_response(
        self,
        user_message: str,
        streaming_callback: Optional[Callable[[str], None]] = None,
    ) -> str:
        """ê¸°ë³¸ ì‘ë‹µ ìƒì„±"""
        try:
            messages = self.conversation_service.get_messages()
            response = await self.llm_service.generate_response(messages, streaming_callback)
            return response.response
        except Exception as e:
            logger.error(f"ê¸°ë³¸ ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
            return f"ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"

    def _get_llm_mode(self) -> str:
        """LLM ëª¨ë“œ ë°˜í™˜"""
        mode = self.llm_config.mode
        if mode and isinstance(mode, str):
            return mode.lower()
        return "basic"

    def _create_response_data(
        self,
        response: str,
        reasoning: str = "",
        used_tools: List[str] = None
    ) -> Dict[str, Any]:
        """ì‘ë‹µ ë°ì´í„° ìƒì„±"""
        if used_tools is None:
            used_tools = []
            
        # ì–´ì‹œìŠ¤í„´íŠ¸ ë©”ì‹œì§€ ì¶”ê°€
        self.add_assistant_message(response)
        
        return {
            "response": response,
            "reasoning": reasoning,
            "used_tools": used_tools
        }

    def _create_error_response(self, error_msg: str, detail: str = "") -> Dict[str, Any]:
        """ì—ëŸ¬ ì‘ë‹µ ìƒì„±"""
        response = f"ì£„ì†¡í•©ë‹ˆë‹¤. {error_msg}"
        self.add_assistant_message(response)
        
        return {
            "response": response,
            "reasoning": detail,
            "used_tools": []
        }

    def add_user_message(self, message: str) -> None:
        """ì‚¬ìš©ì ë©”ì‹œì§€ë¥¼ ëŒ€í™” íˆìŠ¤í† ë¦¬ì— ì¶”ê°€"""
        self.conversation_service.add_user_message(message)
        # í•˜ìœ„ í˜¸í™˜ì„±
        self.history.append({"role": "user", "content": message})

    def add_assistant_message(self, message: str) -> None:
        """ì–´ì‹œìŠ¤í„´íŠ¸ ë©”ì‹œì§€ë¥¼ ëŒ€í™” íˆìŠ¤í† ë¦¬ì— ì¶”ê°€"""
        self.conversation_service.add_assistant_message(message)
        # í•˜ìœ„ í˜¸í™˜ì„±
        self.history.append({"role": "assistant", "content": message})

    def clear_conversation(self) -> None:
        """ëŒ€í™” íˆìŠ¤í† ë¦¬ ì´ˆê¸°í™”"""
        self.conversation_service.clear_conversation()
        # í•˜ìœ„ í˜¸í™˜ì„±
        self.history.clear()
        logger.info("ëŒ€í™” íˆìŠ¤í† ë¦¬ ì´ˆê¸°í™”")

    def get_conversation_history(self) -> List[Dict[str, str]]:
        """ëŒ€í™” íˆìŠ¤í† ë¦¬ ë°˜í™˜"""
        return self.conversation_service.get_messages_as_dict()

    async def cleanup(self) -> None:
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        await self.llm_service.cleanup()
        logger.info("LLM ì—ì´ì „íŠ¸ ì •ë¦¬ ì™„ë£Œ")

    def reinitialize_client(self) -> None:
        """í´ë¼ì´ì–¸íŠ¸ ì¬ì´ˆê¸°í™” - í”„ë¡œí•„ ë³€ê²½ ì‹œ ì‚¬ìš©"""
        try:
            logger.info("LLM ì—ì´ì „íŠ¸ í´ë¼ì´ì–¸íŠ¸ ì¬ì´ˆê¸°í™” ì‹œì‘")
            
            # ì„¤ì • ë‹¤ì‹œ ë¡œë“œ
            self._load_config()
            
            # LLM ì„œë¹„ìŠ¤ ì¬ì´ˆê¸°í™”
            self.llm_service = LLMService(self.llm_config)
            
            # ëŒ€í™” ì„œë¹„ìŠ¤ ì¬ì´ˆê¸°í™” (íˆìŠ¤í† ë¦¬ëŠ” ìœ ì§€)
            # self.conversation_serviceëŠ” ê·¸ëŒ€ë¡œ ìœ ì§€í•˜ì—¬ ëŒ€í™” ë§¥ë½ ë³´ì¡´
            
            logger.info(f"LLM ì—ì´ì „íŠ¸ ì¬ì´ˆê¸°í™” ì™„ë£Œ: ëª¨ë¸={self.llm_config.model}, ëª¨ë“œ={self.llm_config.mode}")
            
        except Exception as e:
            logger.error(f"LLM ì—ì´ì „íŠ¸ ì¬ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")

    # ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì € ì§€ì›
    async def __aenter__(self):
        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        await self.cleanup() 