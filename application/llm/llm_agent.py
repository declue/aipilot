from __future__ import annotations

import logging
import re
from typing import Any, Callable, Dict, List, Optional, cast

from openai import AsyncOpenAI
from openai.types.chat import ChatCompletionMessageParam

from application.config.config_manager import ConfigManager
from application.llm.mcp.mcp_tool_manager import MCPToolManager
from application.util.logger import setup_logger

logger: logging.Logger = setup_logger("llm") or logging.getLogger("llm_agent")
# ë””ë²„ê¹…ì„ ìœ„í•œ ë¡œê·¸ ë ˆë²¨ INFO ì„¤ì •
logger.setLevel(logging.INFO)

# ---------------------------------------------------------------------------
# Small helpers
# ---------------------------------------------------------------------------


def _is_reasoning_model(model: str) -> bool:
    """Heuristic check for models that emit visible chainâ€‘ofâ€‘thought."""
    reasoning_names = (
        "o1",
        "claude-3-5",
        "deepseek-r1",
        "qwen-qvq",
        "qwen3:32b-q8_0",
        "gemini-2.5-pro-preview-06-05",
        "deepseek-chat",
        "llama-3.3-70b-reasoning",
    )
    return any(name in model.lower() for name in reasoning_names)


def _strip_reasoning(raw: str) -> str:
    """Drop `<think>` â€¦ `</think>` or similar sections, keep the answer only."""
    if "</think>" in raw:
        return raw.split("</think>")[-1].strip()
    for pat in (
        r"<thinking>[\s\S]*?</thinking>",
        r"<thought>[\s\S]*?</thought>",
        r"ì¶”ë¡  ê³¼ì •:[\s\S]*?(?=ë‹µë³€:|$)",
    ):
        raw = re.sub(pat, "", raw, flags=re.I)
    return raw.strip()


# ---------------------------------------------------------------------------
# Core class
# ---------------------------------------------------------------------------


class LLMAgent:
    """MCPToolManagerë¥¼ ì‚¬ìš©í•˜ëŠ” LLM ì—ì´ì „íŠ¸"""

    def __init__(self, config_manager: ConfigManager, mcp_tool_manager: MCPToolManager):
        self.config_manager = config_manager
        self.history: List[ChatCompletionMessageParam] = []
        self._client: Optional[AsyncOpenAI] = None
        self.mcp_tool_manager = mcp_tool_manager

    def reinitialize_client(self) -> None:
        """ì„¤ì • ë³€ê²½ ì‹œ í´ë¼ì´ì–¸íŠ¸ë¥¼ ì¬ì´ˆê¸°í™”í•©ë‹ˆë‹¤."""
        self._client = None

        # ìƒˆë¡œìš´ ì„¤ì • í™•ì¸ì„ ìœ„í•œ ë¡œê·¸
        try:
            cfg = self.config_manager.get_llm_config()
            logger.info(
                f"LLM Agent í´ë¼ì´ì–¸íŠ¸ ì¬ì´ˆê¸°í™”: ëª¨ë¸={cfg.get('model')}, base_url={cfg.get('base_url')}"
            )
        except Exception as e:
            logger.error(f"LLM Agent ì„¤ì • ë¡œë“œ ì‹¤íŒ¨: {e}")

        logger.info("LLM Agent í´ë¼ì´ì–¸íŠ¸ê°€ ì¬ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")

    @property
    def client(self) -> AsyncOpenAI:
        """OpenAI í´ë¼ì´ì–¸íŠ¸ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
        if not self._client:
            cfg = self.config_manager.get_llm_config()
            self._client = AsyncOpenAI(
                api_key=cfg["api_key"], base_url=cfg["base_url"], timeout=300.0
            )
        return self._client

    @staticmethod
    async def test_connection(
        api_key: str, base_url: str, model: str
    ) -> Dict[str, Any]:
        """LLM ì„œë²„ ì—°ê²° í…ŒìŠ¤íŠ¸"""
        try:
            client = AsyncOpenAI(api_key=api_key, base_url=base_url, timeout=300.0)
            response = await client.chat.completions.create(
                model=model,
                messages=[{"role": "user", "content": "Hello"}],
                max_tokens=10,
            )

            return {
                "success": True,
                "message": "ì—°ê²° ì„±ê³µ",
                "response": response.choices[0].message.content or "",
                "model": model,
            }

        except Exception as exception:
            return {
                "success": False,
                "message": f"ì—°ê²° ì‹¤íŒ¨: {str(exception)}",
                "error": str(exception),
            }

    @staticmethod
    async def get_available_models(api_key: str, base_url: str) -> Dict[str, Any]:
        """ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°"""
        try:
            client = AsyncOpenAI(api_key=api_key, base_url=base_url, timeout=300.0)
            models_response = await client.models.list()

            models = []
            async for model in models_response:
                models.append(model.id)
            models.sort()

            return {
                "success": True,
                "models": models,
                "message": f"{len(models)}ê°œ ëª¨ë¸ì„ ì°¾ì•˜ìŠµë‹ˆë‹¤",
            }

        except Exception as exception:
            return {
                "success": False,
                "models": [],
                "message": f"ëª¨ë¸ ëª©ë¡ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {str(exception)}",
                "error": str(exception),
            }

    def add_user_message(self, text: str) -> None:
        self.history.append({"role": "user", "content": text})

    def add_assistant_message(self, text: str) -> None:
        self.history.append({"role": "assistant", "content": text})

    def clear_conversation(self) -> None:
        self.history.clear()

    async def generate_response(self, user_message: str) -> str:
        result = await self._respond(user_message)
        return cast(str, result["response"])

    async def generate_response_streaming(
        self, user_message: str, streaming_callback: Optional[Callable[[str], None]]
    ) -> Dict[str, Any]:
        return await self._respond(user_message, streaming_callback)

    async def _respond(
        self, user_msg: str, streaming_cb: Optional[Callable[[str], None]] = None
    ) -> Dict[str, Any]:
        """ì‚¬ìš©ì ë©”ì‹œì§€ì— ëŒ€í•œ ì‘ë‹µì„ ìƒì„±í•©ë‹ˆë‹¤."""
        logger.info(f"ì‚¬ìš©ì ë©”ì‹œì§€: {user_msg}")
        self.add_user_message(user_msg)
        response_data = {}

        # ------------------------------------------------------------------
        # 1) LLM Workflow ëª¨ë“œ ìš°ì„  ì²˜ë¦¬
        # ------------------------------------------------------------------
        llm_mode: str = (
            self.config_manager.get_config_value("LLM", "mode", "basic") or "basic"
        ).lower()

        if llm_mode == "workflow":
            # ì›Œí¬í”Œë¡œìš° ì´ë¦„ ê°€ì ¸ì˜¤ê¸° (ì—†ìœ¼ë©´ basic_chat)
            workflow_name: str = (
                self.config_manager.get_config_value("LLM", "workflow", "basic_chat")
                or "basic_chat"
            )

            try:
                from application.llm.workflow import (
                    get_workflow,  # pylint: disable=import-outside-toplevel
                )

                workflow_cls = get_workflow(workflow_name)
                if workflow_cls is None:
                    logger.warning(
                        "ì›Œí¬í”Œë¡œìš° '%s' ì„ ì°¾ì„ ìˆ˜ ì—†ì–´ ê¸°ë³¸ ì›Œí¬í”Œë¡œìš°ë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤.",
                        workflow_name,
                    )
                    workflow_cls = get_workflow("basic_chat")

                assert (
                    workflow_cls is not None
                ), "ê¸°ë³¸ ì›Œí¬í”Œë¡œìš°ê°€ ë ˆì§€ìŠ¤íŠ¸ë¦¬ì— ë“±ë¡ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤"

                workflow = workflow_cls()
                response_text: str = await workflow.run(self, user_msg, streaming_cb)
                self.add_assistant_message(response_text)
                return {
                    "response": response_text,
                    "reasoning": "",  # ì¶”í›„ ì›Œí¬í”Œë¡œìš° ì„¸ë¶€ reasoning ì¶”ê°€ ê°€ëŠ¥
                    "used_tools": [],
                    "workflow": workflow_name,
                }

            except Exception as exc:  # pylint: disable=broad-except
                logger.error("ì›Œí¬í”Œë¡œìš° ì‹¤í–‰ ì¤‘ ì˜ˆì™¸ ë°œìƒ: %s", exc)
                fallback_response = "ì£„ì†¡í•©ë‹ˆë‹¤. ì›Œí¬í”Œë¡œìš° ì²˜ë¦¬ ì¤‘ ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
                self.add_assistant_message(fallback_response)
                return {
                    "response": fallback_response,
                    "reasoning": str(exc),
                    "used_tools": [],
                }

        # MCPToolManagerë¥¼ ì‚¬ìš©í•˜ëŠ” ê²½ìš°
        if self.mcp_tool_manager:
            try:
                # ë„êµ¬ê°€ í•„ìš”í•œì§€ í™•ì¸
                if await self._should_use_tools(user_msg):
                    # MCP ë„êµ¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ì‘ë‹µ ìƒì„±
                    openai_tools = await self.mcp_tool_manager.get_openai_tools()

                    if openai_tools:
                        tool_result = await self._generate_with_tools(
                            user_msg, openai_tools, streaming_cb
                        )
                        response_data = {
                            "response": tool_result.get("response", ""),
                            "reasoning": tool_result.get("reasoning", ""),
                            "used_tools": [],  # ì¶”í›„ í™•ì¥
                        }
                    else:
                        response_text = await self._generate_basic_response(
                            user_msg, streaming_cb
                        )
                        response_data = {
                            "response": response_text,
                            "reasoning": "",
                            "used_tools": [],
                        }
                else:
                    response_text = await self._generate_basic_response(
                        user_msg, streaming_cb
                    )
                    response_data = {
                        "response": response_text,
                        "reasoning": "",
                        "used_tools": [],
                    }

                self.add_assistant_message(response_data["response"])
                return response_data

            except Exception as exc:
                logger.error(f"MCPToolManager ì‚¬ìš© ì¤‘ ì˜ˆì™¸ ë°œìƒ: {exc}")
                response = "ì£„ì†¡í•©ë‹ˆë‹¤. ë„êµ¬ ì²˜ë¦¬ ì¤‘ ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
                self.add_assistant_message(response)
                return {"response": response, "reasoning": "", "used_tools": []}

        # ê¸°ë³¸ ì‘ë‹µ ìƒì„±
        try:
            response_text = await self._generate_basic_response(user_msg, streaming_cb)
            self.add_assistant_message(response_text)
            return {
                "response": response_text,
                "reasoning": "",
                "used_tools": [],
            }
        except Exception as exc:
            logger.error(f"ì‘ë‹µ ìƒì„± ì¤‘ ì˜ˆì™¸ ë°œìƒ: {exc}")
            response = "ì£„ì†¡í•©ë‹ˆë‹¤. ì‘ë‹µ ìƒì„± ì¤‘ ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
            self.add_assistant_message(response)
            return {"response": response, "reasoning": "", "used_tools": []}

    async def _should_use_tools(self, msg: str) -> bool:
        """ë„êµ¬ ì‚¬ìš© ì—¬ë¶€ë¥¼ ê²°ì •í•©ë‹ˆë‹¤."""
        if not self.mcp_tool_manager:
            return False

        # MCP ë„êµ¬ ì‚¬ìš©ì„ ë‚˜íƒ€ë‚´ëŠ” í‚¤ì›Œë“œë“¤
        tool_keywords = [
            "github",
            "ê¹ƒí—ˆë¸Œ",
            "MCP",
            "ë„êµ¬",
            "tool",
            "ê²€ìƒ‰",
            "ì‹œê°„",
            "ì‹¤í–‰",
            "execute",
        ]
        msg_lower = msg.lower()

        # íŠ¹ìˆ˜ íŒ¨í„´ í™•ì¸ (ì˜ˆ: owner/repo, @ì„œë²„ëª… ë“±)

        special_patterns = [
            r"@\w+",  # @ë¡œ ì‹œì‘í•˜ëŠ” íŒ¨í„´ (ì˜ˆ: @github)
            r"\b\w+/\w+\b",  # owner/repo í˜•ì‹
            r"\b\w+\.\w+\b",  # domain.extension í˜•ì‹
        ]

        has_special_pattern = any(
            re.search(pattern, msg) for pattern in special_patterns
        )
        has_keyword = any(keyword in msg_lower for keyword in tool_keywords)

        return has_special_pattern or has_keyword

    async def _generate_with_tools(
        self, user_msg: str, tools: List[Dict], streaming_cb: Optional[Callable[[str], None]]
    ) -> Dict[str, Any]:
        """OpenAI agents SDKë¥¼ ì‚¬ìš©í•˜ì—¬ ë³µí•©ì ì¸ ë„êµ¬ ì‘ì—…ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤."""
        try:
            if streaming_cb:
                # ì‚¬ìš© ê°€ëŠ¥í•œ ë„êµ¬ ì •ë³´ í‘œì‹œ
                tool_count = len(tools)
                streaming_cb(f"ğŸ”§ {tool_count}ê°œì˜ MCP ë„êµ¬ê°€ ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤.\n\n")

                streaming_cb("ğŸš€ ë³µí•©ì ì¸ ë„êµ¬ ì‘ì—…ì„ ì‹œì‘í•©ë‹ˆë‹¤...\n\n")

            # MCPToolManagerë¥¼ í†µí•´ agents SDK ê¸°ë°˜ ì‘ë‹µ ìƒì„±
            result = await self.mcp_tool_manager.run_agent_with_tools(
                user_msg, streaming_cb
            )

            return result

        except Exception as exc:
            logger.error(f"agents SDK ê¸°ë°˜ ë„êµ¬ ì‚¬ìš© ì‹¤íŒ¨: {exc}")
            error_msg = "ì£„ì†¡í•©ë‹ˆë‹¤. ë„êµ¬ë¥¼ ì‚¬ìš©í•œ ì‘ë‹µ ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤."
            if streaming_cb:
                streaming_cb(f"\nâŒ **ì˜¤ë¥˜ ë°œìƒ:** {error_msg}\n")
                streaming_cb(f"**ìƒì„¸ ì˜¤ë¥˜:** {str(exc)}\n")
            return {"response": error_msg, "reasoning": str(exc)}

    async def _generate_basic_response(
        self, _user_msg: str, streaming_cb: Optional[Callable[[str], None]]
    ) -> str:
        """ê¸°ë³¸ ì‘ë‹µì„ ìƒì„±í•©ë‹ˆë‹¤."""
        cfg = self.config_manager.get_llm_config()

        try:
            if streaming_cb is None:
                response = await self.client.chat.completions.create(
                    model=cfg["model"],
                    messages=self.history,
                    max_tokens=cfg["max_tokens"],
                    temperature=cfg["temperature"],
                )

                content = response.choices[0].message.content or ""

                show_cot_flag = str(cfg.get("show_cot", "false")).lower() == "true"
                if not show_cot_flag and _is_reasoning_model(cfg["model"]):
                    content = _strip_reasoning(content)

                return content

            # ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ
            acc: List[str] = []
            stream = await self.client.chat.completions.create(
                model=cfg["model"],
                messages=self.history,
                stream=True,
                max_tokens=cfg["max_tokens"],
                temperature=cfg["temperature"],
            )
            async for chunk in stream:
                if chunk.choices and chunk.choices[0].delta:
                    delta = chunk.choices[0].delta
                    if delta.content:
                        streaming_cb(delta.content)
                        acc.append(delta.content)

            content_joined = "".join(acc)

            show_cot_flag = str(cfg.get("show_cot", "false")).lower() == "true"
            if not show_cot_flag and _is_reasoning_model(cfg["model"]):
                # reasoning ì„ ìˆ¨ê²¨ì•¼ í•  ë•ŒëŠ” ìµœì¢… í•©ì³ì§„ ì½˜í…ì¸ ì—ì„œ ì œê±° í›„ UI ë¡œ ë³´ëƒ„
                return _strip_reasoning(content_joined)

            return content_joined

        except Exception as exc:
            logger.error(f"ê¸°ë³¸ ì‘ë‹µ ìƒì„± ì‹¤íŒ¨: {exc}")
            raise
