"""
Langchain ê¸°ë°˜ LLM ì—ì´ì „íŠ¸
"""

import logging
from typing import Any, Callable, Dict, List, Optional

from application.llm.interfaces.llm_interface import LLMInterface
from application.llm.models.llm_config import LLMConfig
from application.llm.services.conversation_service import ConversationService
from application.llm.services.llm_service import LLMService
from application.llm.workflow.workflow_utils import get_workflow
from application.util.logger import setup_logger

logger = setup_logger("llm_agent") or logging.getLogger("llm_agent")


class LLMAgent(LLMInterface):
    """Langchain ê¸°ë°˜ LLM ì—ì´ì „íŠ¸"""

    def __init__(self, config_manager, mcp_tool_manager=None):
        """
        LLM ì—ì´ì „íŠ¸ ì´ˆê¸°í™”
        
        Args:
            config_manager: ì„¤ì • ê´€ë¦¬ì
            mcp_tool_manager: MCP ë„êµ¬ ê´€ë¦¬ì (ì„ íƒì‚¬í•­)
        """
        self.config_manager = config_manager
        self.mcp_tool_manager = mcp_tool_manager
        
        # ì„¤ì • ë¡œë“œ
        self._load_config()
        
        # ì„œë¹„ìŠ¤ ì´ˆê¸°í™”
        self.llm_service = LLMService(self.llm_config)
        self.conversation_service = ConversationService()
        
        # íˆìŠ¤í† ë¦¬ (í•˜ìœ„ í˜¸í™˜ì„±)
        self.history = []
        
        logger.info("LLM ì—ì´ì „íŠ¸ ì´ˆê¸°í™” ì™„ë£Œ")

    def _load_config(self) -> None:
        """ì„¤ì • ë¡œë“œ"""
        try:
            # í”„ë¡œí•„ ê¸°ë°˜ ì„¤ì • ë¡œë“œ (modeì™€ workflow í¬í•¨)
            llm_config_dict = self.config_manager.get_llm_config()
            
            self.llm_config = LLMConfig.from_dict(llm_config_dict)
            logger.debug(f"LLM ì„¤ì • ë¡œë“œ ì™„ë£Œ: {self.llm_config.model}, ëª¨ë“œ: {self.llm_config.mode}")
            
        except Exception as e:
            logger.error(f"ì„¤ì • ë¡œë“œ ì‹¤íŒ¨: {e}")
            # ê¸°ë³¸ ì„¤ì •ìœ¼ë¡œ í´ë°±
            self.llm_config = LLMConfig(
                api_key="",
                model="gpt-3.5-turbo",
                mode="basic"
            )

    async def generate_response(
        self,
        user_message: str,
        streaming_callback: Optional[Callable[[str], None]] = None,
    ) -> Dict[str, Any]:
        """
        ì‚¬ìš©ì ë©”ì‹œì§€ì— ëŒ€í•œ ì‘ë‹µ ìƒì„±
        
        Args:
            user_message: ì‚¬ìš©ì ì…ë ¥ ë©”ì‹œì§€
            streaming_callback: ìŠ¤íŠ¸ë¦¬ë° ì½œë°± í•¨ìˆ˜
            
        Returns:
            Dict[str, Any]: ì‘ë‹µ ë°ì´í„°
        """
        try:
            logger.info(f"LLM ì‘ë‹µ ìƒì„± ì‹œì‘: {user_message[:50]}...")
            
            # ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€
            self.add_user_message(user_message)
            
            # ëª¨ë“œì— ë”°ë¥¸ ì²˜ë¦¬
            mode = self._get_llm_mode()
            
            if mode == "workflow":
                return await self._handle_workflow_mode(user_message, streaming_callback)
            elif mode == "mcp_tools" and self.mcp_tool_manager:
                return await self._handle_mcp_tools_mode(user_message, streaming_callback)
            else:
                return await self._handle_basic_mode(user_message, streaming_callback)
                
        except Exception as e:
            logger.error(f"ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
            return self._create_error_response("ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤", str(e))

    async def generate_response_streaming(
        self,
        user_message: str,
        streaming_callback: Optional[Callable[[str], None]] = None,
    ) -> Dict[str, Any]:
        """ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ìƒì„± (í•˜ìœ„ í˜¸í™˜ì„±)"""
        return await self.generate_response(user_message, streaming_callback)

    async def _handle_basic_mode(
        self,
        user_message: str,
        streaming_callback: Optional[Callable[[str], None]] = None,
    ) -> Dict[str, Any]:
        """ê¸°ë³¸ ëª¨ë“œ ì²˜ë¦¬"""
        try:
            response = await self._generate_basic_response(user_message, streaming_callback)
            return self._create_response_data(response)
        except Exception as e:
            logger.error(f"ê¸°ë³¸ ëª¨ë“œ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
            return self._create_error_response("ê¸°ë³¸ ëª¨ë“œ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤", str(e))

    async def _handle_workflow_mode(
        self,
        user_message: str,
        streaming_callback: Optional[Callable[[str], None]] = None,
    ) -> Dict[str, Any]:
        """ì›Œí¬í”Œë¡œìš° ëª¨ë“œ ì²˜ë¦¬"""
        try:
            workflow_name = self.llm_config.workflow or "basic_chat"
            workflow_class = get_workflow(workflow_name)
            workflow = workflow_class()
            
            result = await workflow.run(self, user_message, streaming_callback)
            
            return {
                "response": result,
                "workflow": workflow_name,
                "reasoning": "",
                "used_tools": []
            }
            
        except Exception as e:
            logger.error(f"ì›Œí¬í”Œë¡œìš° ëª¨ë“œ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
            return {
                "response": "ì›Œí¬í”Œë¡œìš° ì²˜ë¦¬ ì¤‘ ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.",
                "workflow": self.llm_config.workflow or "basic_chat",
                "reasoning": str(e),
                "used_tools": []
            }

    async def _handle_mcp_tools_mode(
        self,
        user_message: str,
        streaming_callback: Optional[Callable[[str], None]] = None,
    ) -> Dict[str, Any]:
        """MCP ë„êµ¬ ëª¨ë“œ ì²˜ë¦¬ - Langchain Agent íŒ¨í„´ ì‚¬ìš©"""
        try:
            if not self.mcp_tool_manager:
                return self._create_error_response("MCP ë„êµ¬ ê´€ë¦¬ìê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
            
            # Langchain Agentë¥¼ ì‚¬ìš©í•œ ë„êµ¬ ê¸°ë°˜ ì‘ë‹µ ìƒì„±
            result = await self._run_langchain_agent_with_tools(user_message, streaming_callback)
            
            return {
                "response": result.get("response", ""),
                "reasoning": result.get("reasoning", ""),
                "used_tools": result.get("used_tools", [])
            }
            
        except Exception as e:
            logger.error(f"MCP ë„êµ¬ ëª¨ë“œ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
            return self._create_error_response("MCP ë„êµ¬ ëª¨ë“œ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤", str(e))

    async def _run_langchain_agent_with_tools(
        self,
        user_message: str,
        streaming_callback: Optional[Callable[[str], None]] = None,
    ) -> Dict[str, Any]:
        """ì§„ì •í•œ Langchain MCP Agentë¥¼ ì‚¬ìš©í•œ ë„êµ¬ ê¸°ë°˜ ì‘ë‹µ ìƒì„±"""
        # JSON ìŠ¤í‚¤ë§ˆ ì´ìŠˆë¡œ ì¸í•´ ì¼ì‹œì ìœ¼ë¡œ ê°„ë‹¨í•œ ë°©ì‹ ì‚¬ìš©
        logger.info("ğŸ”§ Langchain Agent JSON ìŠ¤í‚¤ë§ˆ ì´ìŠˆë¡œ ì¸í•´ ê°„ë‹¨í•œ MCP ë°©ì‹ ì‚¬ìš©")
        return await self._fallback_to_simple_mcp_approach(user_message, streaming_callback)

    async def _fallback_to_simple_mcp_approach(
        self,
        user_message: str,
        streaming_callback: Optional[Callable[[str], None]] = None,
    ) -> Dict[str, Any]:
        """ì§„ì •í•œ MCP ì ‘ê·¼ë²• - LLMì´ ì§ì ‘ ë„êµ¬ë¥¼ ì„ íƒí•˜ë„ë¡"""
        try:
            # ì‚¬ìš© ê°€ëŠ¥í•œ ë„êµ¬ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
            langchain_tools = await self.mcp_tool_manager.get_langchain_tools()
            
            if not langchain_tools:
                logger.warning("ì‚¬ìš© ê°€ëŠ¥í•œ MCP ë„êµ¬ê°€ ì—†ìŠµë‹ˆë‹¤")
                return await self._fallback_to_basic_response(user_message, streaming_callback)
            
            # ë„êµ¬ ì„¤ëª… ìƒì„±
            tool_descriptions = []
            for tool in langchain_tools:
                tool_descriptions.append(f"- {tool.name}: {tool.description}")
            
            tools_info = "\n".join(tool_descriptions)
            
            # LLMì—ê²Œ ë„êµ¬ ì‚¬ìš© ì—¬ë¶€ë¥¼ ì§ì ‘ ê²°ì •í•˜ë„ë¡ ìš”ì²­
            decision_prompt = f"""ğŸš¨ **ì¤‘ìš”: ë°˜ë“œì‹œ ì§€ì •ëœ í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•˜ì„¸ìš”**

ì§ˆë¬¸: "{user_message}"

ë„êµ¬ ëª©ë¡:
{tools_info}

**ê·œì¹™**: 
- í˜„ì¬ ì‹œê°„/ë‚ ì”¨ ë“± ì‹¤ì‹œê°„ ì •ë³´ê°€ í•„ìš”í•˜ë©´: TOOL_NEEDED: ë„êµ¬ëª…(ì¸ì)
- ì¼ë°˜ ì§€ì‹ìœ¼ë¡œ ë‹µë³€ ê°€ëŠ¥í•˜ë©´: NO_TOOLS_NEEDED
- ë‹¤ë¥¸ í˜•íƒœì˜ ì‘ë‹µì€ ì ˆëŒ€ ê¸ˆì§€

**ì˜ˆì‹œ**:
ì§ˆë¬¸: "ì§€ê¸ˆ ì‹œê°„ì€?" â†’ TOOL_NEEDED: get_current_time()
ì§ˆë¬¸: "ì„œìš¸ ë‚ ì”¨ëŠ”?" â†’ TOOL_NEEDED: get_current_weather(city="Seoul")  
ì§ˆë¬¸: "ì„œìš¸ê³¼ ë¶€ì‚° ë‚ ì”¨ ë¹„êµí•´ì¤˜" â†’ TOOL_NEEDED: get_current_weather(city="Seoul"), get_current_weather(city="Busan")

**ê²½ê³ **: í‘œ, ì„¤ëª…, ì¶”ê°€ í…ìŠ¤íŠ¸ ì—†ì´ ìœ„ í˜•ì‹ë§Œ ì¶œë ¥í•˜ì„¸ìš”."""

            # LLMì˜ ë„êµ¬ ì„ íƒ ê²°ì • ë°›ê¸°
            decision_response = await self._generate_basic_response(decision_prompt, None)
            
            logger.info(f"ğŸ¤– LLM ë„êµ¬ ì„ íƒ ê²°ì •: {decision_response[:200]}...")
            
            # ê²°ì •ì— ë”°ë¼ ë„êµ¬ ì‹¤í–‰ ë˜ëŠ” ê¸°ë³¸ ì‘ë‹µ
            if "TOOL_NEEDED:" in decision_response:
                # ë„êµ¬ ì‹¤í–‰ í›„ ìµœì¢… ë‹µë³€
                tool_results = await self._execute_tools_based_on_llm_decision(decision_response, langchain_tools)
                
                if tool_results["used_tools"]:
                    enhanced_prompt = self._create_enhanced_prompt_with_tools(user_message, tool_results)
                    final_response = await self._generate_basic_response(enhanced_prompt, streaming_callback)
                    
                    return {
                        "response": final_response,
                        "reasoning": "LLMì´ ë„êµ¬ë¥¼ ì§ì ‘ ì„ íƒí•˜ì—¬ ì‹¤í–‰",
                        "used_tools": tool_results["used_tools"]
                    }
            
            # ë„êµ¬ ì‚¬ìš©ì´ ë¶ˆí•„ìš”í•œ ê²½ìš° ê¸°ë³¸ ì‘ë‹µ
            return await self._fallback_to_basic_response(user_message, streaming_callback)
            
        except Exception as e:
            logger.error(f"ì§„ì •í•œ MCP ì ‘ê·¼ë²• ì‹¤íŒ¨: {e}")
            return await self._fallback_to_basic_response(user_message, streaming_callback)

    async def _execute_tools_based_on_llm_decision(self, decision_response: str, langchain_tools: List[Any]) -> Dict[str, Any]:
        """LLM ê²°ì •ì— ë”°ë¥¸ ë„êµ¬ ì‹¤í–‰"""
        try:
            used_tools = []
            responses = []
            
            # TOOL_NEEDED ë¼ì¸ë“¤ ì¶”ì¶œ
            lines = decision_response.split('\n')
            for line in lines:
                if "TOOL_NEEDED:" in line:
                    # "TOOL_NEEDED: get_current_time()" -> "get_current_time()"
                    tool_calls = line.split("TOOL_NEEDED:")[1].strip()
                    
                    # ë‹¤ì¤‘ ë„êµ¬ í˜¸ì¶œ ì²˜ë¦¬ (ì‰¼í‘œë¡œ êµ¬ë¶„)
                    for tool_call in tool_calls.split(','):
                        tool_call = tool_call.strip()
                        
                        # ë„êµ¬ëª…ê³¼ ì¸ì íŒŒì‹±
                        if '(' in tool_call and ')' in tool_call:
                            tool_name = tool_call.split('(')[0].strip()
                            args_str = tool_call.split('(')[1].split(')')[0].strip()
                            
                            # ì¸ì íŒŒì‹± (ê°„ë‹¨í•œ ë°©ì‹)
                            args = {}
                            if args_str and args_str != '':
                                # city="Seoul" í˜•íƒœ íŒŒì‹±
                                if '=' in args_str:
                                    for arg in args_str.split(','):
                                        if '=' in arg:
                                            key, value = arg.split('=', 1)
                                            key = key.strip()
                                            value = value.strip().strip('"').strip("'")
                                            args[key] = value
                            
                            # í•´ë‹¹ ë„êµ¬ ì°¾ê¸°
                            target_tool = None
                            for tool in langchain_tools:
                                if tool.name == tool_name:
                                    target_tool = tool
                                    break
                            
                            if target_tool:
                                logger.info(f"ğŸ”§ LLM ì„ íƒ ë„êµ¬ ì‹¤í–‰: {tool_name}({args})")
                                result = await target_tool.ainvoke(args)
                                logger.info(f"âœ… ë„êµ¬ ê²°ê³¼: {result}")
                                responses.append(f"[{tool_name}] {str(result)}")
                                used_tools.append(f"{tool_name}({args})")
                            else:
                                logger.warning(f"âŒ ë„êµ¬ '{tool_name}'ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            
            return {
                "response": "\n\n".join(responses),
                "reasoning": f"LLMì´ ì„ íƒí•œ {len(used_tools)}ê°œ ë„êµ¬ ì‹¤í–‰",
                "used_tools": used_tools
            }
            
        except Exception as e:
            logger.error(f"âŒ LLM ê²°ì • ê¸°ë°˜ ë„êµ¬ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
            return {"response": "", "reasoning": str(e), "used_tools": []}

    async def _execute_relevant_tools(self, user_message: str) -> Dict[str, Any]:
        """ì‚¬ìš©ì ë©”ì‹œì§€ì—ì„œ ê´€ë ¨ ë„êµ¬ ì‹¤í–‰ (ë ˆê±°ì‹œ - ì‚¬ìš© ì•ˆ í•¨)"""
        logger.warning("âš ï¸ ë ˆê±°ì‹œ ìˆ˜ë™ ë„êµ¬ ì‹¤í–‰ ë©”ì„œë“œ í˜¸ì¶œë¨ - LLM ê¸°ë°˜ ë°©ì‹ì„ ì‚¬ìš©í•´ì•¼ í•©ë‹ˆë‹¤")
        return {"response": "", "reasoning": "ë ˆê±°ì‹œ ë©”ì„œë“œ", "used_tools": []}

    def _create_enhanced_prompt_with_tools(self, user_message: str, tool_results: Dict[str, Any]) -> str:
        """ë„êµ¬ ê²°ê³¼ë¥¼ í¬í•¨í•œ í–¥ìƒëœ í”„ë¡¬í”„íŠ¸ ìƒì„±"""
        tool_info = ""
        if tool_results.get("used_tools"):
            tool_info = f"\n\n=== ì‹¤ì‹œê°„ ë„êµ¬ ì‹¤í–‰ ê²°ê³¼ ===\n{tool_results.get('response', '')}\n=== ì‹¤ì‹œê°„ ë„êµ¬ ì‹¤í–‰ ê²°ê³¼ ë ===\n"
        
        enhanced_prompt = f"""ì‚¬ìš©ì ì§ˆë¬¸: {user_message}
{tool_info}

ğŸ”´ **ì ˆëŒ€ ì¤€ìˆ˜ ì‚¬í•­ (ë°˜ë“œì‹œ ì½ê³  ë”°ë¥¼ ê²ƒ!):**
1. **ì˜¤ì§ ìœ„ì˜ ì‹¤ì‹œê°„ ë„êµ¬ ì‹¤í–‰ ê²°ê³¼ë§Œ ì‚¬ìš©í•˜ì„¸ìš”**
2. **ì ˆëŒ€ë¡œ ì¶”ì¸¡, ì˜ˆì‹œ, ê°€ì§œ ë°ì´í„°ë¥¼ ìƒì„±í•˜ì§€ ë§ˆì„¸ìš”**
3. **ë„êµ¬ ê²°ê³¼ì˜ ì •í™•í•œ ìˆ˜ì¹˜, ì‹œê°„, ë‚ ì”¨ ìƒíƒœë§Œ ì‚¬ìš©í•˜ì„¸ìš”**
4. **ì‹œê°„ì€ ë„êµ¬ì—ì„œ ì œê³µí•œ ISO í˜•ì‹ì„ í•œêµ­ ì‹œê°„ìœ¼ë¡œ ë³€í™˜í•˜ì—¬ í‘œì‹œí•˜ì„¸ìš”**
5. **ë‚ ì”¨ ë°ì´í„°ê°€ ì—†ëŠ” ë„ì‹œëŠ” "ë°ì´í„° ì—†ìŒ"ìœ¼ë¡œ í‘œì‹œí•˜ì„¸ìš”**

ğŸš« **ì ˆëŒ€ ê¸ˆì§€ ì‚¬í•­:**
- ì„ì˜ì˜ ì˜¨ë„, ìŠµë„, í’ì† ìˆ˜ì¹˜ ìƒì„± ê¸ˆì§€
- "ì˜ˆìƒ", "ëŒ€ëµ", "ë³´í†µ" ë“±ì˜ ì¶”ì¸¡ì„± í‘œí˜„ ê¸ˆì§€  
- ë„êµ¬ ê²°ê³¼ì— ì—†ëŠ” ë¯¸ì„¸ë¨¼ì§€, ê³µê¸°ì§ˆ ì •ë³´ ì¶”ê°€ ê¸ˆì§€
- ë„êµ¬ ê²°ê³¼ì™€ ë‹¤ë¥¸ ë‚ ì”¨ ìƒíƒœ í‘œì‹œ ê¸ˆì§€

âœ… **ì •í™•í•œ ë‹µë³€ ë°©ë²•:**
- ì‹œê°„: ë„êµ¬ ê²°ê³¼ì˜ ISO ì‹œê°„ì„ "2025ë…„ 6ì›” 24ì¼ ì˜¤ì „ 11:45" í˜•ì‹ìœ¼ë¡œ ë³€í™˜
- ë‚ ì”¨: ë„êµ¬ ê²°ê³¼ì˜ ì •í™•í•œ ìƒíƒœì™€ ì˜¨ë„ë§Œ ì‚¬ìš©
- í‘œ: ì‹¤ì œ ë„êµ¬ ë°ì´í„°ë¡œë§Œ êµ¬ì„±, ì—†ëŠ” ë°ì´í„°ëŠ” "ì •ë³´ ì—†ìŒ"

í•œêµ­ì–´ë¡œ ì •í™•í•˜ê³  ì¹œì ˆí•˜ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”."""

        return enhanced_prompt

    async def _fallback_to_basic_response(
        self,
        user_message: str,
        streaming_callback: Optional[Callable[[str], None]] = None,
    ) -> Dict[str, Any]:
        """ë„êµ¬ê°€ ì—†ì„ ë•Œ ê¸°ë³¸ ì‘ë‹µìœ¼ë¡œ í´ë°±"""
        try:
            response = await self._generate_basic_response(user_message, streaming_callback)
            return {
                "response": response,
                "reasoning": "ë„êµ¬ ì—†ì´ ê¸°ë³¸ ì‘ë‹µ ìƒì„±",
                "used_tools": []
            }
        except Exception as e:
            logger.error(f"ê¸°ë³¸ ì‘ë‹µ í´ë°± ì‹¤íŒ¨: {e}")
            return {
                "response": "ì£„ì†¡í•©ë‹ˆë‹¤. ì‘ë‹µ ìƒì„± ì¤‘ ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.",
                "reasoning": str(e),
                "used_tools": []
            }

    async def _fallback_to_mcp_tools(self, user_message: str) -> Dict[str, Any]:
        """Langchain Agent ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ MCP ë„êµ¬ ì‚¬ìš©ìœ¼ë¡œ í´ë°±"""
        try:
            result = await self.mcp_tool_manager.run_agent_with_tools(user_message)
            return {
                "response": result.get("response", ""),
                "reasoning": "Langchain Agent ì‹¤íŒ¨ë¡œ ê¸°ë³¸ MCP ë„êµ¬ ì‚¬ìš©",
                "used_tools": result.get("used_tools", [])
            }
        except Exception as e:
            logger.error(f"MCP ë„êµ¬ í´ë°±ë„ ì‹¤íŒ¨: {e}")
            return {
                "response": "ì£„ì†¡í•©ë‹ˆë‹¤. ë„êµ¬ ì‚¬ìš© ì¤‘ ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.",
                "reasoning": str(e),
                "used_tools": []
            }

    async def _generate_basic_response(
        self,
        user_message: str,
        streaming_callback: Optional[Callable[[str], None]] = None,
    ) -> str:
        """ê¸°ë³¸ ì‘ë‹µ ìƒì„±"""
        try:
            messages = self.conversation_service.get_messages()
            response = await self.llm_service.generate_response(messages, streaming_callback)
            return response.response
        except Exception as e:
            logger.error(f"ê¸°ë³¸ ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
            return f"ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"

    def _get_llm_mode(self) -> str:
        """LLM ëª¨ë“œ ë°˜í™˜"""
        mode = self.llm_config.mode
        if mode and isinstance(mode, str):
            return mode.lower()
        return "basic"

    def _create_response_data(
        self,
        response: str,
        reasoning: str = "",
        used_tools: List[str] = None
    ) -> Dict[str, Any]:
        """ì‘ë‹µ ë°ì´í„° ìƒì„±"""
        if used_tools is None:
            used_tools = []
            
        # ì–´ì‹œìŠ¤í„´íŠ¸ ë©”ì‹œì§€ ì¶”ê°€
        self.add_assistant_message(response)
        
        return {
            "response": response,
            "reasoning": reasoning,
            "used_tools": used_tools
        }

    def _create_error_response(self, error_msg: str, detail: str = "") -> Dict[str, Any]:
        """ì—ëŸ¬ ì‘ë‹µ ìƒì„±"""
        response = f"ì£„ì†¡í•©ë‹ˆë‹¤. {error_msg}"
        self.add_assistant_message(response)
        
        return {
            "response": response,
            "reasoning": detail,
            "used_tools": []
        }

    def add_user_message(self, message: str) -> None:
        """ì‚¬ìš©ì ë©”ì‹œì§€ë¥¼ ëŒ€í™” íˆìŠ¤í† ë¦¬ì— ì¶”ê°€"""
        self.conversation_service.add_user_message(message)
        # í•˜ìœ„ í˜¸í™˜ì„±
        self.history.append({"role": "user", "content": message})

    def add_assistant_message(self, message: str) -> None:
        """ì–´ì‹œìŠ¤í„´íŠ¸ ë©”ì‹œì§€ë¥¼ ëŒ€í™” íˆìŠ¤í† ë¦¬ì— ì¶”ê°€"""
        self.conversation_service.add_assistant_message(message)
        # í•˜ìœ„ í˜¸í™˜ì„±
        self.history.append({"role": "assistant", "content": message})

    def clear_conversation(self) -> None:
        """ëŒ€í™” íˆìŠ¤í† ë¦¬ ì´ˆê¸°í™”"""
        self.conversation_service.clear_conversation()
        # í•˜ìœ„ í˜¸í™˜ì„±
        self.history.clear()
        logger.info("ëŒ€í™” íˆìŠ¤í† ë¦¬ ì´ˆê¸°í™”")

    def get_conversation_history(self) -> List[Dict[str, str]]:
        """ëŒ€í™” íˆìŠ¤í† ë¦¬ ë°˜í™˜"""
        return self.conversation_service.get_messages_as_dict()

    async def cleanup(self) -> None:
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        await self.llm_service.cleanup()
        logger.info("LLM ì—ì´ì „íŠ¸ ì •ë¦¬ ì™„ë£Œ")

    def reinitialize_client(self) -> None:
        """í´ë¼ì´ì–¸íŠ¸ ì¬ì´ˆê¸°í™” - í”„ë¡œí•„ ë³€ê²½ ì‹œ ì‚¬ìš©"""
        try:
            logger.info("LLM ì—ì´ì „íŠ¸ í´ë¼ì´ì–¸íŠ¸ ì¬ì´ˆê¸°í™” ì‹œì‘")
            
            # ì„¤ì • ë‹¤ì‹œ ë¡œë“œ
            self._load_config()
            
            # LLM ì„œë¹„ìŠ¤ ì¬ì´ˆê¸°í™”
            self.llm_service = LLMService(self.llm_config)
            
            # ëŒ€í™” ì„œë¹„ìŠ¤ ì¬ì´ˆê¸°í™” (íˆìŠ¤í† ë¦¬ëŠ” ìœ ì§€)
            # self.conversation_serviceëŠ” ê·¸ëŒ€ë¡œ ìœ ì§€í•˜ì—¬ ëŒ€í™” ë§¥ë½ ë³´ì¡´
            
            logger.info(f"LLM ì—ì´ì „íŠ¸ ì¬ì´ˆê¸°í™” ì™„ë£Œ: ëª¨ë¸={self.llm_config.model}, ëª¨ë“œ={self.llm_config.mode}")
            
        except Exception as e:
            logger.error(f"LLM ì—ì´ì „íŠ¸ ì¬ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")

    # ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì € ì§€ì›
    async def __aenter__(self):
        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        await self.cleanup() 