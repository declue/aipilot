from __future__ import annotations

import asyncio
import json
import logging
import re
from functools import partial
from typing import Any, Dict, List, cast

from agents import Agent, ModelSettings, Runner, set_default_openai_client
from agents.mcp import MCPServerStdio
from openai import AsyncOpenAI  # type: ignore
from openai.types.chat import ChatCompletionMessageParam  # type: ignore

from application.config.config_manager import ConfigManager
from application.llm.mcp.mcp_manager import MCPManager
from application.llm.mcp.tool.cache import ToolCache
from application.llm.mcp.tool.converter import ToolConverter
from application.util.logger import setup_logger

logger = setup_logger(__name__) or logging.getLogger(__name__)


class ToolExecutor:  # pylint: disable=too-few-public-methods
    """ë‹¨ì¼ MCP ë„êµ¬ í˜¸ì¶œì„ ì‹¤í–‰í•œë‹¤. (Strategy íŒ¨í„´ ì ìš© ê°€ëŠ¥ ì§€ì )"""

    def __init__(
        self, mcp_manager: MCPManager, config_manager: ConfigManager, cache: ToolCache
    ) -> None:
        self._mcp_manager = mcp_manager
        self._config_manager = config_manager
        self._cache = cache

    async def __call__(self, tool_key: str, arguments: Dict[str, Any]) -> str:  # noqa: D401
        """`await executor(tool_key, args)` í˜•íƒœë¡œ ì‚¬ìš©í•˜ê¸° ìœ„í•œ í˜¸ì¶œ ê°€ëŠ¥ ê°ì²´."""
        # 1) ìºì‹œ í™•ì¸
        if tool_key not in self._cache:
            return f"ì˜¤ë¥˜: ë„êµ¬ '{tool_key}'ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

        tool_meta = self._cache.get(tool_key)
        server_name = tool_meta["server_name"]
        actual_tool_name = tool_meta["tool_name"]

        # 2) ì„œë²„ ì„¤ì • ì¡°íšŒ
        server_config = self._mcp_manager.get_enabled_servers().get(server_name)
        if not server_config:
            return f"ì˜¤ë¥˜: ì„œë²„ '{server_name}' ì„¤ì •ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

        # 4) OpenAI í´ë¼ì´ì–¸íŠ¸ ì¬ì„¤ì • (ConfigManagerë¡œë¶€í„°)
        cfg = self._config_manager.get_llm_config()
        try:
            client = AsyncOpenAI(api_key=cfg.get("api_key"), base_url=cfg.get("base_url"))
            set_default_openai_client(client)
        except Exception as exc:  # pragma: no cover
            logger.warning("OpenAI í´ë¼ì´ì–¸íŠ¸ ì„¤ì • ì‹¤íŒ¨: %s", exc)

        # 5) MCPServerStdio ë¥¼ ì´ìš©í•´ ì‹¤ì œ ë„êµ¬ í˜¸ì¶œ ì‹¤í–‰
        async with MCPServerStdio(
            cache_tools_list=True,
            params={
                "command": server_config.command,
                "args": server_config.args,
                "env": server_config.env or {},
            },
        ) as mcp_server:
            agent = Agent(
                name=f"{server_name}_agent",
                model=cfg.get("model", "gpt-3.5-turbo"),
                instructions="ë„êµ¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ì§ˆë¬¸ì— ë‹µí•˜ì„¸ìš”.",
                mcp_servers=[mcp_server],
                model_settings=ModelSettings(tool_choice="required"),
            )

            # arguments ë”•ì…”ë„ˆë¦¬ë¥¼ í”„ë¡¬í”„íŠ¸ ë¬¸ìì—´ë¡œ ë‹¨ìˆœ ë³€í™˜ (í–¥í›„ ê°œì„  ê°€ëŠ¥)
            args_str = ", ".join(f"{k}={v}" for k, v in arguments.items())
            prompt = f"Use the {actual_tool_name} tool with these parameters: {args_str}"

            result = await Runner.run(starting_agent=agent, input=prompt, max_turns=10)
            return getattr(result, "final_output", str(result))


class MCPToolManager:  # pylint: disable=too-many-instance-attributes
    """ê³ ìˆ˜ì¤€ í¼ì‚¬ë“œ(Facade) ì—­í• ì„ í•˜ëŠ” MCPToolManager.

    ì‹¤ì œ ë¡œì§ì€ `ToolCache`, `ToolConverter`, `ToolExecutor` ì— ìœ„ì„í•œë‹¤.
    """

    def __init__(self, mcp_manager: MCPManager, config_manager: ConfigManager) -> None:
        self._mcp_manager = mcp_manager
        self._config_manager = config_manager
        self._cache = ToolCache()
        self._converter = ToolConverter()
        self._executor = ToolExecutor(mcp_manager, config_manager, self._cache)

        # ë¹„ë™ê¸° í™˜ê²½ì´ ì•„ë‹Œ ê³³ì—ì„œ ìƒì„±ë  ìˆ˜ë„ ìˆìœ¼ë¯€ë¡œ, ì´ë²¤íŠ¸ ë£¨í”„ê°€ ìˆìœ¼ë©´ ì¦‰ì‹œ ê°±ì‹ .
        try:
            loop = asyncio.get_event_loop()
            if loop.is_running():  # pragma: no cover (ì¼ë°˜ í…ŒìŠ¤íŠ¸ í™˜ê²½ì—ì„œëŠ” False)
                loop.create_task(self.refresh_tools())
        except RuntimeError:
            # ì´ë²¤íŠ¸ ë£¨í”„ê°€ ì—†ìœ¼ë©´ ë¬´ì‹œ
            pass

    # ------------------------------------------------------------------
    # Public API
    # ------------------------------------------------------------------
    async def get_openai_tools(self) -> List[Dict[str, Any]]:
        """OpenAI function call í¬ë§·ìœ¼ë¡œ ë³€í™˜ëœ MCP ë„êµ¬ ëª©ë¡ì„ ë°˜í™˜í•©ë‹ˆë‹¤."""
        if not self._cache:  # type: ignore[truthy-bool]
            await self.refresh_tools()
        return [tool for tool in self._build_openai_tools_response()]

    async def call_mcp_tool(self, tool_key: str, arguments: Dict[str, Any]) -> str:
        """ì§€ì •ëœ MCP ë„êµ¬ë¥¼ í˜¸ì¶œí•©ë‹ˆë‹¤."""
        return await self._executor(tool_key, arguments)

    def get_tool_descriptions(self) -> str:
        """í˜„ì¬ ìºì‹œì— ì €ì¥ëœ ë„êµ¬ ì„¤ëª… ë¬¸ìì—´ì„ ë°˜í™˜í•©ë‹ˆë‹¤."""
        if not self._cache:
            return "í˜„ì¬ ì‚¬ìš© ê°€ëŠ¥í•œ MCP ë„êµ¬ê°€ ì—†ìŠµë‹ˆë‹¤."

        lines: List[str] = ["=== ì‚¬ìš© ê°€ëŠ¥í•œ MCP ë„êµ¬ë“¤ ===\n"]
        for tool_key, meta in self._cache.items():
            lines.append(
                f"- **{tool_key}** ({meta['server_name']}): {meta['tool_info']['description']}"
            )
        return "\n".join(lines)

    # ------------------------------------------------------------------
    # ì„ì‹œ ReAct-style ì‹¤í–‰ (ê°„ì†Œí™”)
    # ------------------------------------------------------------------

    async def run_agent_with_tools(self, user_msg: str, streaming_cb=None) -> Dict[str, Any]:  # noqa: D401
        """OpenAI function-call ê¸°ë°˜ ReAct(Reason-Act-Observe) ë£¨í”„.

        1. ì‚¬ìš©ì ì§ˆë¬¸ â†’ LLM í˜¸ì¶œ
        2. LLM ì´ tool_call ì„ ë°˜í™˜í•˜ë©´ í•´ë‹¹ MCP ë„êµ¬ ì‹¤í–‰(Act)
        3. ì‹¤í–‰ ê²°ê³¼ë¥¼ Observation ìœ¼ë¡œ LLM ì— ì „ë‹¬ í›„ ë°˜ë³µ
        4. LLM ì´ finish_reason == "stop" ìœ¼ë¡œ ë‹µë³€ì„ ì œì¶œí•˜ë©´ ì¢…ë£Œ

        ì‹¤íŒ¨í•˜ê±°ë‚˜ OpenAI ì‚¬ìš©ì´ ë¶ˆê°€ëŠ¥í•œ í™˜ê²½ì—ì„œëŠ” ê°„ì†Œí™” ë²„ì „ìœ¼ë¡œ í´ë°±í•©ë‹ˆë‹¤.
        """
        tools = await self.get_openai_tools()
        if not tools:
            logger.info("ì‚¬ìš© ê°€ëŠ¥í•œ MCP ë„êµ¬ê°€ ì—†ìŠµë‹ˆë‹¤ â€“ ê°„ì†Œí™” ëª¨ë“œë¡œ ì „í™˜")
            return await self._run_agent_with_tools_simple(user_msg, streaming_cb)

        cfg = self._config_manager.get_llm_config()
        client = AsyncOpenAI(api_key=cfg.get("api_key"), base_url=cfg.get("base_url"), timeout=300.0)

        messages: List[ChatCompletionMessageParam] = [
            {
                "role": "system",
                "content": (
                    "ë‹¹ì‹ ì€ ì‚¬ìš©ì ì§ˆë¬¸ì— ë‹µí•˜ê¸° ìœ„í•´ MCP ë„êµ¬ë¥¼ ëŠ¥ìˆ™í•˜ê²Œ ì‚¬ìš©í•˜ëŠ” AI ë¹„ì„œì…ë‹ˆë‹¤. "
                    "í•„ìš”í•œ ê²½ìš° í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•˜ì—¬ ë¬¸ì œë¥¼ í•´ê²°í•˜ê³ , ì¶©ë¶„í•œ ê´€ì°° ê²°ê³¼ë¥¼ ì–»ì€ í›„ ìµœì¢… ë‹µë³€ì„ ì œê³µí•©ë‹ˆë‹¤."
                ),
            },
            {"role": "user", "content": user_msg},
        ]

        used_tools: List[str] = []
        reasoning_parts: List[str] = []
        show_cot_flag_global = str(cfg.get("show_cot", "false")).lower() == "true"

        max_turns_cfg = int(self._config_manager.get_llm_config().get("react_max_turns", 5))

        for turn in range(max_turns_cfg):
            if streaming_cb:
                streaming_cb(f"\nğŸ¤” LLM ì‘ë‹µ ìƒì„± (turn {turn + 1})...\n")

            _create_any = cast(Any, client.chat.completions.create)

            completion_factory = partial(
                _create_any,
                model=cfg.get("model"),
                messages=messages,
                tools=cast(Any, tools),  # typing stub ë¯¸ì§€ì›
                tool_choice="auto",  # type: ignore[call-arg]
                temperature=cfg.get("temperature", 0.7),
                max_tokens=cfg.get("max_tokens", 1024),
            )

            response = await _retry_async(
                completion_factory,  # type: ignore[arg-type]
                attempts=int(cfg.get("llm_retry_attempts", 3)),
                backoff=float(cfg.get("retry_backoff_sec", 1)),
            )

            choice = response.choices[0]
            assistant_msg = choice.message

            if choice.finish_reason == "stop":
                # ìµœì¢… ë‹µë³€ í™•ë³´
                final_answer = assistant_msg.content or ""

                if assistant_msg.content:
                    reasoning_parts.append(assistant_msg.content)

                if not show_cot_flag_global:
                    try:
                        from application.llm.llm_agent import \
                            _is_reasoning_model as \
                            _irm  # pylint: disable=import-outside-toplevel; type: ignore
                        from application.llm.llm_agent import \
                            _strip_reasoning as _sr

                        if _irm(cfg.get("model", "")):
                            final_answer = _sr(final_answer)
                    except Exception as exc:  # pylint: disable=broad-except
                        logger.debug("reasoning ì²˜ë¦¬ ì‹¤íŒ¨: %s", exc)

                reasoning_text = "\n".join(reasoning_parts) if show_cot_flag_global else ""

                return {
                    "response": final_answer,
                    "reasoning": reasoning_text,
                    "used_tools": used_tools,
                }

            if choice.finish_reason != "tool_calls":
                # ì˜ˆìƒì¹˜ ëª»í•œ ì¢…ë£Œ â€“ ê·¸ëŒ€ë¡œ ë°˜í™˜
                return {
                    "response": assistant_msg.content or "",
                    "reasoning": ("\n".join(reasoning_parts) if show_cot_flag_global else ""),
                    "used_tools": used_tools,
                }

            # tool_calls ì²˜ë¦¬
            tool_calls = assistant_msg.tool_calls or []
            if streaming_cb:
                streaming_cb(f"ğŸ”¨ {len(tool_calls)}ê°œ ë„êµ¬ í˜¸ì¶œ ê°ì§€\n")

            for tc in tool_calls:
                tool_name = tc.function.name
                try:
                    args_dict = json.loads(tc.function.arguments)
                except Exception as exc:  # pylint: disable=broad-except
                    logger.error("ë„êµ¬ ì¸ìˆ˜ íŒŒì‹± ì‹¤íŒ¨: %s", exc)
                    args_dict = {}

                if streaming_cb:
                    streaming_cb(f"ğŸ› ï¸ '{tool_name}' ì‹¤í–‰ ì¤‘...\n")

                try:
                    tool_result = await _retry_async(
                        partial(self.call_mcp_tool, tool_name, args_dict),
                        attempts=int(cfg.get("llm_retry_attempts", 3)),
                        backoff=float(cfg.get("retry_backoff_sec", 1)),
                    )
                except Exception as exc:  # pylint: disable=broad-except
                    logger.error("ë„êµ¬ ì‹¤í–‰ ì¤‘ ì˜ˆì™¸: %s", exc)
                    tool_result = f"ë„êµ¬ ì‹¤í–‰ ì‹¤íŒ¨: {exc}"

                used_tools.append(tool_name)

                # assistant tool_call ë©”ì‹œì§€ë¥¼ ê¸°ë¡ (id ìœ ì§€)
                messages.append(
                    {
                        "role": "assistant",
                        "content": None,
                        "tool_calls": [tc],  # type: ignore[arg-type]
                    }
                )

                # observation ë©”ì‹œì§€ ì¶”ê°€
                messages.append(
                    {
                        "role": "tool",
                        "tool_call_id": tc.id,
                        "content": tool_result,
                    }
                )

                if streaming_cb:
                    streaming_cb(f"âœ… '{tool_name}' ì™„ë£Œ\n")

        # ë°˜ë³µ ì´ˆê³¼
        logger.warning("ReAct ë£¨í”„ ìµœëŒ€ ë°˜ë³µ ì´ˆê³¼")
        return {
            "response": "ì£„ì†¡í•©ë‹ˆë‹¤. ìš”ì²­ì„ ì™„ë£Œí•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.",
            "reasoning": "Max iterations exceeded",
            "used_tools": used_tools,
        }

    # ------------------------------------------------------------------
    # ê°„ë‹¨í•œ íŒ¨í„´ ê¸°ë°˜ í´ë°± êµ¬í˜„ (ì´ì „ ë²„ì „)
    # ------------------------------------------------------------------

    async def _run_agent_with_tools_simple(self, user_msg: str, streaming_cb=None) -> Dict[str, Any]:  # noqa: D401
        """ê°„ë‹¨í•œ '<tool>(args)' íŒ¨í„´ íŒŒì‹± ë²„ì „ (ë„¤íŠ¸ì›Œí¬ í˜¸ì¶œ ì—†ì´ë„ ë™ì‘)."""


        tool_pattern = re.compile(r"(?P<tool>[\w_]+)\s*\((?P<args>[^)]*)\)")
        match = tool_pattern.search(user_msg)

        if not match:
            return {
                "response": "ë„êµ¬ í˜¸ì¶œ íŒ¨í„´ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.",
                "reasoning": "No tool pattern detected",
                "used_tools": [],
            }

        tool_key = match.group("tool")
        args_str = match.group("args")

        args_dict: Dict[str, Any] = {}
        if args_str.strip():
            for part in args_str.split(","):
                if "=" in part:
                    k, v = part.split("=", 1)
                    args_dict[k.strip()] = v.strip().strip("'\"")

        if streaming_cb:
            streaming_cb(f"ğŸ› ï¸ MCP ë„êµ¬ '{tool_key}' í˜¸ì¶œ ì¤‘...\n")

        try:
            result_text = await self.call_mcp_tool(tool_key, args_dict)
        except Exception as exc:  # pylint: disable=broad-except
            logger.error("ë„êµ¬ ì‹¤í–‰ ì‹¤íŒ¨: %s", exc)
            return {
                "response": "ë„êµ¬ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.",
                "reasoning": str(exc),
                "used_tools": [tool_key],
            }

        return {
            "response": result_text,
            "reasoning": "MCP tool executed (simple mode)",
            "used_tools": [tool_key],
        }

    # ------------------------------------------------------------------
    # ë‚´ë¶€ êµ¬í˜„
    # ------------------------------------------------------------------
    async def refresh_tools(self) -> None:
        """í™œì„±í™”ëœ MCP ì„œë²„ë¡œë¶€í„° ë„êµ¬ ë©”íƒ€ë°ì´í„°ë¥¼ ìƒˆë¡œ ê°€ì ¸ì™€ ìºì‹œë¥¼ ê°±ì‹ í•©ë‹ˆë‹¤."""
        self._cache.clear()
        enabled_servers = self._mcp_manager.get_enabled_servers()
        if not enabled_servers:
            logger.info("í™œì„±í™”ëœ MCP ì„œë²„ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return

        async def _process_server(server_name: str, _server_config):  # type: ignore[valid-type]
            status = await self._mcp_manager.test_server_connection(server_name)
            if not getattr(status, "connected", False):
                logger.warning("%s ì„œë²„ ì—°ê²° ì‹¤íŒ¨", server_name)
                return

            for tool in getattr(status, "tools", []):
                key = f"{server_name}_{tool['name']}"
                self._cache.add(key, server_name, tool["name"], tool)

        # ë³‘ë ¬ë¡œ ê° ì„œë²„ ìƒíƒœ í™•ì¸
        await asyncio.gather(*[_process_server(name, cfg) for name, cfg in enabled_servers.items()])

    # -------------------------------------------------
    def _build_openai_tools_response(self) -> List[Dict[str, Any]]:
        """ToolCache ë‚´ìš©ì„ OpenAI í•¨ìˆ˜ í¬ë§· ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜í•˜ëŠ” ë„ìš°ë¯¸."""
        response: List[Dict[str, Any]] = []
        for key, meta in self._cache.items():
            tool_info = meta["tool_info"]
            response.append(
                {
                    "type": "function",
                    "function": {
                        "name": key,
                        "description": self._converter.enhance_description(
                            meta["server_name"], meta["tool_name"], tool_info["description"]
                        ),
                        "parameters": self._converter.convert_schema(tool_info.get("inputSchema", {})),
                    },
                }
            )
        return response

# ------------------------------------------------------------------
# ë‚´ë¶€ í—¬í¼: ë¹„ë™ê¸° ì¬ì‹œë„
# ------------------------------------------------------------------

def _retry_async(coro_factory, *, attempts: int = 3, backoff: float = 1.0):  # noqa: D401
    """ì£¼ì–´ì§„ awaitable factory ì— ëŒ€í•´ ì¬ì‹œë„(backoff) ìˆ˜í–‰."""

    async def _inner():
        delay = backoff
        for attempt in range(1, attempts + 1):
            try:
                return await coro_factory()
            except Exception as exc:  # pylint: disable=broad-except
                if attempt == attempts:
                    raise
                logger.warning("ì¬ì‹œë„ %s/%s: %s", attempt, attempts, exc)
                await asyncio.sleep(delay)
                delay *= 2

    return _inner() 