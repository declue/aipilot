from __future__ import annotations

import asyncio
import json
import logging
import re
from functools import partial
from typing import Any, Callable, Dict, List, cast

from agents import Agent, ModelSettings, Runner, set_default_openai_client
from agents.mcp import MCPServerStdio
from openai import AsyncOpenAI  # type: ignore
from openai.types.chat import ChatCompletionMessageParam  # type: ignore

from application.config.config_manager import ConfigManager
from application.llm.mcp.mcp_manager import MCPManager
from application.llm.mcp.tool.cache import ToolCache
from application.llm.mcp.tool.converter import ToolConverter
from application.util.logger import setup_logger

logger = setup_logger(__name__) or logging.getLogger(__name__)


class ToolExecutor:  # pylint: disable=too-few-public-methods
    """ë‹¨ì¼ MCP ë„êµ¬ í˜¸ì¶œì„ ì‹¤í–‰í•œë‹¤. (Strategy íŒ¨í„´ ì ìš© ê°€ëŠ¥ ì§€ì )"""

    def __init__(
        self, mcp_manager: MCPManager, config_manager: ConfigManager, cache: ToolCache
    ) -> None:
        self._mcp_manager = mcp_manager
        self._config_manager = config_manager
        self._cache = cache

    async def __call__(self, tool_key: str, arguments: Dict[str, Any]) -> str:  # noqa: D401
        """`await executor(tool_key, args)` í˜•íƒœë¡œ ì‚¬ìš©í•˜ê¸° ìœ„í•œ í˜¸ì¶œ ê°€ëŠ¥ ê°ì²´."""
        # 1) ìºì‹œ í™•ì¸
        if tool_key not in self._cache:
            return f"ì˜¤ë¥˜: ë„êµ¬ '{tool_key}'ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

        tool_meta = self._cache.get(tool_key)
        server_name = tool_meta["server_name"]
        actual_tool_name = tool_meta["tool_name"]        # 2) ì„œë²„ ì„¤ì • ì¡°íšŒ
        server_config = self._mcp_manager.get_enabled_servers().get(server_name)
        if not server_config:
            return f"ì˜¤ë¥˜: ì„œë²„ '{server_name}' ì„¤ì •ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

        # 4) OpenAI í´ë¼ì´ì–¸íŠ¸ ì¬ì„¤ì • (ConfigManagerë¡œë¶€í„°)
        cfg = self._config_manager.get_llm_config()
        try:
            client = AsyncOpenAI(
                api_key=cfg.get("api_key"), 
                base_url=cfg.get("base_url"),
                timeout=60.0  # íƒ€ì„ì•„ì›ƒ ì„¤ì • ì¶”ê°€
            )
            set_default_openai_client(client)
        except Exception as exc:  # pragma: no cover
            logger.error("OpenAI í´ë¼ì´ì–¸íŠ¸ ì„¤ì • ì‹¤íŒ¨: %s", exc)
            return f"ì˜¤ë¥˜: OpenAI í´ë¼ì´ì–¸íŠ¸ ì„¤ì • ì‹¤íŒ¨ - {exc}"        # 5) MCPServerStdio ë¥¼ ì´ìš©í•´ ì‹¤ì œ ë„êµ¬ í˜¸ì¶œ ì‹¤í–‰
        try:
            async with MCPServerStdio(
                cache_tools_list=True,
                params={
                    "command": server_config.command,
                    "args": server_config.args,
                    "env": server_config.env or {},
                },
            ) as mcp_server:
                agent = Agent(
                    name=f"{server_name}_agent",
                    model=cfg.get("model", "gpt-3.5-turbo"),
                    instructions="ë„êµ¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ì§ˆë¬¸ì— ë‹µí•˜ì„¸ìš”.",
                    mcp_servers=[mcp_server],
                    model_settings=ModelSettings(tool_choice="required"),
                )

                # arguments ë”•ì…”ë„ˆë¦¬ë¥¼ í”„ë¡¬í”„íŠ¸ ë¬¸ìì—´ë¡œ ë‹¨ìˆœ ë³€í™˜ (í–¥í›„ ê°œì„  ê°€ëŠ¥)
                args_str = ", ".join(f"{k}={v}" for k, v in arguments.items())
                prompt = f"Use the {actual_tool_name} tool with these parameters: {args_str}"

                result = await Runner.run(starting_agent=agent, input=prompt, max_turns=10)
                return getattr(result, "final_output", str(result))
                
        except Exception as exc:
            logger.error("MCP ì„œë²„ ì—°ê²° ë˜ëŠ” ë„êµ¬ ì‹¤í–‰ ì‹¤íŒ¨: %s", exc)
            return f"ì˜¤ë¥˜: MCP ë„êµ¬ ì‹¤í–‰ ì‹¤íŒ¨ - {exc}"


class MCPToolManager:  # pylint: disable=too-many-instance-attributes
    """ê³ ìˆ˜ì¤€ í¼ì‚¬ë“œ(Facade) ì—­í• ì„ í•˜ëŠ” MCPToolManager.

    ì‹¤ì œ ë¡œì§ì€ `ToolCache`, `ToolConverter`, `ToolExecutor` ì— ìœ„ì„í•œë‹¤.
    """

    def __init__(self, mcp_manager: MCPManager, config_manager: ConfigManager) -> None:
        self._mcp_manager = mcp_manager
        self._config_manager = config_manager
        self._cache = ToolCache()
        self._converter = ToolConverter()
        self._executor = ToolExecutor(mcp_manager, config_manager, self._cache)

        # ë¹„ë™ê¸° í™˜ê²½ì´ ì•„ë‹Œ ê³³ì—ì„œ ìƒì„±ë  ìˆ˜ë„ ìˆìœ¼ë¯€ë¡œ, ì´ë²¤íŠ¸ ë£¨í”„ê°€ ìˆìœ¼ë©´ ì¦‰ì‹œ ê°±ì‹ .
        try:
            loop = asyncio.get_event_loop()
            if loop.is_running():  # pragma: no cover (ì¼ë°˜ í…ŒìŠ¤íŠ¸ í™˜ê²½ì—ì„œëŠ” False)
                loop.create_task(self.refresh_tools())
        except RuntimeError:
            # ì´ë²¤íŠ¸ ë£¨í”„ê°€ ì—†ìœ¼ë©´ ë¬´ì‹œ
            pass

    # ------------------------------------------------------------------
    # Public API
    # ------------------------------------------------------------------
    async def get_openai_tools(self) -> List[Dict[str, Any]]:
        """OpenAI function call í¬ë§·ìœ¼ë¡œ ë³€í™˜ëœ MCP ë„êµ¬ ëª©ë¡ì„ ë°˜í™˜í•©ë‹ˆë‹¤."""
        if not self._cache:  # type: ignore[truthy-bool]
            await self.refresh_tools()
        return [tool for tool in self._build_openai_tools_response()]

    async def call_mcp_tool(self, tool_key: str, arguments: Dict[str, Any]) -> str:
        """ì§€ì •ëœ MCP ë„êµ¬ë¥¼ í˜¸ì¶œí•©ë‹ˆë‹¤."""
        return await self._executor(tool_key, arguments)

    def get_tool_descriptions(self) -> str:
        """í˜„ì¬ ìºì‹œì— ì €ì¥ëœ ë„êµ¬ ì„¤ëª… ë¬¸ìì—´ì„ ë°˜í™˜í•©ë‹ˆë‹¤."""
        if not self._cache:
            return "í˜„ì¬ ì‚¬ìš© ê°€ëŠ¥í•œ MCP ë„êµ¬ê°€ ì—†ìŠµë‹ˆë‹¤."

        lines: List[str] = ["=== ì‚¬ìš© ê°€ëŠ¥í•œ MCP ë„êµ¬ë“¤ ===\n"]
        for tool_key, meta in self._cache.items():
            lines.append(
                f"- **{tool_key}** ({meta['server_name']}): {meta['tool_info']['description']}"
            )
        return "\n".join(lines)

    # ------------------------------------------------------------------
    # ì„ì‹œ ReAct-style ì‹¤í–‰ (ê°„ì†Œí™”)
    # ------------------------------------------------------------------

    async def run_agent_with_tools(self, user_msg: str, streaming_cb: Callable[[str], None] | None = None) -> Dict[str, Any]:  # noqa: D401
        """OpenAI function-call ê¸°ë°˜ ReAct(Reason-Act-Observe) ë£¨í”„.

        1. ì‚¬ìš©ì ì§ˆë¬¸ â†’ LLM í˜¸ì¶œ
        2. LLM ì´ tool_call ì„ ë°˜í™˜í•˜ë©´ í•´ë‹¹ MCP ë„êµ¬ ì‹¤í–‰(Act)
        3. ì‹¤í–‰ ê²°ê³¼ë¥¼ Observation ìœ¼ë¡œ LLM ì— ì „ë‹¬ í›„ ë°˜ë³µ
        4. LLM ì´ finish_reason == "stop" ìœ¼ë¡œ ë‹µë³€ì„ ì œì¶œí•˜ë©´ ì¢…ë£Œ

        ì‹¤íŒ¨í•˜ê±°ë‚˜ OpenAI ì‚¬ìš©ì´ ë¶ˆê°€ëŠ¥í•œ í™˜ê²½ì—ì„œëŠ” ê°„ì†Œí™” ë²„ì „ìœ¼ë¡œ í´ë°±í•©ë‹ˆë‹¤.
        """
        tools = await self.get_openai_tools()
        if not tools:
            logger.info("ì‚¬ìš© ê°€ëŠ¥í•œ MCP ë„êµ¬ê°€ ì—†ìŠµë‹ˆë‹¤ â€“ ê°„ì†Œí™” ëª¨ë“œë¡œ ì „í™˜")
            return await self._run_agent_with_tools_simple(user_msg, streaming_cb)

        cfg = self._config_manager.get_llm_config()
        
        # API í‚¤ì™€ base_url ê²€ì¦
        api_key = cfg.get("api_key")
        base_url = cfg.get("base_url")
        
        if not api_key:
            logger.error("OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
            return await self._run_agent_with_tools_simple(user_msg, streaming_cb)
            
        # ì´ˆê¸° ì§„ë‹¨ ë¡œê¹…
        logger.info("=== ReAct ë£¨í”„ ì‹œì‘ ===")
        logger.info("ì‚¬ìš©ì ë©”ì‹œì§€: %s", user_msg[:100] + "..." if len(user_msg) > 100 else user_msg)
        logger.info("ì‚¬ìš© ê°€ëŠ¥í•œ ë„êµ¬: %dê°œ", len(tools))
        logger.info("ëª¨ë¸: %s", cfg.get("model", "unknown"))

        try:
            client = AsyncOpenAI(
                api_key=api_key, 
                base_url=base_url, 
                timeout=300.0
            )
            logger.info("OpenAI í´ë¼ì´ì–¸íŠ¸ ìƒì„± ì™„ë£Œ")
        except Exception as exc:
            logger.error("OpenAI í´ë¼ì´ì–¸íŠ¸ ìƒì„± ì‹¤íŒ¨: %s", exc)
            return await self._run_agent_with_tools_simple(user_msg, streaming_cb)

        messages: List[ChatCompletionMessageParam] = [
            {
                "role": "system",
                "content": (
                    "ë‹¹ì‹ ì€ ì‚¬ìš©ì ì§ˆë¬¸ì— ë‹µí•˜ê¸° ìœ„í•´ MCP ë„êµ¬ë¥¼ ëŠ¥ìˆ™í•˜ê²Œ ì‚¬ìš©í•˜ëŠ” AI ë¹„ì„œì…ë‹ˆë‹¤. "
                    "í•„ìš”í•œ ê²½ìš° í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•˜ì—¬ ë¬¸ì œë¥¼ í•´ê²°í•˜ê³ , ì¶©ë¶„í•œ ê´€ì°° ê²°ê³¼ë¥¼ ì–»ì€ í›„ ìµœì¢… ë‹µë³€ì„ ì œê³µí•©ë‹ˆë‹¤."
                ),
            },
            {"role": "user", "content": user_msg},
        ]

        used_tools: List[str] = []
        reasoning_parts: List[str] = []
        show_cot_flag_global = str(cfg.get("show_cot", "false")).lower() == "true"

        max_turns_cfg = int(self._config_manager.get_llm_config().get("react_max_turns", 5))

        for turn in range(max_turns_cfg):
            if streaming_cb:
                streaming_cb(f"\nğŸ¤” LLM ì‘ë‹µ ìƒì„± (turn {turn + 1})...\n")

            _create_any = cast(Any, client.chat.completions.create)

            completion_factory = partial(
                _create_any,
                model=cfg.get("model"),
                messages=messages,
                tools=cast(Any, tools),  # typing stub ë¯¸ì§€ì›
                tool_choice="auto",  # type: ignore[call-arg]
                temperature=cfg.get("temperature", 0.7),
                max_tokens=cfg.get("max_tokens", 1024),
            )

            try:
                response = await _retry_async(
                    completion_factory,  # type: ignore[arg-type]
                    attempts=int(cfg.get("llm_retry_attempts", 3)),
                    backoff=float(cfg.get("retry_backoff_sec", 1)),
                )
            except Exception as exc:
                logger.error("OpenAI API í˜¸ì¶œ ì‹¤íŒ¨: %s", exc)
                return {
                    "response": "ì£„ì†¡í•©ë‹ˆë‹¤. AI ì„œë¹„ìŠ¤ì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
                    "reasoning": f"OpenAI API ì˜¤ë¥˜: {exc}",
                    "used_tools": used_tools,
                }

            choice = response.choices[0]
            assistant_msg = choice.message

            if choice.finish_reason == "stop":
                # ìµœì¢… ë‹µë³€ í™•ë³´
                final_answer = assistant_msg.content or ""

                if assistant_msg.content:
                    reasoning_parts.append(assistant_msg.content)

                if not show_cot_flag_global:
                    try:
                        from application.llm.llm_agent import (
                            _is_reasoning_model as _irm,  # pylint: disable=import-outside-toplevel; type: ignore
                        )
                        from application.llm.llm_agent import _strip_reasoning as _sr

                        if _irm(cfg.get("model", "")):
                            final_answer = _sr(final_answer)
                    except Exception as exc:  # pylint: disable=broad-except
                        logger.debug("reasoning ì²˜ë¦¬ ì‹¤íŒ¨: %s", exc)

                reasoning_text = "\n".join(reasoning_parts) if show_cot_flag_global else ""

                return {
                    "response": final_answer,
                    "reasoning": reasoning_text,
                    "used_tools": used_tools,
                }

            if choice.finish_reason != "tool_calls":
                # ì˜ˆìƒì¹˜ ëª»í•œ ì¢…ë£Œ â€“ ê·¸ëŒ€ë¡œ ë°˜í™˜
                return {
                    "response": assistant_msg.content or "",
                    "reasoning": ("\n".join(reasoning_parts) if show_cot_flag_global else ""),
                    "used_tools": used_tools,
                }

            # tool_calls ì²˜ë¦¬
            tool_calls = assistant_msg.tool_calls or []
            if streaming_cb:
                streaming_cb(f"ğŸ”¨ {len(tool_calls)}ê°œ ë„êµ¬ í˜¸ì¶œ ê°ì§€\n")

            for tc in tool_calls:
                tool_name = tc.function.name
                try:
                    args_dict = json.loads(tc.function.arguments)
                except Exception as exc:  # pylint: disable=broad-except
                    logger.error("ë„êµ¬ ì¸ìˆ˜ íŒŒì‹± ì‹¤íŒ¨: %s", exc)
                    args_dict = {}

                if streaming_cb:
                    streaming_cb(f"ğŸ› ï¸ '{tool_name}' ì‹¤í–‰ ì¤‘...\n")

                try:
                    tool_result = await _retry_async(
                        partial(self.call_mcp_tool, tool_name, args_dict),
                        attempts=int(cfg.get("llm_retry_attempts", 3)),
                        backoff=float(cfg.get("retry_backoff_sec", 1)),
                    )
                except Exception as exc:  # pylint: disable=broad-except
                    logger.error("ë„êµ¬ ì‹¤í–‰ ì¤‘ ì˜ˆì™¸: %s", exc)
                    tool_result = f"ë„êµ¬ ì‹¤í–‰ ì‹¤íŒ¨: {exc}"

                used_tools.append(tool_name)

                # assistant tool_call ë©”ì‹œì§€ë¥¼ ê¸°ë¡ (id ìœ ì§€)
                messages.append(
                    {
                        "role": "assistant",
                        "content": None,
                        "tool_calls": [tc],  # type: ignore[arg-type]
                    }
                )

                # observation ë©”ì‹œì§€ ì¶”ê°€
                messages.append(
                    {
                        "role": "tool",
                        "tool_call_id": tc.id,
                        "content": tool_result,
                    }
                )

                if streaming_cb:
                    streaming_cb(f"âœ… '{tool_name}' ì™„ë£Œ\n")

        # ë°˜ë³µ ì´ˆê³¼
        logger.warning("ReAct ë£¨í”„ ìµœëŒ€ ë°˜ë³µ ì´ˆê³¼")
        return {
            "response": "ì£„ì†¡í•©ë‹ˆë‹¤. ìš”ì²­ì„ ì™„ë£Œí•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.",
            "reasoning": "Max iterations exceeded",
            "used_tools": used_tools,
        }

    # ------------------------------------------------------------------
    # ê°„ë‹¨í•œ íŒ¨í„´ ê¸°ë°˜ í´ë°± êµ¬í˜„ (ì´ì „ ë²„ì „)
    # ------------------------------------------------------------------

    async def _run_agent_with_tools_simple(self, user_msg: str, streaming_cb: Callable[[str], None] | None = None) -> Dict[str, Any]:  # noqa: D401
        """ê°„ë‹¨í•œ '<tool>(args)' íŒ¨í„´ íŒŒì‹± ë²„ì „ (ë„¤íŠ¸ì›Œí¬ í˜¸ì¶œ ì—†ì´ë„ ë™ì‘)."""

        tool_pattern = re.compile(r"(?P<tool>[\w_]+)\s*\((?P<args>[^)]*)\)")
        match = tool_pattern.search(user_msg)

        if not match:
            return {
                "response": "ë„êµ¬ í˜¸ì¶œ íŒ¨í„´ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.",
                "reasoning": "No tool pattern detected",
                "used_tools": [],
            }

        tool_key = match.group("tool")
        args_str = match.group("args")

        args_dict: Dict[str, Any] = {}
        if args_str.strip():
            for part in args_str.split(","):
                if "=" in part:
                    k, v = part.split("=", 1)
                    args_dict[k.strip()] = v.strip().strip("'\"")

        if streaming_cb:
            streaming_cb(f"ğŸ› ï¸ MCP ë„êµ¬ '{tool_key}' í˜¸ì¶œ ì¤‘...\n")

        try:
            result_text = await self.call_mcp_tool(tool_key, args_dict)
        except Exception as exc:  # pylint: disable=broad-except
            logger.error("ë„êµ¬ ì‹¤í–‰ ì‹¤íŒ¨: %s", exc)
            return {
                "response": "ë„êµ¬ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.",
                "reasoning": str(exc),
                "used_tools": [tool_key],
            }

        return {
            "response": result_text,
            "reasoning": "MCP tool executed (simple mode)",
            "used_tools": [tool_key],
        }

    # ------------------------------------------------------------------
    # ë‚´ë¶€ êµ¬í˜„
    # ------------------------------------------------------------------

    async def refresh_tools(self) -> None:
        """í™œì„±í™”ëœ MCP ì„œë²„ë¡œë¶€í„° ë„êµ¬ ë©”íƒ€ë°ì´í„°ë¥¼ ìƒˆë¡œ ê°€ì ¸ì™€ ìºì‹œë¥¼ ê°±ì‹ í•©ë‹ˆë‹¤."""
        self._cache.clear()
        enabled_servers = self._mcp_manager.get_enabled_servers()
        if not enabled_servers:
            logger.info("í™œì„±í™”ëœ MCP ì„œë²„ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return

        async def _process_server(server_name: str, _server_config: Any) -> None:
            try:
                status = await self._mcp_manager.test_server_connection(server_name)
                if not getattr(status, "connected", False):
                    logger.warning("%s ì„œë²„ ì—°ê²° ì‹¤íŒ¨", server_name)
                    return

                for tool in getattr(status, "tools", []):
                    key = f"{server_name}_{tool['name']}"
                    self._cache.add(key, server_name, tool["name"], tool)
            except Exception as exc:
                logger.error("%s ì„œë²„ ì²˜ë¦¬ ì¤‘ ì˜ˆì™¸ ë°œìƒ: %s", server_name, exc)

        # ë³‘ë ¬ë¡œ ê° ì„œë²„ ìƒíƒœ í™•ì¸ (ì˜ˆì™¸ ì²˜ë¦¬ í¬í•¨)
        try:
            await asyncio.gather(*[_process_server(name, cfg) for name, cfg in enabled_servers.items()], return_exceptions=True)
        except Exception as exc:
            logger.error("ì„œë²„ ì²˜ë¦¬ ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: %s", exc)

    # ------------------------------------------------------------------
    # ì—°ê²° ìƒíƒœ í™•ì¸ ë° ì§„ë‹¨
    # ------------------------------------------------------------------
    
    async def check_connection_status(self) -> Dict[str, Any]:
        """í˜„ì¬ ì—°ê²° ìƒíƒœë¥¼ í™•ì¸í•˜ê³  ì§„ë‹¨ ì •ë³´ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
        status: Dict[str, Any] = {
            "openai_client": False,
            "mcp_servers": {},
            "config_valid": False,
            "errors": []
        }
        
        # 1. ì„¤ì • í™•ì¸
        try:
            cfg = self._config_manager.get_llm_config()
            api_key = cfg.get("api_key")
            if api_key:
                status["config_valid"] = True
            else:
                status["errors"].append("OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
        except Exception as exc:
            status["errors"].append(f"ì„¤ì • ë¡œë“œ ì‹¤íŒ¨: {exc}")
        
        # 2. OpenAI í´ë¼ì´ì–¸íŠ¸ í…ŒìŠ¤íŠ¸
        if status["config_valid"]:
            try:
                cfg = self._config_manager.get_llm_config()
                client = AsyncOpenAI(
                    api_key=cfg.get("api_key"), 
                    base_url=cfg.get("base_url"),
                    timeout=10.0
                )
                # ê°„ë‹¨í•œ ëª¨ë¸ ëª©ë¡ ìš”ì²­ìœ¼ë¡œ ì—°ê²° í…ŒìŠ¤íŠ¸
                await client.models.list()
                status["openai_client"] = True
            except Exception as exc:
                status["errors"].append(f"OpenAI ì—°ê²° ì‹¤íŒ¨: {exc}")
        
        # 3. MCP ì„œë²„ ìƒíƒœ í™•ì¸
        enabled_servers = self._mcp_manager.get_enabled_servers()
        for server_name, server_config in enabled_servers.items():
            try:
                server_status = await self._mcp_manager.test_server_connection(server_name)
                status["mcp_servers"][server_name] = {
                    "connected": getattr(server_status, "connected", False),
                    "tools_count": len(getattr(server_status, "tools", [])),
                    "command": server_config.command
                }
            except Exception as exc:
                status["mcp_servers"][server_name] = {
                    "connected": False,
                    "error": str(exc),
                    "command": server_config.command
                }
        
        return status

    def validate_configuration(self) -> List[str]:
        """ì„¤ì •ì˜ ìœ íš¨ì„±ì„ ê²€ì¦í•˜ê³  ë¬¸ì œì ì„ ë°˜í™˜í•©ë‹ˆë‹¤."""
        issues = []
        
        try:
            cfg = self._config_manager.get_llm_config()
            
            # API í‚¤ í™•ì¸
            if not cfg.get("api_key"):
                issues.append("OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
            
            # ëª¨ë¸ í™•ì¸
            if not cfg.get("model"):
                issues.append("LLM ëª¨ë¸ì´ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
              # base_url í™•ì¸ (ì„ íƒì‚¬í•­ì´ì§€ë§Œ ì„¤ì •ëœ ê²½ìš° ìœ íš¨ì„± í™•ì¸)
            base_url = cfg.get("base_url")
            if base_url and not (
                base_url.startswith("http://") or 
                base_url.startswith("https://") or
                base_url.startswith("localhost") or
                "localhost" in base_url
            ):
                issues.append("base_urlì´ ì˜¬ë°”ë¥¸ í˜•ì‹ì´ ì•„ë‹™ë‹ˆë‹¤")
                
        except Exception as exc:
            issues.append(f"ì„¤ì • ë¡œë“œ ì¤‘ ì˜¤ë¥˜: {exc}")
        
        # MCP ì„œë²„ ì„¤ì • í™•ì¸
        enabled_servers = self._mcp_manager.get_enabled_servers()
        if not enabled_servers:
            issues.append("í™œì„±í™”ëœ MCP ì„œë²„ê°€ ì—†ìŠµë‹ˆë‹¤")
        
        return issues

    # ------------------------------------------------------------------
    # ë‚´ë¶€ êµ¬í˜„
    # ------------------------------------------------------------------
    def _build_openai_tools_response(self) -> List[Dict[str, Any]]:
        """ToolCache ë‚´ìš©ì„ OpenAI í•¨ìˆ˜ í¬ë§· ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜í•˜ëŠ” ë„ìš°ë¯¸."""
        response: List[Dict[str, Any]] = []
        for key, meta in self._cache.items():
            tool_info = meta["tool_info"]
            response.append(
                {
                    "type": "function",
                    "function": {
                        "name": key,
                        "description": self._converter.enhance_description(
                            meta["server_name"], meta["tool_name"], tool_info["description"]
                        ),
                        "parameters": self._converter.convert_schema(tool_info.get("inputSchema", {})),
                    },
                }
            )
        return response

    async def diagnose_llm_issue(self, user_msg: str = "í…ŒìŠ¤íŠ¸") -> Dict[str, Any]:
        """LLM ì—°ê²° ë° ì‘ë‹µ ë¬¸ì œë¥¼ ì§„ë‹¨í•©ë‹ˆë‹¤."""
        diagnosis: Dict[str, Any] = {
            "timestamp": "2025-06-21",
            "config_status": "unknown",
            "connection_test": "unknown", 
            "simple_test": "unknown",
            "tools_available": 0,
            "recommendations": []
        }
        
        # 1. ì„¤ì • ê²€ì¦
        config_issues = self.validate_configuration()
        if config_issues:
            diagnosis["config_status"] = "failed"
            diagnosis["recommendations"].extend([f"ì„¤ì • ë¬¸ì œ: {issue}" for issue in config_issues])
        else:
            diagnosis["config_status"] = "ok"
        
        # 2. ë„êµ¬ ê°€ìš©ì„± í™•ì¸
        try:
            tools = await self.get_openai_tools()
            diagnosis["tools_available"] = len(tools)
            if not tools:
                diagnosis["recommendations"].append("ì‚¬ìš© ê°€ëŠ¥í•œ MCP ë„êµ¬ê°€ ì—†ìŠµë‹ˆë‹¤. MCP ì„œë²„ ìƒíƒœë¥¼ í™•ì¸í•˜ì„¸ìš”.")
        except Exception as exc:
            diagnosis["recommendations"].append(f"ë„êµ¬ ë¡œë“œ ì‹¤íŒ¨: {exc}")
        
        # 3. ì—°ê²° ìƒíƒœ í…ŒìŠ¤íŠ¸
        try:
            connection_status = await self.check_connection_status()
            if connection_status["openai_client"]:
                diagnosis["connection_test"] = "ok"
            else:
                diagnosis["connection_test"] = "failed"
                diagnosis["recommendations"].extend(connection_status["errors"])
        except Exception as exc:
            diagnosis["connection_test"] = "error"
            diagnosis["recommendations"].append(f"ì—°ê²° í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {exc}")
        
        # 4. ê°„ë‹¨í•œ ì‘ë‹µ í…ŒìŠ¤íŠ¸
        try:
            simple_result = await self._run_agent_with_tools_simple(user_msg)
            diagnosis["simple_test"] = "ok" if simple_result["response"] else "failed"
        except Exception as exc:
            diagnosis["simple_test"] = "error"
            diagnosis["recommendations"].append(f"ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {exc}")
        
        # 5. êµ¬ì²´ì ì¸ ê¶Œì¥ì‚¬í•­ ì¶”ê°€
        if diagnosis["config_status"] == "ok" and diagnosis["connection_test"] == "failed":
            diagnosis["recommendations"].append("API í‚¤ë‚˜ base_url ì„¤ì •ì„ í™•ì¸í•˜ì„¸ìš”.")
        
        if diagnosis["tools_available"] == 0:
            diagnosis["recommendations"].append("MCP ì„œë²„ë¥¼ ì‹œì‘í•˜ê³  ë„êµ¬ë¥¼ ìƒˆë¡œê³ ì¹¨í•˜ì„¸ìš”.")
        
        return diagnosis


# ------------------------------------------------------------------
# ë‚´ë¶€ í—¬í¼: ë¹„ë™ê¸° ì¬ì‹œë„
# ------------------------------------------------------------------

def _retry_async(coro_factory: Callable[[], Any], *, attempts: int = 3, backoff: float = 1.0) -> Any:
    """ì£¼ì–´ì§„ awaitable factory ì— ëŒ€í•´ ì¬ì‹œë„(backoff) ìˆ˜í–‰."""

    async def _inner() -> Any:
        delay = backoff
        for attempt in range(1, attempts + 1):
            try:
                return await coro_factory()
            except Exception as exc:  # pylint: disable=broad-except
                if attempt == attempts:
                    raise
                logger.warning("ì¬ì‹œë„ %s/%s: %s", attempt, attempts, exc)
                await asyncio.sleep(delay)
                delay *= 2

    return _inner()