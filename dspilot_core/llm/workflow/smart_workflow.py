"""
í†µí•© ìŠ¤ë§ˆíŠ¸ ì›Œí¬í”Œë¡œìš° (SmartWorkflow)
=====================================

DSPilotì˜ í•µì‹¬ ì›Œí¬í”Œë¡œìš°ë¡œ, ì‚¬ìš©ì ìš”ì²­ì˜ ë³µì¡ë„ë¥¼ ìë™ ë¶„ì„í•˜ì—¬ 
ìµœì ì˜ ì²˜ë¦¬ ë°©ì‹ì„ ì„ íƒí•˜ëŠ” ì§€ëŠ¥í˜• ì›Œí¬í”Œë¡œìš°ì…ë‹ˆë‹¤.

ê¸°ì¡´ì˜ AgentWorkflowì™€ AdaptiveWorkflowì˜ ì¥ì ì„ í†µí•©í•˜ì—¬,
ë‹¨ìˆœí•œ ìš”ì²­ë¶€í„° ë³µì¡í•œ ë‹¤ë‹¨ê³„ ì‘ì—…ê¹Œì§€ ëª¨ë‘ íš¨ìœ¨ì ìœ¼ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤.

ì£¼ìš” íŠ¹ì§•
=========

1. **ìë™ ë³µì¡ë„ ë¶„ì„**
   - LLMì´ ìš”ì²­ì„ ë¶„ì„í•˜ì—¬ simple/medium/complexë¡œ ë¶„ë¥˜
   - ë¶„ë¥˜ ê²°ê³¼ì— ë”°ë¼ ìµœì ì˜ ì²˜ë¦¬ ì „ëµ ìë™ ì„ íƒ

2. **ì´ì¤‘ ì²˜ë¦¬ ì „ëµ**
   - ë‹¨ìˆœ ìš”ì²­: ì§ì ‘ ë„êµ¬ ì‹¤í–‰ (ë¹ ë¥¸ ì²˜ë¦¬)
   - ë³µì¡ ìš”ì²­: Plan & Execute (ì •í™•í•œ ì²˜ë¦¬)

3. **MCP ë„êµ¬ í†µí•©**
   - ëª¨ë“  MCP ë„êµ¬ì™€ í˜¸í™˜
   - ë„êµ¬ ë©”íƒ€ë°ì´í„° ê¸°ë°˜ ë™ì  ì²˜ë¦¬

4. **ìŠ¤íŠ¸ë¦¬ë° ì§€ì›**
   - ì‹¤ì‹œê°„ ì§„í–‰ ìƒí™© í”¼ë“œë°±
   - ì‚¬ìš©ì ê²½í—˜ ìµœì í™”

ì²˜ë¦¬ íë¦„ ë‹¤ì´ì–´ê·¸ë¨
==================

```mermaid
flowchart TD
    A[ì‚¬ìš©ì ìš”ì²­] --> B{ë„êµ¬ ì‚¬ìš© ê°€ëŠ¥?}
    B -->|No| C[ì¼ë°˜ LLM ì‘ë‹µ]
    B -->|Yes| D[ë³µì¡ë„ ë¶„ì„]
    D --> E{ë³µì¡ë„ íŒë‹¨}
    E -->|Simple| F[ì§ì ‘ ë„êµ¬ ì‹¤í–‰]
    E -->|Medium| F
    E -->|Complex| G[Plan & Execute]
    F --> H[ê²°ê³¼ ë°˜í™˜]
    G --> I[ê³„íš ìˆ˜ë¦½]
    I --> J[ë‹¨ê³„ë³„ ì‹¤í–‰]
    J --> K[ê²°ê³¼ í†µí•©]
    K --> H
```

ë³µì¡ë„ íŒë‹¨ ê¸°ì¤€
===============

### Simple (ë‹¨ìˆœ)
- 1-2ê°œ ë„êµ¬ë¡œ ì¦‰ì‹œ í•´ê²° ê°€ëŠ¥
- ì˜ˆì‹œ: "í˜„ì¬ ì‹œê°„ ì•Œë ¤ì¤˜", "ë‚ ì”¨ ê²€ìƒ‰í•´ì¤˜"
- ì²˜ë¦¬ ë°©ì‹: BaseAgent.auto_tool_flow() ì§ì ‘ í˜¸ì¶œ

### Medium (ì¤‘ê°„)
- 2-3ê°œ ë„êµ¬ì˜ ìˆœì°¨ ì‹¤í–‰ìœ¼ë¡œ í•´ê²° ê°€ëŠ¥
- ì˜ˆì‹œ: "íŒŒì¼ ì½ê³  ìš”ì•½í•´ì¤˜", "ê²€ìƒ‰ í›„ ê²°ê³¼ ì €ì¥í•´ì¤˜"
- ì²˜ë¦¬ ë°©ì‹: ë‹¨ìˆœ ì²˜ë¦¬ë¡œ í´ë°± (ëŒ€ë¶€ë¶„ í•´ê²° ê°€ëŠ¥)

### Complex (ë³µì¡)
- ë‹¤ë‹¨ê³„ ê³„íšì´ í•„ìš”í•œ ë³µí•© ì‘ì—…
- ì˜ˆì‹œ: "ì—¬ëŸ¬ ì†ŒìŠ¤ ê²€ìƒ‰ í›„ ë¹„êµ ë¶„ì„í•˜ì—¬ ë³´ê³ ì„œ ì‘ì„±"
- ì²˜ë¦¬ ë°©ì‹: Plan & Execute ì „ëµ

ì‚¬ìš©ë²• ë° ì˜ˆì‹œ
=============

### 1. ê¸°ë³¸ ì‚¬ìš©ë²•

```python
from dspilot_core.llm.workflow import SmartWorkflow

# ì›Œí¬í”Œë¡œìš° ì´ˆê¸°í™”
workflow = SmartWorkflow(llm_service=llm_service, mcp_tool_manager=tool_manager)

# ì‹¤í–‰
result = await workflow.run(agent, "ì‚¬ìš©ì ìš”ì²­", streaming_callback)
```

### 2. ìŠ¤íŠ¸ë¦¬ë° ì½œë°±ê³¼ í•¨ê»˜ ì‚¬ìš©

```python
def progress_callback(content: str):
    print(f"[ì§„í–‰ìƒí™©] {content}")

result = await workflow.run(
    agent=my_agent,
    user_message="ë³µì¡í•œ ì‘ì—… ìš”ì²­",
    streaming_callback=progress_callback
)
```

### 3. ì—ì´ì „íŠ¸ì—ì„œ í†µí•© ì‚¬ìš©

```python
class ProblemAgent(BaseAgent):
    def _get_workflow_name(self, mode: str) -> str:
        # ëŒ€ë¶€ë¶„ì˜ ëª¨ë“œë¥¼ SmartWorkflowë¡œ í†µí•©
        if mode in ["mcp_tools", "workflow", "auto"]:
            return "smart"
        elif mode == "basic":
            return "basic"
        elif mode == "research":
            return "research"
        else:
            return "smart"  # ê¸°ë³¸ê°’
```

ë‚´ë¶€ êµ¬í˜„ ì„¸ë¶€ì‚¬í•­
=================

### ë³µì¡ë„ ë¶„ì„ í”„ë¡¬í”„íŠ¸
```
ë‹¤ìŒ ì‚¬ìš©ì ìš”ì²­ì˜ ë³µì¡ë„ë¥¼ ë¶„ì„í•´ì£¼ì„¸ìš”:
ì‚¬ìš©ì ìš”ì²­: {message}
ì‚¬ìš© ê°€ëŠ¥í•œ ë„êµ¬ë“¤: {tools_list}

ë³µì¡ë„ ê¸°ì¤€:
- simple: 1-2ê°œ ë„êµ¬ë¡œ ì¦‰ì‹œ í•´ê²° ê°€ëŠ¥
- medium: 2-3ê°œ ë„êµ¬ ìˆœì°¨ ì‹¤í–‰ìœ¼ë¡œ í•´ê²° ê°€ëŠ¥  
- complex: ë‹¤ë‹¨ê³„ ê³„íšì´ í•„ìš”í•œ ë³µí•© ì‘ì—…

JSON ì‘ë‹µ: {"complexity": "simple|medium|complex", "reason": "íŒë‹¨ ì´ìœ "}
```

### Plan & Execute ì „ëµ
1. **ê³„íš ìˆ˜ë¦½**: LLMì´ JSON í˜•íƒœì˜ ì‹¤í–‰ ê³„íš ìƒì„±
2. **ë‹¨ê³„ë³„ ì‹¤í–‰**: MCP ë„êµ¬ë¥¼ ìˆœì°¨ì ìœ¼ë¡œ í˜¸ì¶œ
3. **ê²°ê³¼ í†µí•©**: ëª¨ë“  ê²°ê³¼ë¥¼ ì¢…í•©í•˜ì—¬ ìµœì¢… ì‘ë‹µ ìƒì„±

ê³ ê¸‰ ê¸°ëŠ¥ (AdaptiveWorkflow í†µí•©)
=================================

### 1. ë°˜ë³µì  ê³„íš ê°œì„ 
- **ì¤‘ë³µ ê³„íš ê°ì§€**: í•´ì‹œ ê¸°ë°˜ìœ¼ë¡œ ì´ì „ì— ì‹¤í–‰í•œ ê³„íšê³¼ ì¤‘ë³µ ë°©ì§€
- **ì‹¤íŒ¨ ë¶„ì„**: ì‹¤í–‰ ì‹¤íŒ¨ ì›ì¸ì„ ë¶„ì„í•˜ì—¬ ë‹¤ìŒ ê³„íšì— ë°˜ì˜
- **ê³„íš ìˆ˜ì •**: ì‹¤íŒ¨ìœ¨ì´ ë†’ì„ ë•Œ ìë™ìœ¼ë¡œ ê³„íšì„ ê°œì„ í•˜ì—¬ ì¬ì‹œë„
- **ìµœëŒ€ 3íšŒ ë°˜ë³µ**: ê³„íš ìˆ˜ë¦½ â†’ ì‹¤í–‰ â†’ í‰ê°€ â†’ ê°œì„  ì‚¬ì´í´

### 2. ê³ ê¸‰ ì‹¤í–‰ ì œì–´
- **ë‹¨ê³„ë³„ ì¬ì‹œë„**: ê° ë‹¨ê³„ë§ˆë‹¤ ìµœëŒ€ 3íšŒ ì¬ì‹œë„ (ì„¤ì • ê°€ëŠ¥)
- **ë§¤ê°œë³€ìˆ˜ ë™ì  ì¹˜í™˜**: ì´ì „ ë‹¨ê³„ ê²°ê³¼ë¥¼ ë‹¤ìŒ ë‹¨ê³„ ë§¤ê°œë³€ìˆ˜ë¡œ ìë™ ì¹˜í™˜
- **ì‹¤í–‰ ê²°ê³¼ ê²€ì¦**: ê° ë‹¨ê³„ ê²°ê³¼ì˜ ìœ íš¨ì„±ì„ ìë™ ê²€ì¦
- **ì‚¬ìš©ì ìŠ¹ì¸ ëª¨ë“œ**: ì¤‘ìš”í•œ ë‹¨ê³„ ì‹¤í–‰ ì „ ì‚¬ìš©ì ìŠ¹ì¸ ìš”ì²­ (ì„ íƒì‚¬í•­)

### 3. ì»¨í…ìŠ¤íŠ¸ ë©”ëª¨ë¦¬ ê´€ë¦¬
- **ë‹¨ê³„ë³„ ë©”ëª¨ë¦¬**: ê° ë‹¨ê³„ì˜ ì‹¤í–‰ ê²°ê³¼ì™€ ë©”íƒ€ë°ì´í„° ì €ì¥
- **ì‹¤í–‰ ì»¨í…ìŠ¤íŠ¸**: ì „ì²´ ì‹¤í–‰ ê³¼ì •ì˜ ìƒíƒœ ì •ë³´ ìœ ì§€
- **ì‹¤íŒ¨ ì´ë ¥ ê´€ë¦¬**: ì‹¤íŒ¨ íŒ¨í„´ ë¶„ì„ìœ¼ë¡œ í–¥í›„ ê³„íš ê°œì„ 

### 4. ì§€ëŠ¥í˜• ì˜¤ë¥˜ ë³µêµ¬
- **ê³µí†µ ì˜¤ë¥˜ íŒ¨í„´ ë¶„ì„**: ê¶Œí•œ, íŒŒì¼ ê²½ë¡œ, ë„¤íŠ¸ì›Œí¬ ë“± ì˜¤ë¥˜ ìœ í˜•ë³„ ë¶„ì„
- **ìë™ í´ë°±**: ë³µì¡ ì²˜ë¦¬ ì‹¤íŒ¨ ì‹œ ë‹¨ìˆœ ì²˜ë¦¬ë¡œ ìë™ ì „í™˜
- **ë¶€ë¶„ ì„±ê³µ ì²˜ë¦¬**: ì¼ë¶€ ë‹¨ê³„ ì‹¤íŒ¨ ì‹œì—ë„ ì„±ê³µí•œ ë¶€ë¶„ì˜ ê²°ê³¼ í™œìš©

ê³ ê¸‰ ì‚¬ìš©ë²•
===========

### 1. ì¸í„°ë™ì…˜ ëª¨ë“œ ì„¤ì •

```python
# ì‚¬ìš©ì ìŠ¹ì¸ ëª¨ë“œ í™œì„±í™”
workflow = SmartWorkflow()
workflow.set_interaction_mode(True)

# ìë™ ì‹¤í–‰ ëª¨ë“œ (ê¸°ë³¸ê°’)
workflow.set_interaction_mode(False)
```

### 2. ì‹¤í–‰ í†µê³„ í™•ì¸

```python
# ì‹¤í–‰ í†µê³„ ì¡°íšŒ
stats = workflow.get_execution_statistics()
print(f"ì‹¤í–‰ëœ ê³„íš ìˆ˜: {stats['executed_plans']}")
print(f"ë‹¨ê³„ ë©”ëª¨ë¦¬ í¬ê¸°: {stats['step_memory_size']}")
```

### 3. ì‹¤í–‰ íˆìŠ¤í† ë¦¬ ê´€ë¦¬

```python
# íˆìŠ¤í† ë¦¬ ì´ˆê¸°í™” (ë©”ëª¨ë¦¬ ì ˆì•½)
workflow.clear_execution_history()
```

### 4. ë§¤ê°œë³€ìˆ˜ ë™ì  ì¹˜í™˜ ì˜ˆì‹œ

```json
{
  "steps": [
    {
      "step": 1,
      "tool": "file_read",
      "args": {"path": "data.txt"},
      "desc": "ë°ì´í„° íŒŒì¼ ì½ê¸°"
    },
    {
      "step": 2,
      "tool": "text_process",
      "args": {"content": "$step_1"},
      "desc": "ì½ì€ ë°ì´í„° ì²˜ë¦¬"
    }
  ]
}
```

ì„±ëŠ¥ ìµœì í™”
===========

- **ìºì‹±**: ë„êµ¬ ëª©ë¡ê³¼ ë³µì¡ë„ ë¶„ì„ ê²°ê³¼ ìºì‹±
- **ë³‘ë ¬ ì²˜ë¦¬**: ë…ë¦½ì ì¸ ë‹¨ê³„ë“¤ì˜ ë³‘ë ¬ ì‹¤í–‰ ì§€ì› (í–¥í›„ í™•ì¥)
- **ì˜¤ë¥˜ ë³µêµ¬**: ë‹¤ì¸µ í´ë°± ë©”ì»¤ë‹ˆì¦˜ìœ¼ë¡œ ì•ˆì •ì„± í™•ë³´
- **ë¦¬ì†ŒìŠ¤ ê´€ë¦¬**: ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ ê²°ê³¼ ì €ì¥ ë° íˆìŠ¤í† ë¦¬ ê´€ë¦¬
- **ì§€ëŠ¥í˜• ì¬ì‹œë„**: ì‹¤íŒ¨ ì›ì¸ ë¶„ì„ ê¸°ë°˜ ì„ íƒì  ì¬ì‹œë„

í™•ì¥ì„±
======

ìƒˆë¡œìš´ MCP ë„êµ¬ê°€ ì¶”ê°€ë˜ì–´ë„ ì½”ë“œ ìˆ˜ì • ì—†ì´ ìë™ìœ¼ë¡œ ì§€ì›ë©ë‹ˆë‹¤.
ë„êµ¬ì˜ ë©”íƒ€ë°ì´í„°(ì´ë¦„, ì„¤ëª…)ë§Œìœ¼ë¡œ ë³µì¡ë„ ë¶„ì„ê³¼ ê³„íš ìˆ˜ë¦½ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤.

### í™•ì¥ ê°€ëŠ¥í•œ ìš”ì†Œë“¤:
- **ê²€ì¦ ê·œì¹™**: ë„êµ¬ë³„ ë§ì¶¤ ê²°ê³¼ ê²€ì¦ ë¡œì§ ì¶”ê°€
- **ì¬ì‹œë„ ì „ëµ**: ì˜¤ë¥˜ ìœ í˜•ë³„ ì¬ì‹œë„ ì „ëµ ì»¤ìŠ¤í„°ë§ˆì´ì§•
- **ê³„íš ê°œì„  ì•Œê³ ë¦¬ì¦˜**: ë” ì •êµí•œ ê³„íš ê°œì„  ë¡œì§ êµ¬í˜„

ì œí•œì‚¬í•­
========

- **ë³µì¡ë„ ë¶„ì„ ì •í™•ë„**: LLM ì„±ëŠ¥ì— ì˜ì¡´
- **ê³„íš ìˆ˜ë¦½ ì‹œê°„**: ë³µì¡í•œ ì‘ì—…ì˜ ê²½ìš° ë‹¤ì†Œ ì‹œê°„ ì†Œìš”
- **ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰**: ì‹¤í–‰ íˆìŠ¤í† ë¦¬ê°€ ëˆ„ì ë˜ë©´ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¦ê°€
- **ìˆœì°¨ ì‹¤í–‰**: í˜„ì¬ëŠ” ë‹¨ê³„ë³„ ìˆœì°¨ ì‹¤í–‰ë§Œ ì§€ì› (ë³‘ë ¬ ì‹¤í–‰ì€ í–¥í›„ í™•ì¥)

ë¬¸ì œ í•´ê²°
=========

### ë³µì¡ë„ ë¶„ì„ ì‹¤íŒ¨
- ìë™ìœ¼ë¡œ 'simple'ë¡œ í´ë°±í•˜ì—¬ ì•ˆì „í•˜ê²Œ ì²˜ë¦¬
- ë¡œê·¸ì—ì„œ ë¶„ì„ ì‹¤íŒ¨ ì›ì¸ í™•ì¸ ê°€ëŠ¥

### ê³„íš ìˆ˜ë¦½ ì‹¤íŒ¨  
- ë‹¨ìˆœ ì²˜ë¦¬ ë°©ì‹ìœ¼ë¡œ ìë™ í´ë°±
- ëŒ€ë¶€ë¶„ì˜ ìš”ì²­ì€ ë‹¨ìˆœ ì²˜ë¦¬ë¡œë„ í•´ê²° ê°€ëŠ¥

### ë°˜ë³µì  ì‹¤í–‰ ì‹¤íŒ¨
- ìµœëŒ€ 3íšŒ ê³„íš ìˆ˜ì • ì‹œë„ í›„ ë¶€ë¶„ ê²°ê³¼ë¼ë„ ë°˜í™˜
- ì‹¤íŒ¨ ì›ì¸ ë¶„ì„ì„ í†µí•œ ê·¼ë³¸ ë¬¸ì œ í•´ê²° ê°€ì´ë“œ ì œê³µ

### ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¦ê°€
- `clear_execution_history()` í˜¸ì¶œë¡œ ì£¼ê¸°ì  íˆìŠ¤í† ë¦¬ ì •ë¦¬
- ì¥ì‹œê°„ ì‹¤í–‰ ì‹œ ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§ ê¶Œì¥

### ë„êµ¬ ì‹¤í–‰ ì‹¤íŒ¨
- ë‹¨ê³„ë³„ ì¬ì‹œë„ ë©”ì»¤ë‹ˆì¦˜ìœ¼ë¡œ ì¼ì‹œì  ì˜¤ë¥˜ ìë™ ë³µêµ¬
- ì˜êµ¬ì  ì‹¤íŒ¨ ì‹œ ëŒ€ì•ˆ ê³„íš ìë™ ìƒì„±
"""

import json
import logging
from typing import Any, Callable, Dict, List, Optional

from dspilot_core.llm.models.conversation_message import ConversationMessage
from dspilot_core.llm.models.llm_response import LLMResponse
from dspilot_core.llm.workflow.base_workflow import BaseWorkflow

logger = logging.getLogger(__name__)


class SmartWorkflow(BaseWorkflow):
    """
    í†µí•© ìŠ¤ë§ˆíŠ¸ ì›Œí¬í”Œë¡œìš°
    
    ì‚¬ìš©ì ìš”ì²­ì˜ ë³µì¡ë„ë¥¼ ìë™ ë¶„ì„í•˜ì—¬ ìµœì ì˜ ì²˜ë¦¬ ë°©ì‹ì„ ì„ íƒí•˜ëŠ” ì§€ëŠ¥í˜• ì›Œí¬í”Œë¡œìš°ì…ë‹ˆë‹¤.
    
    - ë‹¨ìˆœ ìš”ì²­: ì§ì ‘ ë„êµ¬ ì‹¤í–‰ìœ¼ë¡œ ë¹ ë¥¸ ì²˜ë¦¬
    - ë³µì¡ ìš”ì²­: Plan & Executeë¡œ ì •í™•í•œ ì²˜ë¦¬
    - ëª¨ë“  MCP ë„êµ¬ì™€ í˜¸í™˜ë˜ëŠ” ë²”ìš© ì›Œí¬í”Œë¡œìš°
    
    Attributes:
        llm_service: LLM ì„œë¹„ìŠ¤ ì¸ìŠ¤í„´ìŠ¤
        mcp_tool_manager: MCP ë„êµ¬ ê´€ë¦¬ì
        workflow_name: ì›Œí¬í”Œë¡œìš° ì‹ë³„ëª… ("smart")
        max_iterations: ìµœëŒ€ ë°˜ë³µ íšŸìˆ˜ (ê¸°ë³¸ê°’: 10)
        context_window: ì»¨í…ìŠ¤íŠ¸ ìœˆë„ìš° í¬ê¸° (ê¸°ë³¸ê°’: 20)
    """

    def __init__(self, llm_service=None, mcp_tool_manager=None):
        """
        SmartWorkflow ì´ˆê¸°í™”
        
        Args:
            llm_service: LLM ì„œë¹„ìŠ¤ ì¸ìŠ¤í„´ìŠ¤ (ì„ íƒì‚¬í•­)
            mcp_tool_manager: MCP ë„êµ¬ ê´€ë¦¬ì (ì„ íƒì‚¬í•­)
        """
        self.llm_service = llm_service
        self.mcp_tool_manager = mcp_tool_manager
        self.workflow_name = "smart"
        self.max_iterations = 10
        self.context_window = 20
        
        # AdaptiveWorkflow ê³ ê¸‰ ê¸°ëŠ¥ë“¤ ì¶”ê°€
        self.execution_context = {}  # ì‹¤í–‰ ì»¨í…ìŠ¤íŠ¸ ì €ì¥
        self.step_memory = {}  # ë‹¨ê³„ë³„ ë©”ëª¨ë¦¬
        self.executed_plan_hashes = set()  # ì¤‘ë³µ ê³„íš ê°ì§€
        self.retry_count = {}  # ë‹¨ê³„ë³„ ì¬ì‹œë„ íšŸìˆ˜
        self.max_retries = 3  # ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜
        self.interaction_mode = True  # ì‚¬ìš©ì ìŠ¹ì¸ ëª¨ë“œ

    async def run(
        self,
        agent: "BaseAgent",
        user_message: str,
        streaming_callback: Optional[Callable[[str], None]] = None,
    ) -> str:
        """
        ìŠ¤ë§ˆíŠ¸ ì›Œí¬í”Œë¡œìš° ì‹¤í–‰
        
        ì‚¬ìš©ì ìš”ì²­ì„ ë¶„ì„í•˜ì—¬ ë³µì¡ë„ì— ë”°ë¼ ìµœì ì˜ ì²˜ë¦¬ ë°©ì‹ì„ ìë™ ì„ íƒí•©ë‹ˆë‹¤.
        
        Args:
            agent: ì‹¤í–‰í•  BaseAgent ì¸ìŠ¤í„´ìŠ¤
            user_message: ì‚¬ìš©ì ìš”ì²­ ë©”ì‹œì§€
            streaming_callback: ì‹¤ì‹œê°„ ì§„í–‰ ìƒí™© ì½œë°± í•¨ìˆ˜
            
        Returns:
            str: ì²˜ë¦¬ ê²°ê³¼ ë©”ì‹œì§€
            
        Raises:
            Exception: ì›Œí¬í”Œë¡œìš° ì‹¤í–‰ ì¤‘ ë³µêµ¬ ë¶ˆê°€ëŠ¥í•œ ì˜¤ë¥˜ ë°œìƒ ì‹œ
        """
        try:
            logger.info("=== SmartWorkflow: ì²˜ë¦¬ ì‹œì‘ ===")

            # 1. ë„êµ¬ ì‚¬ìš© ê°€ëŠ¥ì„± í™•ì¸
            available_tools = await self._get_available_tools()
            if not available_tools:
                logger.info("ì‚¬ìš© ê°€ëŠ¥í•œ ë„êµ¬ê°€ ì—†ìŒ - LLM ì‘ë‹µìœ¼ë¡œ ì²˜ë¦¬")
                return await self._generate_llm_response(agent, user_message, streaming_callback)

            # 2. ìš”ì²­ ë³µì¡ë„ ë¶„ì„
            complexity = await self._analyze_complexity(agent, user_message, available_tools)
            logger.info(f"ìš”ì²­ ë³µì¡ë„ ë¶„ì„ ê²°ê³¼: {complexity}")

            # 3. ë³µì¡ë„ì— ë”°ë¥¸ ì²˜ë¦¬ ë°©ì‹ ì„ íƒ
            if complexity == "simple":
                return await self._handle_simple_request(agent, user_message, streaming_callback)
            elif complexity == "complex":
                return await self._handle_complex_request(agent, user_message, streaming_callback)
            else:
                # ì¤‘ê°„ ë³µì¡ë„ëŠ” ë‹¨ìˆœ ë°©ì‹ìœ¼ë¡œ ì²˜ë¦¬ (ëŒ€ë¶€ë¶„ í•´ê²° ê°€ëŠ¥)
                return await self._handle_simple_request(agent, user_message, streaming_callback)

        except Exception as e:
            logger.error(f"SmartWorkflow ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
            return f"ì›Œí¬í”Œë¡œìš° ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"

    async def _get_available_tools(self) -> List[Any]:
        """
        ì‚¬ìš© ê°€ëŠ¥í•œ MCP ë„êµ¬ ëª©ë¡ ë°˜í™˜
        
        Returns:
            List[Any]: ì‚¬ìš© ê°€ëŠ¥í•œ ë„êµ¬ ëª©ë¡ (ë¹ˆ ë¦¬ìŠ¤íŠ¸ ê°€ëŠ¥)
        """
        if not self.mcp_tool_manager or not hasattr(self.mcp_tool_manager, 'get_langchain_tools'):
            return []
        
        try:
            tools = await self.mcp_tool_manager.get_langchain_tools()
            logger.debug(f"ì‚¬ìš© ê°€ëŠ¥í•œ ë„êµ¬ ìˆ˜: {len(tools)}")
            return tools
        except Exception as e:
            logger.warning(f"ë„êµ¬ ëª©ë¡ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {e}")
            return []

    async def _analyze_complexity(self, agent: Any, message: str, tools: List[Any]) -> str:
        """
        ì‚¬ìš©ì ìš”ì²­ì˜ ë³µì¡ë„ ë¶„ì„
        
        LLMì—ê²Œ ìš”ì²­ ë‚´ìš©ê³¼ ì‚¬ìš© ê°€ëŠ¥í•œ ë„êµ¬ ëª©ë¡ì„ ì œê³µí•˜ì—¬
        simple/medium/complex ì¤‘ í•˜ë‚˜ë¡œ ë¶„ë¥˜í•˜ë„ë¡ í•©ë‹ˆë‹¤.
        
        Args:
            agent: BaseAgent ì¸ìŠ¤í„´ìŠ¤
            message: ì‚¬ìš©ì ìš”ì²­ ë©”ì‹œì§€  
            tools: ì‚¬ìš© ê°€ëŠ¥í•œ ë„êµ¬ ëª©ë¡
            
        Returns:
            str: "simple", "medium", "complex" ì¤‘ í•˜ë‚˜ (ê¸°ë³¸ê°’: "simple")
        """
        tools_desc = "\n".join([f"- {tool.name}: {tool.description}" for tool in tools])
        
        analysis_prompt = f"""ë‹¤ìŒ ì‚¬ìš©ì ìš”ì²­ì˜ ë³µì¡ë„ë¥¼ ë¶„ì„í•´ì£¼ì„¸ìš”:

ì‚¬ìš©ì ìš”ì²­: {message}

ì‚¬ìš© ê°€ëŠ¥í•œ ë„êµ¬ë“¤:
{tools_desc}

ë³µì¡ë„ ê¸°ì¤€:
- simple: 1-2ê°œ ë„êµ¬ë¡œ ì¦‰ì‹œ í•´ê²° ê°€ëŠ¥ (ì‹œê°„ ì¡°íšŒ, ë‹¨ìˆœ ê²€ìƒ‰, íŒŒì¼ ì½ê¸° ë“±)
- medium: 2-3ê°œ ë„êµ¬ ìˆœì°¨ ì‹¤í–‰ìœ¼ë¡œ í•´ê²° ê°€ëŠ¥ (ê²€ìƒ‰ í›„ ì €ì¥, íŒŒì¼ ì²˜ë¦¬ í›„ ë¶„ì„ ë“±)
- complex: ë‹¤ë‹¨ê³„ ê³„íšì´ í•„ìš”í•œ ë³µí•© ì‘ì—… (ì—¬ëŸ¬ ê²€ìƒ‰ + ë¶„ì„ + ë³´ê³ ì„œ ì‘ì„± ë“±)

JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µ:
{{
    "complexity": "simple|medium|complex",
    "reason": "íŒë‹¨ ì´ìœ ",
    "estimated_steps": ì˜ˆìƒë‹¨ê³„ìˆ˜
}}"""

        try:
            response = await agent._generate_basic_response(analysis_prompt, None)
            result = self._extract_json_from_response(response)
            complexity = result.get("complexity", "simple")
            reason = result.get("reason", "ë¶„ì„ ì‹¤íŒ¨")
            logger.debug(f"ë³µì¡ë„ ë¶„ì„: {complexity} - {reason}")
            return complexity
        except Exception as e:
            logger.warning(f"ë³µì¡ë„ ë¶„ì„ ì‹¤íŒ¨: {e} - ê¸°ë³¸ê°’ 'simple' ì‚¬ìš©")
            return "simple"

    async def _handle_simple_request(
        self, agent: Any, message: str, streaming_callback: Optional[Callable[[str], None]] = None
    ) -> str:
        """
        ë‹¨ìˆœ ìš”ì²­ ì²˜ë¦¬
        
        BaseAgentì˜ auto_tool_flowë¥¼ í™œìš©í•˜ì—¬ ì§ì ‘ì ì´ê³  ë¹ ë¥¸ ì²˜ë¦¬ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.
        ëŒ€ë¶€ë¶„ì˜ ì¼ë°˜ì ì¸ ìš”ì²­ë“¤ì´ ì´ ë°©ì‹ìœ¼ë¡œ íš¨ìœ¨ì ìœ¼ë¡œ ì²˜ë¦¬ë©ë‹ˆë‹¤.
        
        Args:
            agent: BaseAgent ì¸ìŠ¤í„´ìŠ¤
            message: ì‚¬ìš©ì ìš”ì²­ ë©”ì‹œì§€
            streaming_callback: ìŠ¤íŠ¸ë¦¬ë° ì½œë°±
            
        Returns:
            str: ì²˜ë¦¬ ê²°ê³¼
        """
        logger.info("ë‹¨ìˆœ ìš”ì²­ìœ¼ë¡œ ì²˜ë¦¬ - ì§ì ‘ ë„êµ¬ ì‹¤í–‰")
        
        if streaming_callback:
            streaming_callback("ğŸ”§ ë„êµ¬ ì‹¤í–‰ ì¤‘...\n")

        # BaseAgentì˜ auto_tool_flow í™œìš©
        if hasattr(agent, 'auto_tool_flow'):
            try:
                result = await agent.auto_tool_flow(message, streaming_callback)
                if result and isinstance(result, dict):
                    return result.get("response", "ë„êµ¬ ì‹¤í–‰ ì™„ë£Œ")
                elif result:
                    return str(result)
            except Exception as e:
                logger.warning(f"auto_tool_flow ì‹¤í–‰ ì‹¤íŒ¨: {e} - LLM ì‘ë‹µìœ¼ë¡œ í´ë°±")
        
        # í´ë°±: ì¼ë°˜ LLM ì‘ë‹µ
        return await self._generate_llm_response(agent, message, streaming_callback)

    async def _handle_complex_request(
        self, agent: Any, message: str, streaming_callback: Optional[Callable[[str], None]] = None
    ) -> str:
        """
        ë³µì¡ ìš”ì²­ ì²˜ë¦¬ (ê³ ê¸‰ ê¸°ëŠ¥ í¬í•¨)
        
        AdaptiveWorkflowì˜ ê³ ê¸‰ Plan & Execute ì „ëµ:
        1. ì‹¤í–‰ ê³„íš ìˆ˜ë¦½ ë° ì¤‘ë³µ ê°ì§€
        2. ê³„íš ê°œì„  ë° ìµœì í™”
        3. ë‹¨ê³„ë³„ ì‹¤í–‰ (ì¬ì‹œë„, ê²€ì¦ í¬í•¨)
        4. ì‹¤íŒ¨ ì‹œ ê³„íš ìˆ˜ì • ë° ì¬ì‹œë„
        5. ê²°ê³¼ í†µí•© ë° í’ˆì§ˆ í‰ê°€
        
        Args:
            agent: BaseAgent ì¸ìŠ¤í„´ìŠ¤  
            message: ì‚¬ìš©ì ìš”ì²­ ë©”ì‹œì§€
            streaming_callback: ìŠ¤íŠ¸ë¦¬ë° ì½œë°±
            
        Returns:
            str: í†µí•©ëœ ìµœì¢… ê²°ê³¼
        """
        logger.info("ë³µì¡ ìš”ì²­ìœ¼ë¡œ ì²˜ë¦¬ - ê³ ê¸‰ Plan & Execute ì „ëµ")
        
        max_plan_iterations = 3  # ìµœëŒ€ ê³„íš ìˆ˜ì • íšŸìˆ˜
        plan_iteration = 0
        
        while plan_iteration < max_plan_iterations:
            try:
                # 1. ê³„íš ìˆ˜ë¦½
                plan = await self._create_execution_plan(agent, message, streaming_callback)
                if not plan or not plan.get("steps"):
                    logger.warning("ê³„íš ìˆ˜ë¦½ ì‹¤íŒ¨ - ë‹¨ìˆœ ì²˜ë¦¬ë¡œ í´ë°±")
                    return await self._handle_simple_request(agent, message, streaming_callback)

                # 2. ì¤‘ë³µ ê³„íš ê°ì§€
                if self._is_duplicate_plan(plan):
                    logger.warning(f"ì¤‘ë³µ ê³„íš ê°ì§€ (ë°˜ë³µ {plan_iteration + 1})")
                    if streaming_callback:
                        streaming_callback("âš ï¸ ì¤‘ë³µ ê³„íš ê°ì§€, ë‹¤ë¥¸ ì ‘ê·¼ ë°©ë²• ì‹œë„ ì¤‘...\n")
                    
                    # ê³„íš ìˆ˜ì • ìš”ì²­
                    plan = await self._refine_execution_plan(agent, message, plan, streaming_callback)
                    if not plan or not plan.get("steps"):
                        break

                # 3. ê³„íš ì‹¤í–‰
                if streaming_callback:
                    streaming_callback(f"ğŸ“‹ ì‹¤í–‰ ê³„íš ìŠ¹ì¸ë¨ (ë°˜ë³µ {plan_iteration + 1})\n")
                
                results = await self._execute_plan(agent, plan, streaming_callback)
                
                # 4. ì‹¤í–‰ ê²°ê³¼ í‰ê°€
                success_rate = sum(1 for r in results.values() if r.get("success")) / len(results) if results else 0
                
                if success_rate >= 0.7:  # 70% ì´ìƒ ì„±ê³µ ì‹œ ê²°ê³¼ í†µí•©
                    return await self._integrate_results(agent, message, plan, results, streaming_callback)
                else:
                    # ì‹¤íŒ¨ìœ¨ì´ ë†’ìœ¼ë©´ ê³„íš ìˆ˜ì • ì‹œë„
                    logger.warning(f"ì‹¤í–‰ ì„±ê³µë¥  ë‚®ìŒ: {success_rate:.1%}, ê³„íš ìˆ˜ì • ì‹œë„")
                    if streaming_callback:
                        streaming_callback(f"âš ï¸ ì‹¤í–‰ ì„±ê³µë¥  ë‚®ìŒ ({success_rate:.1%}), ê³„íš ìˆ˜ì • ì¤‘...\n")
                    
                    plan_iteration += 1
                    
                    # ì‹¤íŒ¨ ì›ì¸ ë¶„ì„í•˜ì—¬ ë‹¤ìŒ ê³„íšì— ë°˜ì˜
                    failure_context = self._analyze_execution_failures(results)
                    self.execution_context["failure_analysis"] = failure_context
                    
                    if plan_iteration >= max_plan_iterations:
                        # ìµœëŒ€ ì‹œë„ íšŸìˆ˜ ë„ë‹¬ ì‹œ ë¶€ë¶„ ì„±ê³µ ê²°ê³¼ë¼ë„ ë°˜í™˜
                        logger.info("ìµœëŒ€ ê³„íš ìˆ˜ì • íšŸìˆ˜ ë„ë‹¬, ë¶€ë¶„ ê²°ê³¼ ë°˜í™˜")
                        return await self._integrate_results(agent, message, plan, results, streaming_callback)

            except Exception as e:
                logger.error(f"ë³µì¡ ìš”ì²­ ì²˜ë¦¬ ì‹¤íŒ¨ (ë°˜ë³µ {plan_iteration + 1}): {e}")
                plan_iteration += 1
                
                if plan_iteration >= max_plan_iterations:
                    logger.error("ìµœëŒ€ ì‹œë„ íšŸìˆ˜ ë„ë‹¬ - ë‹¨ìˆœ ì²˜ë¦¬ë¡œ í´ë°±")
                    return await self._handle_simple_request(agent, message, streaming_callback)
        
        # ëª¨ë“  ì‹œë„ ì‹¤íŒ¨ ì‹œ ë‹¨ìˆœ ì²˜ë¦¬ë¡œ í´ë°±
        logger.warning("ëª¨ë“  ë³µì¡ ì²˜ë¦¬ ì‹œë„ ì‹¤íŒ¨ - ë‹¨ìˆœ ì²˜ë¦¬ë¡œ í´ë°±")
        return await self._handle_simple_request(agent, message, streaming_callback)

    async def _create_execution_plan(
        self, agent: Any, message: str, streaming_callback: Optional[Callable[[str], None]] = None
    ) -> Dict[str, Any]:
        """
        ì‹¤í–‰ ê³„íš ìˆ˜ë¦½
        
        ì‚¬ìš©ì ìš”ì²­ì„ ë¶„ì„í•˜ì—¬ ë‹¨ê³„ë³„ ì‹¤í–‰ ê³„íšì„ JSON í˜•íƒœë¡œ ìƒì„±í•©ë‹ˆë‹¤.
        ê° ë‹¨ê³„ëŠ” ì‚¬ìš©í•  ë„êµ¬ì™€ ë§¤ê°œë³€ìˆ˜ë¥¼ í¬í•¨í•©ë‹ˆë‹¤.
        
        Args:
            agent: BaseAgent ì¸ìŠ¤í„´ìŠ¤
            message: ì‚¬ìš©ì ìš”ì²­ ë©”ì‹œì§€  
            streaming_callback: ìŠ¤íŠ¸ë¦¬ë° ì½œë°±
            
        Returns:
            Dict[str, Any]: ì‹¤í–‰ ê³„íš (ë¹ˆ ë”•ì…”ë„ˆë¦¬ ê°€ëŠ¥)
        """
        tools = await self._get_available_tools()
        tools_desc = "\n".join([f"- {tool.name}: {tool.description}" for tool in tools])
        
        planning_prompt = f"""ì‚¬ìš©ì ìš”ì²­ì„ ë¶„ì„í•˜ì—¬ ë‹¨ê³„ë³„ ì‹¤í–‰ ê³„íšì„ ìˆ˜ë¦½í•´ì£¼ì„¸ìš”:

ì‚¬ìš©ì ìš”ì²­: {message}

ì‚¬ìš© ê°€ëŠ¥í•œ ë„êµ¬ë“¤:
{tools_desc}

ë‹¤ìŒ í˜•ì‹ì˜ JSONìœ¼ë¡œ ê³„íšì„ ì‘ì„±í•´ì£¼ì„¸ìš”:
{{
    "goal": "ìµœì¢… ëª©í‘œ ì„¤ëª…",
    "steps": [
        {{
            "step": 1,
            "tool": "ì‚¬ìš©í• _ë„êµ¬ëª…",
            "args": {{"ë§¤ê°œë³€ìˆ˜": "ê°’"}},
            "desc": "ì´ ë‹¨ê³„ì—ì„œ ìˆ˜í–‰í•  ì‘ì—… ì„¤ëª…"
        }},
        {{
            "step": 2,
            "tool": "ë‹¤ìŒ_ë„êµ¬ëª…", 
            "args": {{"ë§¤ê°œë³€ìˆ˜": "ê°’"}},
            "desc": "ë‹¤ìŒ ë‹¨ê³„ ì‘ì—… ì„¤ëª…"
        }}
    ]
}}

ë‹¨ê³„ë³„ ì‹¤í–‰ì´ ë…¼ë¦¬ì  ìˆœì„œë¥¼ ë”°ë¥´ë„ë¡ ê³„íší•´ì£¼ì„¸ìš”."""

        if streaming_callback:
            streaming_callback("ğŸ“‹ ì‹¤í–‰ ê³„íš ìˆ˜ë¦½ ì¤‘...\n")

        try:
            response = await agent._generate_basic_response(planning_prompt, None)
            plan = self._extract_json_from_response(response)
            
            if plan and plan.get("steps"):
                logger.debug(f"ì‹¤í–‰ ê³„íš ìˆ˜ë¦½ ì™„ë£Œ: {len(plan['steps'])}ë‹¨ê³„")
                return plan
            else:
                logger.warning("ìœ íš¨í•œ ì‹¤í–‰ ê³„íšì„ ìƒì„±í•˜ì§€ ëª»í•¨")
                return {}
                
        except Exception as e:
            logger.error(f"ì‹¤í–‰ ê³„íš ìˆ˜ë¦½ ì¤‘ ì˜¤ë¥˜: {e}")
            return {}

    async def _execute_plan(
        self, agent: Any, plan: Dict[str, Any], streaming_callback: Optional[Callable[[str], None]] = None
    ) -> Dict[int, Any]:
        """
        ê³„íšëœ ë‹¨ê³„ë“¤ì„ ìˆœì°¨ì ìœ¼ë¡œ ì‹¤í–‰ (ê³ ê¸‰ ê¸°ëŠ¥ í¬í•¨)
        
        AdaptiveWorkflowì˜ ê³ ê¸‰ ê¸°ëŠ¥ë“¤ì„ í†µí•©:
        - ë‹¨ê³„ë³„ ì¬ì‹œë„ ë©”ì»¤ë‹ˆì¦˜
        - ë§¤ê°œë³€ìˆ˜ ë™ì  ì¹˜í™˜
        - ì‹¤í–‰ ê²°ê³¼ ê²€ì¦
        - ì»¨í…ìŠ¤íŠ¸ ë©”ëª¨ë¦¬ ê´€ë¦¬
        
        Args:
            agent: BaseAgent ì¸ìŠ¤í„´ìŠ¤
            plan: ì‹¤í–‰ ê³„íš ë”•ì…”ë„ˆë¦¬
            streaming_callback: ìŠ¤íŠ¸ë¦¬ë° ì½œë°±
            
        Returns:
            Dict[int, Any]: ë‹¨ê³„ë³„ ì‹¤í–‰ ê²°ê³¼
        """
        results = {}
        steps = plan.get("steps", [])
        
        if streaming_callback:
            streaming_callback(f"âš¡ {len(steps)}ë‹¨ê³„ ì‹¤í–‰ ì‹œì‘\n")

        for step in steps:
            step_num = step.get("step", 0)
            tool_name = step.get("tool")
            args = step.get("args", {})
            desc = step.get("desc", f"ë‹¨ê³„ {step_num}")
            
            # ë§¤ê°œë³€ìˆ˜ ë™ì  ì¹˜í™˜ (ì´ì „ ë‹¨ê³„ ê²°ê³¼ ì°¸ì¡°)
            processed_args = self._process_step_arguments(args, results)
            
            # ë‹¨ê³„ë³„ ì¬ì‹œë„ ë¡œì§
            retry_count = 0
            success = False
            last_error = None
            
            while retry_count <= self.max_retries and not success:
                try:
                    if self.mcp_tool_manager and tool_name:
                        logger.debug(f"ë‹¨ê³„ {step_num} ì‹¤í–‰ (ì‹œë„ {retry_count + 1}): {tool_name}")
                        
                        # ì‚¬ìš©ì ìŠ¹ì¸ ìš”ì²­ (ì¸í„°ë™í‹°ë¸Œ ëª¨ë“œ)
                        if self.interaction_mode and retry_count == 0:
                            approval = await self._request_step_approval(step, streaming_callback)
                            if not approval:
                                results[step_num] = {
                                    "success": False,
                                    "error": "ì‚¬ìš©ìê°€ ë‹¨ê³„ ì‹¤í–‰ì„ ê±°ë¶€í–ˆìŠµë‹ˆë‹¤",
                                    "description": desc,
                                    "skipped": True
                                }
                                break
                        
                        result = await self.mcp_tool_manager.call_mcp_tool(tool_name, processed_args)
                        
                        # ê²°ê³¼ ê²€ì¦
                        if self._validate_step_result(result, step):
                            results[step_num] = {
                                "success": True, 
                                "result": result,
                                "tool": tool_name,
                                "description": desc,
                                "retry_count": retry_count
                            }
                            
                            # ì»¨í…ìŠ¤íŠ¸ ë©”ëª¨ë¦¬ì— ì €ì¥
                            self.step_memory[step_num] = {
                                "result": result,
                                "tool": tool_name,
                                "timestamp": self._get_timestamp()
                            }
                            
                            success = True
                            if streaming_callback:
                                retry_msg = f" (ì¬ì‹œë„ {retry_count}íšŒ)" if retry_count > 0 else ""
                                streaming_callback(f"âœ… {desc}{retry_msg}\n")
                        else:
                            raise Exception("ê²°ê³¼ ê²€ì¦ ì‹¤íŒ¨")
                    else:
                        results[step_num] = {
                            "success": False, 
                            "error": "ë„êµ¬ ê´€ë¦¬ì ë˜ëŠ” ë„êµ¬ëª… ì—†ìŒ",
                            "description": desc
                        }
                        break
                        
                except Exception as e:
                    last_error = str(e)
                    retry_count += 1
                    logger.warning(f"ë‹¨ê³„ {step_num} ì‹¤í–‰ ì‹¤íŒ¨ (ì‹œë„ {retry_count}): {e}")
                    
                    if retry_count <= self.max_retries:
                        if streaming_callback:
                            streaming_callback(f"âš ï¸ {desc} ì‹¤íŒ¨, ì¬ì‹œë„ ì¤‘... ({retry_count}/{self.max_retries})\n")
                        
                        # ì¬ì‹œë„ ì „ ì ì‹œ ëŒ€ê¸°
                        import asyncio
                        await asyncio.sleep(1)
            
            # ìµœì¢… ì‹¤íŒ¨ ì²˜ë¦¬
            if not success:
                results[step_num] = {
                    "success": False, 
                    "error": last_error or "ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜",
                    "description": desc,
                    "retry_count": retry_count - 1
                }
                
                if streaming_callback:
                    streaming_callback(f"âŒ {desc} (ìµœì¢… ì‹¤íŒ¨)\n")

        successful_count = sum(1 for r in results.values() if r.get("success"))
        logger.info(f"ê³„íš ì‹¤í–‰ ì™„ë£Œ: {successful_count}/{len(steps)}ë‹¨ê³„ ì„±ê³µ")
        return results

    async def _integrate_results(
        self, agent: Any, message: str, plan: Dict[str, Any], results: Dict[int, Any],
        streaming_callback: Optional[Callable[[str], None]] = None
    ) -> str:
        """
        ì‹¤í–‰ ê²°ê³¼ë“¤ì„ í†µí•©í•˜ì—¬ ìµœì¢… ì‘ë‹µ ìƒì„±
        
        ì„±ê³µí•œ ë‹¨ê³„ë“¤ì˜ ê²°ê³¼ë¥¼ ì¢…í•©í•˜ì—¬ ì‚¬ìš©ì ìš”ì²­ì— ëŒ€í•œ 
        ì™„ì „í•˜ê³  ìœ ìš©í•œ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.
        
        Args:
            agent: BaseAgent ì¸ìŠ¤í„´ìŠ¤
            message: ì›ë˜ ì‚¬ìš©ì ìš”ì²­
            plan: ì‹¤í–‰ ê³„íš
            results: ë‹¨ê³„ë³„ ì‹¤í–‰ ê²°ê³¼
            streaming_callback: ìŠ¤íŠ¸ë¦¬ë° ì½œë°±
            
        Returns:
            str: í†µí•©ëœ ìµœì¢… ì‘ë‹µ
        """
        if streaming_callback:
            streaming_callback("ğŸ“ ê²°ê³¼ í†µí•© ì¤‘...\n")

        # ì„±ê³µí•œ ê²°ê³¼ë“¤ë§Œ ìˆ˜ì§‘
        successful_results = []
        for step_num, result in results.items():
            if result.get("success"):
                result_text = str(result.get("result", ""))[:300]  # ê¸¸ì´ ì œí•œ
                successful_results.append(f"ë‹¨ê³„ {step_num}: {result_text}")

        if not successful_results:
            return "ìš”ì²­ì„ ì²˜ë¦¬í•˜ëŠ” ì¤‘ ëª¨ë“  ë‹¨ê³„ê°€ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì„¸ìš”."

        integration_prompt = f"""ë‹¤ìŒ ì‹¤í–‰ ê²°ê³¼ë“¤ì„ ì¢…í•©í•˜ì—¬ ì‚¬ìš©ì ìš”ì²­ì— ëŒ€í•œ ì™„ì „í•œ ë‹µë³€ì„ ìƒì„±í•´ì£¼ì„¸ìš”:

ì›ë˜ ì‚¬ìš©ì ìš”ì²­: {message}
ì‹¤í–‰ ëª©í‘œ: {plan.get('goal', 'ëª©í‘œ ë¶ˆëª…')}

ë‹¨ê³„ë³„ ì‹¤í–‰ ê²°ê³¼:
{chr(10).join(successful_results)}

ìœ„ ê²°ê³¼ë“¤ì„ ë°”íƒ•ìœ¼ë¡œ:
1. ì‚¬ìš©ìì˜ ì›ë˜ ìš”ì²­ì´ ì–´ë–»ê²Œ ì²˜ë¦¬ë˜ì—ˆëŠ”ì§€ ì„¤ëª…
2. ê° ë‹¨ê³„ì˜ ê²°ê³¼ë¥¼ ë…¼ë¦¬ì ìœ¼ë¡œ ì—°ê²°í•˜ì—¬ í†µí•©ëœ ì •ë³´ ì œê³µ
3. ì‹¤ìš©ì ì´ê³  êµ¬ì²´ì ì¸ ë‹µë³€ ì‘ì„±
4. í•„ìš”ì‹œ ì¶”ê°€ ì¡°ì¹˜ ì‚¬í•­ ì•ˆë‚´

ì‚¬ìš©ìì—ê²Œ ë„ì›€ì´ ë˜ëŠ” ì™„ì „í•œ ë‹µë³€ì„ ì œê³µí•´ì£¼ì„¸ìš”."""

        try:
            final_response = await agent._generate_basic_response(integration_prompt, streaming_callback)
            logger.info("ê²°ê³¼ í†µí•© ì™„ë£Œ")
            return final_response
        except Exception as e:
            logger.error(f"ê²°ê³¼ í†µí•© ì¤‘ ì˜¤ë¥˜: {e}")
            return f"ê²°ê³¼ í†µí•© ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆì§€ë§Œ, ë‹¤ìŒ ì‘ì—…ë“¤ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤:\n" + "\n".join(successful_results)

    async def _generate_llm_response(
        self, agent: Any, message: str, streaming_callback: Optional[Callable[[str], None]] = None
    ) -> str:
        """
        ì¼ë°˜ LLM ì‘ë‹µ ìƒì„± (í´ë°± ë©”ì„œë“œ)
        
        MCP ë„êµ¬ë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ê±°ë‚˜ ë‹¤ë¥¸ ì²˜ë¦¬ ë°©ì‹ì´ ì‹¤íŒ¨í–ˆì„ ë•Œ
        ìˆœìˆ˜ LLM ê¸°ë°˜ìœ¼ë¡œ ì‘ë‹µì„ ìƒì„±í•©ë‹ˆë‹¤.
        
        Args:
            agent: BaseAgent ì¸ìŠ¤í„´ìŠ¤
            message: ì‚¬ìš©ì ë©”ì‹œì§€
            streaming_callback: ìŠ¤íŠ¸ë¦¬ë° ì½œë°±
            
        Returns:
            str: LLM ìƒì„± ì‘ë‹µ
        """
        logger.info("ì¼ë°˜ LLM ì‘ë‹µ ìƒì„±")
        
        try:
            if hasattr(agent, '_generate_basic_response'):
                return await agent._generate_basic_response(message, streaming_callback)
            else:
                return "ì‘ë‹µì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì—ì´ì „íŠ¸ ì„¤ì •ì„ í™•ì¸í•´ ì£¼ì„¸ìš”."
        except Exception as e:
            logger.error(f"LLM ì‘ë‹µ ìƒì„± ì‹¤íŒ¨: {e}")
            return f"ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"

    def _extract_json_from_response(self, response: str) -> Dict[str, Any]:
        """
        LLM ì‘ë‹µì—ì„œ JSON ë°ì´í„° ì¶”ì¶œ
        
        ë§ˆí¬ë‹¤ìš´ ì½”ë“œ ë¸”ë¡ì´ë‚˜ ì¼ë°˜ í…ìŠ¤íŠ¸ì— í¬í•¨ëœ JSONì„ íŒŒì‹±í•©ë‹ˆë‹¤.
        ì—¬ëŸ¬ íŒ¨í„´ì„ ì‹œë„í•˜ì—¬ ìµœëŒ€í•œ JSONì„ ì¶”ì¶œí•˜ë ¤ê³  ì‹œë„í•©ë‹ˆë‹¤.
        
        Args:
            response: LLM ì‘ë‹µ í…ìŠ¤íŠ¸
            
        Returns:
            Dict[str, Any]: íŒŒì‹±ëœ JSON ë°ì´í„° (ì‹¤íŒ¨ ì‹œ ë¹ˆ ë”•ì…”ë„ˆë¦¬)
        """
        try:
            import re

            # ë‹¤ì–‘í•œ JSON ì¶”ì¶œ íŒ¨í„´ ì‹œë„
            patterns = [
                r'```(?:json)?\s*(\{.*?\})\s*```',  # ë§ˆí¬ë‹¤ìš´ ì½”ë“œ ë¸”ë¡
                r'(\{[^{}]*"[^"]*"[^{}]*\})',       # ê¸°ë³¸ JSON ê°ì²´
                r'(\{.*?\})'                        # ë‹¨ìˆœ ì¤‘ê´„í˜¸ íŒ¨í„´
            ]
            
            for pattern in patterns:
                match = re.search(pattern, response, re.DOTALL)
                if match:
                    try:
                        return json.loads(match.group(1))
                    except json.JSONDecodeError:
                        continue
            
            # íŒ¨í„´ ë§¤ì¹­ ì‹¤íŒ¨ ì‹œ ì „ì²´ í…ìŠ¤íŠ¸ë¡œ ì‹œë„
            return json.loads(response.strip())
            
        except Exception as e:
            logger.debug(f"JSON ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return {}

    # === AdaptiveWorkflow ê³ ê¸‰ ê¸°ëŠ¥ í—¬í¼ ë©”ì„œë“œë“¤ ===
    
    def _process_step_arguments(self, args: Dict[str, Any], previous_results: Dict[int, Any]) -> Dict[str, Any]:
        """
        ë‹¨ê³„ ë§¤ê°œë³€ìˆ˜ ë™ì  ì¹˜í™˜
        
        ì´ì „ ë‹¨ê³„ ê²°ê³¼ë¥¼ ì°¸ì¡°í•˜ëŠ” ë§¤ê°œë³€ìˆ˜ë¥¼ ì‹¤ì œ ê°’ìœ¼ë¡œ ì¹˜í™˜í•©ë‹ˆë‹¤.
        ì˜ˆ: {"file": "$step_1"} -> {"file": "actual_filename.txt"}
        
        Args:
            args: ì›ë³¸ ë§¤ê°œë³€ìˆ˜
            previous_results: ì´ì „ ë‹¨ê³„ ì‹¤í–‰ ê²°ê³¼
            
        Returns:
            Dict[str, Any]: ì¹˜í™˜ëœ ë§¤ê°œë³€ìˆ˜
        """
        if not args or not previous_results:
            return args
            
        processed = {}
        for key, value in args.items():
            if isinstance(value, str) and value.startswith("$step_"):
                try:
                    step_num = int(value.replace("$step_", ""))
                    if step_num in previous_results and previous_results[step_num].get("success"):
                        result = previous_results[step_num]["result"]
                        # ê²°ê³¼ì—ì„œ ì ì ˆí•œ ê°’ ì¶”ì¶œ (íŒŒì¼ëª…, ë‚´ìš© ë“±)
                        processed[key] = self._extract_reference_value(result, key)
                    else:
                        processed[key] = value  # ì¹˜í™˜ ì‹¤íŒ¨ ì‹œ ì›ë³¸ ìœ ì§€
                except ValueError:
                    processed[key] = value
            else:
                processed[key] = value
                
        return processed
    
    def _extract_reference_value(self, result: Any, context_key: str) -> str:
        """
        ë‹¨ê³„ ê²°ê³¼ì—ì„œ ì»¨í…ìŠ¤íŠ¸ì— ë§ëŠ” ê°’ ì¶”ì¶œ
        
        Args:
            result: ë‹¨ê³„ ì‹¤í–‰ ê²°ê³¼
            context_key: ë§¤ê°œë³€ìˆ˜ í‚¤ (íŒíŠ¸ë¡œ ì‚¬ìš©)
            
        Returns:
            str: ì¶”ì¶œëœ ê°’
        """
        if isinstance(result, str):
            return result
        elif isinstance(result, dict):
            # í‚¤ ì´ë¦„ì— ë”°ë¼ ì ì ˆí•œ ê°’ ì¶”ì¶œ
            if context_key in ["file", "path", "filename"]:
                return result.get("filename", result.get("path", str(result)))
            elif context_key in ["content", "text", "data"]:
                return result.get("content", result.get("text", str(result)))
            else:
                return str(result)
        else:
            return str(result)
    
    async def _request_step_approval(self, step: Dict[str, Any], streaming_callback: Optional[Callable[[str], None]] = None) -> bool:
        """
        ì‚¬ìš©ìì—ê²Œ ë‹¨ê³„ ì‹¤í–‰ ìŠ¹ì¸ ìš”ì²­
        
        Args:
            step: ì‹¤í–‰í•  ë‹¨ê³„ ì •ë³´
            streaming_callback: ìŠ¤íŠ¸ë¦¬ë° ì½œë°±
            
        Returns:
            bool: ìŠ¹ì¸ ì—¬ë¶€ (True: ìŠ¹ì¸, False: ê±°ë¶€)
        """
        if not self.interaction_mode:
            return True
            
        tool_name = step.get("tool", "ì•Œ ìˆ˜ ì—†ìŒ")
        desc = step.get("desc", "ì„¤ëª… ì—†ìŒ")
        args = step.get("args", {})
        
        approval_message = f"""
ğŸ”§ ë‹¨ê³„ ì‹¤í–‰ ìŠ¹ì¸ ìš”ì²­:
- ë„êµ¬: {tool_name}
- ì„¤ëª…: {desc}
- ë§¤ê°œë³€ìˆ˜: {json.dumps(args, ensure_ascii=False, indent=2)}

ì‹¤í–‰í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/n): """
        
        if streaming_callback:
            streaming_callback(approval_message)
            
        # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ì‚¬ìš©ì ì…ë ¥ì„ ë°›ì•„ì•¼ í•˜ì§€ë§Œ, 
        # ì—¬ê¸°ì„œëŠ” ê¸°ë³¸ì ìœ¼ë¡œ ìŠ¹ì¸ìœ¼ë¡œ ì²˜ë¦¬ (ì›Œí¬í”Œë¡œìš° ì»¨í…ìŠ¤íŠ¸ì—ì„œëŠ” ìë™ ìŠ¹ì¸)
        return True
    
    def _validate_step_result(self, result: Any, step: Dict[str, Any]) -> bool:
        """
        ë‹¨ê³„ ì‹¤í–‰ ê²°ê³¼ ê²€ì¦
        
        Args:
            result: ì‹¤í–‰ ê²°ê³¼
            step: ë‹¨ê³„ ì •ë³´
            
        Returns:
            bool: ê²€ì¦ ì„±ê³µ ì—¬ë¶€
        """
        if result is None:
            return False
            
        # ê¸°ë³¸ ê²€ì¦: ê²°ê³¼ê°€ ì¡´ì¬í•˜ê³  ì˜¤ë¥˜ê°€ ì—†ëŠ”ì§€ í™•ì¸
        if isinstance(result, dict):
            if "error" in result or "exception" in result:
                return False
            if result.get("success") is False:
                return False
                
        # ë„êµ¬ë³„ íŠ¹ë³„ ê²€ì¦ (í•„ìš”ì‹œ í™•ì¥)
        tool_name = step.get("tool", "")
        if tool_name == "file_read" and not result:
            return False
        elif tool_name == "web_search" and not result:
            return False
            
        return True
    
    def _get_timestamp(self) -> str:
        """í˜„ì¬ íƒ€ì„ìŠ¤íƒ¬í”„ ë°˜í™˜"""
        from datetime import datetime
        return datetime.now().isoformat()
    
    def set_interaction_mode(self, interactive: bool) -> None:
        """ì¸í„°ë™ì…˜ ëª¨ë“œ ì„¤ì •"""
        self.interaction_mode = interactive
        logger.debug(f"SmartWorkflow ì¸í„°ë™ì…˜ ëª¨ë“œ: {interactive}")
    
    def _generate_plan_hash(self, plan: Dict[str, Any]) -> str:
        """ê³„íš í•´ì‹œ ìƒì„± (ì¤‘ë³µ ê°ì§€ìš©)"""
        import hashlib
        plan_str = json.dumps(plan, sort_keys=True)
        return hashlib.sha256(plan_str.encode()).hexdigest()
    
    def _is_duplicate_plan(self, plan: Dict[str, Any]) -> bool:
        """ê³„íš ì¤‘ë³µ ì—¬ë¶€ í™•ì¸"""
        plan_hash = self._generate_plan_hash(plan)
        if plan_hash in self.executed_plan_hashes:
            return True
        self.executed_plan_hashes.add(plan_hash)
        return False
    
    async def _refine_execution_plan(
        self, agent: Any, message: str, original_plan: Dict[str, Any], 
        streaming_callback: Optional[Callable[[str], None]] = None
    ) -> Dict[str, Any]:
        """
        ì‹¤í–‰ ê³„íš ê°œì„  ë° ìˆ˜ì •
        
        ì‹¤íŒ¨í•œ ê³„íšì´ë‚˜ ì¤‘ë³µ ê³„íšì„ ë¶„ì„í•˜ì—¬ ê°œì„ ëœ ìƒˆë¡œìš´ ê³„íšì„ ìƒì„±í•©ë‹ˆë‹¤.
        
        Args:
            agent: BaseAgent ì¸ìŠ¤í„´ìŠ¤
            message: ì›ë³¸ ì‚¬ìš©ì ìš”ì²­
            original_plan: ê¸°ì¡´ ê³„íš
            streaming_callback: ìŠ¤íŠ¸ë¦¬ë° ì½œë°±
            
        Returns:
            Dict[str, Any]: ê°œì„ ëœ ì‹¤í–‰ ê³„íš
        """
        tools = await self._get_available_tools()
        tools_desc = "\n".join([f"- {tool.name}: {tool.description}" for tool in tools])
        
        failure_context = self.execution_context.get("failure_analysis", "")
        
        refinement_prompt = f"""ê¸°ì¡´ ì‹¤í–‰ ê³„íšì„ ë¶„ì„í•˜ì—¬ ê°œì„ ëœ ìƒˆë¡œìš´ ê³„íšì„ ìˆ˜ë¦½í•´ì£¼ì„¸ìš”:

ì›ë³¸ ì‚¬ìš©ì ìš”ì²­: {message}

ê¸°ì¡´ ê³„íš:
{json.dumps(original_plan, ensure_ascii=False, indent=2)}

ì‹¤íŒ¨ ë¶„ì„ (ìˆëŠ” ê²½ìš°):
{failure_context}

ì‚¬ìš© ê°€ëŠ¥í•œ ë„êµ¬ë“¤:
{tools_desc}

ê°œì„  ì§€ì¹¨:
1. ê¸°ì¡´ ê³„íšì˜ ë¬¸ì œì ì„ ë¶„ì„í•˜ê³  ë‹¤ë¥¸ ì ‘ê·¼ ë°©ë²• ì‹œë„
2. ì‹¤íŒ¨í•œ ë‹¨ê³„ëŠ” ëŒ€ì•ˆ ë„êµ¬ë‚˜ ë‹¤ë¥¸ ë§¤ê°œë³€ìˆ˜ ì‚¬ìš©
3. ë‹¨ê³„ ìˆœì„œ ìµœì í™” ë° ë¶ˆí•„ìš”í•œ ë‹¨ê³„ ì œê±°
4. ë” ì•ˆì •ì ì´ê³  ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ë°©ë²• ì„ íƒ

ë‹¤ìŒ í˜•ì‹ì˜ JSONìœ¼ë¡œ ê°œì„ ëœ ê³„íšì„ ì‘ì„±í•´ì£¼ì„¸ìš”:
{{
    "goal": "ê°œì„ ëœ ëª©í‘œ ì„¤ëª…",
    "improvements": "ê¸°ì¡´ ê³„íš ëŒ€ë¹„ ê°œì„ ì‚¬í•­",
    "steps": [
        {{
            "step": 1,
            "tool": "ë„êµ¬ëª…",
            "args": {{"ë§¤ê°œë³€ìˆ˜": "ê°’"}},
            "desc": "ê°œì„ ëœ ë‹¨ê³„ ì„¤ëª…"
        }}
    ]
}}"""

        if streaming_callback:
            streaming_callback("ğŸ”§ ê³„íš ê°œì„  ì¤‘...\n")

        try:
            response = await agent._generate_basic_response(refinement_prompt, None)
            refined_plan = self._extract_json_from_response(response)
            
            if refined_plan and refined_plan.get("steps"):
                logger.debug(f"ê³„íš ê°œì„  ì™„ë£Œ: {refined_plan.get('improvements', 'ê°œì„ ì‚¬í•­ ì—†ìŒ')}")
                return refined_plan
            else:
                logger.warning("ê³„íš ê°œì„  ì‹¤íŒ¨")
                return {}
                
        except Exception as e:
            logger.error(f"ê³„íš ê°œì„  ì¤‘ ì˜¤ë¥˜: {e}")
            return {}
    
    def _analyze_execution_failures(self, results: Dict[int, Any]) -> str:
        """
        ì‹¤í–‰ ì‹¤íŒ¨ ì›ì¸ ë¶„ì„
        
        Args:
            results: ë‹¨ê³„ë³„ ì‹¤í–‰ ê²°ê³¼
            
        Returns:
            str: ì‹¤íŒ¨ ì›ì¸ ë¶„ì„ í…ìŠ¤íŠ¸
        """
        failures = []
        for step_num, result in results.items():
            if not result.get("success"):
                error = result.get("error", "ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜")
                tool = result.get("tool", "ì•Œ ìˆ˜ ì—†ìŒ")
                desc = result.get("description", f"ë‹¨ê³„ {step_num}")
                
                failures.append(f"ë‹¨ê³„ {step_num} ({tool}): {desc} - {error}")
        
        if not failures:
            return "ì‹¤í–‰ ì‹¤íŒ¨ ì—†ìŒ"
            
        analysis = "ì‹¤í–‰ ì‹¤íŒ¨ ë¶„ì„:\n" + "\n".join(failures)
        
        # ê³µí†µ ì‹¤íŒ¨ íŒ¨í„´ ë¶„ì„
        common_errors = []
        for failure in failures:
            if "ê¶Œí•œ" in failure or "permission" in failure.lower():
                common_errors.append("ê¶Œí•œ ë¬¸ì œ")
            elif "íŒŒì¼" in failure and "ì—†" in failure:
                common_errors.append("íŒŒì¼ ê²½ë¡œ ë¬¸ì œ")
            elif "ë„¤íŠ¸ì›Œí¬" in failure or "connection" in failure.lower():
                common_errors.append("ë„¤íŠ¸ì›Œí¬ ì—°ê²° ë¬¸ì œ")
            elif "ë§¤ê°œë³€ìˆ˜" in failure or "argument" in failure.lower():
                common_errors.append("ë§¤ê°œë³€ìˆ˜ ì˜¤ë¥˜")
        
        if common_errors:
            analysis += f"\n\nê³µí†µ ë¬¸ì œ íŒ¨í„´: {', '.join(set(common_errors))}"
            
        return analysis
    
    def get_execution_statistics(self) -> Dict[str, Any]:
        """
        ì‹¤í–‰ í†µê³„ ì •ë³´ ë°˜í™˜
        
        Returns:
            Dict[str, Any]: ì‹¤í–‰ í†µê³„
        """
        return {
            "executed_plans": len(self.executed_plan_hashes),
            "step_memory_size": len(self.step_memory),
            "context_keys": list(self.execution_context.keys()),
            "max_retries": self.max_retries,
            "interaction_mode": self.interaction_mode
        }
    
    def clear_execution_history(self) -> None:
        """ì‹¤í–‰ íˆìŠ¤í† ë¦¬ ì´ˆê¸°í™”"""
        self.execution_context.clear()
        self.step_memory.clear()
        self.executed_plan_hashes.clear()
        self.retry_count.clear()
        logger.info("SmartWorkflow ì‹¤í–‰ íˆìŠ¤í† ë¦¬ ì´ˆê¸°í™” ì™„ë£Œ")

    # === ë ˆê±°ì‹œ í˜¸í™˜ ì¸í„°í˜ì´ìŠ¤ ===
    
    async def process(self, message: str, context: List[ConversationMessage] = None) -> LLMResponse:
        """
        ë ˆê±°ì‹œ í˜¸í™˜ ì¸í„°í˜ì´ìŠ¤
        
        ConversationMessage ê¸°ë°˜ì˜ ê¸°ì¡´ ì¸í„°í˜ì´ìŠ¤ì™€ í˜¸í™˜ì„±ì„ ì œê³µí•©ë‹ˆë‹¤.
        ìƒˆë¡œìš´ ì½”ë“œì—ì„œëŠ” run() ë©”ì„œë“œ ì‚¬ìš©ì„ ê¶Œì¥í•©ë‹ˆë‹¤.
        
        Args:
            message: ì‚¬ìš©ì ë©”ì‹œì§€
            context: ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ (ì‚¬ìš©ë˜ì§€ ì•ŠìŒ)
            
        Returns:
            LLMResponse: ì‘ë‹µ ê°ì²´
        """
        try:
            if self.llm_service:
                response = await self.llm_service.generate_response(
                    context or [ConversationMessage(role="user", content=message)]
                )
                return response
            
            return LLMResponse(
                response="LLM ì„œë¹„ìŠ¤ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.",
                metadata={"error": "llm_service_not_initialized", "workflow": self.workflow_name}
            )
        except Exception as e:
            return LLMResponse(
                response=f"ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}",
                metadata={"error": str(e), "workflow": self.workflow_name}
            ) 