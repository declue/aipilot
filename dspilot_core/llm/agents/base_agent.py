"""
BaseAgent ëª¨ë“ˆ
==============

`BaseAgent` ëŠ” ëª¨ë“  DSPilot LLM ì—ì´ì „íŠ¸ì˜ **ê³µí†µ ê¸°ëŠ¥ì„ ìº¡ìŠí™”**í•˜ëŠ” ì¶”ìƒ
ë² ì´ìŠ¤ í´ë˜ìŠ¤ì…ë‹ˆë‹¤. ì£¼ìš” ì±…ì„ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.

1. **LLM ì„¤ì • ë¡œë”© & ê²€ì¦**  
   `ConfigManager` ë¡œë¶€í„° `LLMConfig` ë”•ì…”ë„ˆë¦¬ë¥¼ ë°›ì•„ pydantic ëª¨ë¸ë¡œ ë§ˆìƒ¬ë§
   í›„ `LLMConfigValidator` ë¡œ ì²´í¬.
2. **Service ì´ˆê¸°í™”**  
   - `LLMService` : LangChain Chat API í˜¸ì¶œ ë˜í¼  
   - `ConversationService` : ì‚¬ìš©ìÂ·ì‹œìŠ¤í…œÂ·assistant ë©”ì‹œì§€ ìŠ¤í† ë¦¬ì§€
3. **Tool Result Processing**  
   `ToolProcessorMixin` ê³¼ `ToolResultProcessorRegistry` ë¡œ MCP ì‹¤í–‰ ê²°ê³¼ë¥¼
   í›„ì²˜ë¦¬ ë° ìš”ì•½.
4. **Mode Dispatch**  
   `generate_response()` ì—ì„œ ëª¨ë“œ(basic / mcp_tools / workflow)ì— ë”°ë¼
   ë³„ë„ í•¸ë“¤ëŸ¬ë¡œ ë¶„ê¸°.
5. **ReAct Agent ì§€ì›**  
   LangGraph ê¸°ë°˜ React agentë¥¼ ì´ˆê¸°í™”í•˜ì—¬ *ììœ¨ ë„êµ¬ ì‚¬ìš©* ëª¨ë“œë¥¼ ì§€ì›.

í™•ì¥ ê°€ì´ë“œ
-----------
â€¢ ì»¤ìŠ¤í…€ Agent ë¥¼ ë§Œë“¤ë ¤ë©´ ë³¸ í´ë˜ìŠ¤ë¥¼ ìƒì† í›„ `_handle_*_mode` ë˜ëŠ”
  `generate_response` ë¥¼ ì˜¤ë²„ë¼ì´ë“œí•˜ì„¸ìš”.
â€¢ ë¯¹ìŠ¤ì¸ ì„¤ê³„ë¡œ íŠ¹ì • ê¸°ëŠ¥(ì„¤ì •, ëŒ€í™”, ê²°ê³¼ ì²˜ë¦¬)ì„ ì¬ì •ì˜í•˜ê¸° ìš©ì´í•©ë‹ˆë‹¤.

Mermaid íë¦„
------------
```mermaid
stateDiagram-v2
    [*] --> LoadConfig
    LoadConfig --> InitServices
    InitServices --> WaitQuery
    WaitQuery -->|generate_response| DispatchMode
    DispatchMode --> BasicFlow
    DispatchMode --> ToolFlow
    DispatchMode --> WorkflowFlow
    BasicFlow --> ReturnAnswer
    ToolFlow --> ReturnAnswer
    WorkflowFlow --> ReturnAnswer
    ReturnAnswer --> WaitQuery
```
"""

import json
import logging
import re
import uuid
from typing import Any, Callable, Dict, List, Optional

from langchain_core.messages import HumanMessage
from langchain_core.runnables import RunnableConfig
from langchain_openai import ChatOpenAI
from langgraph.checkpoint.memory import MemorySaver
from langgraph.prebuilt import create_react_agent

from dspilot_core.llm.agents.mixins.config_mixin import ConfigMixin
from dspilot_core.llm.agents.mixins.conversation_mixin import ConversationMixin
from dspilot_core.llm.agents.mixins.tool_processor_mixin import ToolProcessorMixin
from dspilot_core.llm.interfaces.llm_interface import LLMInterface
from dspilot_core.llm.models.conversation_message import ConversationMessage
from dspilot_core.llm.models.llm_config import LLMConfig
from dspilot_core.llm.processors.base_processor import ToolResultProcessorRegistry
from dspilot_core.llm.processors.search_processor import SearchToolResultProcessor
from dspilot_core.llm.services.conversation_service import ConversationService
from dspilot_core.llm.services.llm_service import LLMService
from dspilot_core.llm.validators.config_validator import LLMConfigValidator
from dspilot_core.llm.workflow.workflow_utils import astream_graph
from dspilot_core.util.logger import setup_logger

logger = setup_logger(__name__) or logging.getLogger(__name__)


class BaseAgent(ConfigMixin, ConversationMixin, ToolProcessorMixin, LLMInterface):
    """LLMAgent ì˜ ê³µí†µ ê¸°ëŠ¥ì„ ë‹´ë‹¹í•˜ëŠ” ë² ì´ìŠ¤ í´ë˜ìŠ¤"""

    def __init__(self, config_manager: Any, mcp_tool_manager: Optional[Any] = None) -> None:
        self.config_manager = config_manager
        self.mcp_tool_manager = mcp_tool_manager

        # ì„¤ì • ë¡œë“œ
        self._load_config()

        # ì„œë¹„ìŠ¤ ì´ˆê¸°í™”
        self.llm_service = LLMService(self.llm_config)
        self.conversation_service = ConversationService()

        # í”„ë¡œì„¸ì„œ ë ˆì§€ìŠ¤íŠ¸ë¦¬ (ì§€ì—° ì´ˆê¸°í™”)
        self._processor_registry: Optional[ToolResultProcessorRegistry] = None

        # íˆìŠ¤í† ë¦¬ (í•˜ìœ„ í˜¸í™˜ì„± ìœ ì§€)
        self.history: List[Dict[str, str]] = []

        # ê³ ìœ  ìŠ¤ë ˆë“œ ID
        self.thread_id = str(uuid.uuid4())

        # í•˜ìœ„ í˜¸í™˜: ì™¸ë¶€ì—ì„œ ì§ì ‘ ì ‘ê·¼ ê°€ëŠ¥í•œ _client ì†ì„±
        # ì‹¤ì œ ChatOpenAI ì¸ìŠ¤í„´ìŠ¤ëŠ” llm_service ë‚´ë¶€ì— ì¡´ì¬í•˜ì§€ë§Œ, ì¼ë¶€ í…ŒìŠ¤íŠ¸ì—ì„œ
        # ì´ ì†ì„±ì˜ ì¡´ì¬ ì—¬ë¶€ë§Œì„ í™•ì¸í•˜ë¯€ë¡œ ê¸°ë³¸ê°’ì„ None ìœ¼ë¡œ ë‘¡ë‹ˆë‹¤.
        self._client = None  # pylint: disable=attribute-defined-outside-init

        # ReAct Agent ê´€ë ¨ ì†ì„± ì¶”ê°€
        self.react_agent: Optional[Any] = None
        self.checkpointer: Optional[Any] = MemorySaver()

        logger.debug("BaseAgent ì´ˆê¸°í™” ì™„ë£Œ")

    # ---------------------------------------------------------------------
    # ì„¤ì • ë° ì„œë¹„ìŠ¤ ë¡œë“œ
    # ---------------------------------------------------------------------
    def _load_config(self) -> None:
        """config_manager ë¡œë¶€í„° LLM ì„¤ì • ë¡œë“œ"""
        try:
            cfg_dict = self.config_manager.get_llm_config()
            self.llm_config = LLMConfig.from_dict(cfg_dict)

            # ì„¤ì • ê²€ì¦ ì ìš©
            try:

                LLMConfigValidator.validate_config(self.llm_config)
                logger.debug(
                    "LLM ì„¤ì • ë¡œë“œ ë° ê²€ì¦ ì™„ë£Œ: model=%s, mode=%s",
                    self.llm_config.model, self.llm_config.mode
                )
            except Exception as validation_exc:
                logger.warning("LLM ì„¤ì • ê²€ì¦ ì‹¤íŒ¨: %s", validation_exc)
                # ê²€ì¦ ì‹¤íŒ¨í•´ë„ ì„¤ì •ì€ ë¡œë“œëœ ìƒíƒœë¡œ ì§„í–‰

        except Exception as exc:  # pylint: disable=broad-except
            logger.error("LLM ì„¤ì • ë¡œë“œ ì‹¤íŒ¨: %s", exc)
            # ì•ˆì „í•œ ê¸°ë³¸ê°’
            self.llm_config = LLMConfig(
                api_key="",
                base_url=None,
                model="gpt-4o-mini",
                max_tokens=1000,
                temperature=0.7,
                streaming=True,
                mode="basic",
                workflow=None,
            )

    # ---------------------------------------------------------------------
    # ì™¸ë¶€ API (conversation)
    # ---------------------------------------------------------------------
    def add_user_message(self, message: str) -> None:
        self.conversation_service.add_user_message(message)
        self.history.append({"role": "user", "content": message})

    def add_assistant_message(self, message: str) -> None:
        self.conversation_service.add_assistant_message(message)
        self.history.append({"role": "assistant", "content": message})

    def clear_conversation(self) -> None:
        self.conversation_service.clear_conversation()
        self.history.clear()
        self.thread_id = str(uuid.uuid4())
        logger.info("ëŒ€í™” íˆìŠ¤í† ë¦¬ ì´ˆê¸°í™”")

    def get_conversation_history(self) -> List[Dict[str, str]]:
        messages = self.conversation_service.get_messages_as_dict()
        history: List[Dict[str, str]] = []
        for msg in messages:
            if isinstance(msg, dict):
                history.append(
                    {"role": str(msg.get("role", "")),
                     "content": str(msg.get("content", ""))}
                )
        return history

    async def cleanup(self) -> None:  # noqa: D401
        await self.llm_service.cleanup()
        logger.debug("Agent ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì™„ë£Œ")

    # ---------------------------------------------------------------------
    # ê³µí†µ í—¬í¼
    # ---------------------------------------------------------------------
    def _get_llm_mode(self) -> str:
        """LLM ë™ì‘ ëª¨ë“œ(basic/workflow/mcp_tools ë“±)ë¥¼ ë°˜í™˜.

        â€¢ `self.llm_config` ê°€ dataclass ë˜ëŠ” dict ëª¨ë‘ ì§€ì›
        â€¢ config_manager.get_config_value("LLM","mode") ê°’ì´ ì¡´ì¬í•˜ë©´ ìš°ì„ 
        """

        # 1) config_manager ì— ëª…ì‹œëœ ëª¨ë“œ ìš°ì„ 
        try:
            if hasattr(self, "config_manager") and hasattr(self.config_manager, "get_config_value"):
                cfg_mode = self.config_manager.get_config_value(
                    "LLM", "mode", None)
                if cfg_mode:
                    return str(cfg_mode).lower()
        except Exception:  # pragma: no cover
            pass

        # 2) dataclass ê°ì²´
        if hasattr(self.llm_config, "mode"):
            cfg_mode = getattr(self.llm_config, "mode")
            if cfg_mode:
                return str(cfg_mode).lower()

        # 3) dict í˜•íƒœ
        if isinstance(self.llm_config, dict):
            cfg_mode = self.llm_config.get("mode")
            if cfg_mode:
                return str(cfg_mode).lower()

        return "basic"

    def _create_response_data(
        self,
        response: str,
        reasoning: str = "",
        used_tools: Optional[List[str]] = None,
    ) -> Dict[str, Any]:
        if used_tools is None:
            used_tools = []
        self.add_assistant_message(response)
        return {"response": response, "reasoning": reasoning, "used_tools": used_tools}

    def _create_error_response(self, error_msg: str, detail: str = "") -> Dict[str, Any]:
        response = f"ì£„ì†¡í•©ë‹ˆë‹¤. {error_msg}"
        self.add_assistant_message(response)
        return {"response": response, "reasoning": detail, "used_tools": []}

    async def _generate_basic_response(
        self,
        message: str,
        streaming_callback: Optional[Callable[[str], None]] = None,
    ) -> str:
        """ê¸°ë³¸ LLM ì‘ë‹µ ìƒì„± (ì›Œí¬í”Œë¡œìš°ì—ì„œ ì‚¬ìš©)"""
        try:
            # ë©”ì‹œì§€ë¥¼ ConversationMessageë¡œ ë³€í™˜
            messages = [
                ConversationMessage(role="user", content=message)
            ]

            # LLM ì„œë¹„ìŠ¤ë¥¼ í†µí•´ ì‘ë‹µ ìƒì„±
            response = await self.llm_service.generate_response(
                messages=messages,
                streaming_callback=streaming_callback
            )

            return response.response

        except Exception as e:
            logger.error(f"ê¸°ë³¸ ì‘ë‹µ ìƒì„± ì‹¤íŒ¨: {e}")
            return f"ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"

    # ------------------------------------------------------------------
    # ë„êµ¬ ê²°ê³¼ ì²˜ë¦¬ ë° LLM ë¶„ì„ (SearchTool í¬í•¨)
    # ------------------------------------------------------------------
    def _get_processor_registry(self) -> ToolResultProcessorRegistry:
        if self._processor_registry is None:
            self._processor_registry = ToolResultProcessorRegistry()
            self._processor_registry.register(SearchToolResultProcessor())
            logger.debug("ë„êµ¬ ê²°ê³¼ í”„ë¡œì„¸ì„œ ë ˆì§€ìŠ¤íŠ¸ë¦¬ ì´ˆê¸°í™” ì™„ë£Œ")
        return self._processor_registry

    def _format_tool_results(self, used_tools: List[str], tool_results: Dict[str, str]) -> str:
        try:
            return self._get_processor_registry().process_tool_results(used_tools, tool_results)
        except Exception as exc:  # pylint: disable=broad-except
            logger.error("ë„êµ¬ ê²°ê³¼ í¬ë§·íŒ… ì˜¤ë¥˜: %s", exc)
            return "ë„êµ¬ ê²°ê³¼ë¥¼ ì²˜ë¦¬í•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."

    async def _analyze_tool_results_with_llm(
        self,
        user_message: str,
        used_tools: List[str],
        tool_results: Dict[str, str],
        streaming_callback: Optional[Callable[[str], None]] = None,
    ) -> str:
        try:
            logger.debug("LLM ë¶„ì„ ì‹œì‘ (%dê°œ ë„êµ¬)", len(used_tools))

            # ë„êµ¬ ê²°ê³¼ì— error ê°€ ìˆëŠ”ì§€ ë¨¼ì € í™•ì¸ --------------------------------
            errors: List[str] = []

            for raw in tool_results.values():
                try:
                    data = json.loads(raw)
                    if isinstance(data, dict) and "error" in data:
                        errors.append(str(data["error"]))
                except Exception:
                    if "error" in str(raw).lower():
                        errors.append(str(raw))

            if errors:
                # ì—¬ëŸ¬ ì—ëŸ¬ ë©”ì‹œì§€ë¥¼ ë¬¶ì–´ ì‚¬ìš©ìì—ê²Œ ì§ì ‘ ì „ë‹¬
                msg = "\n".join(f"- {e}" for e in errors)
                logger.info("ë„êµ¬ ê²°ê³¼ì— error ë°œê²¬ â†’ ê·¸ëŒ€ë¡œ ë°˜í™˜")
                return f"ìš”ì²­ì„ ì²˜ë¦¬í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\në„êµ¬ ì˜¤ë¥˜:\n{msg}"

            formatted_results = self._format_tool_results(
                used_tools, tool_results)

            # ë‹¤ì¤‘ ë„êµ¬ ê²°ê³¼ ì¢…í•©ì„ ìœ„í•œ ê°œì„ ëœ í”„ë¡¬í”„íŠ¸
            tools_count = len(used_tools)
            tools_summary = ", ".join(used_tools)

            analysis_prompt = (
                f"ì‚¬ìš©ì ìš”ì²­: {user_message}\n\n"
                f"ì‹¤í–‰ëœ ë„êµ¬ë“¤ ({tools_count}ê°œ): {tools_summary}\n\n"
                f"ìˆ˜ì§‘ëœ ì •ë³´:\n{formatted_results}\n\n"
                "ìœ„ ì •ë³´ë“¤ì„ ì¢…í•©í•˜ì—¬ ì‚¬ìš©ìì˜ ìš”ì²­ì— ëŒ€í•œ ì™„ì „í•˜ê³  ìœ ìš©í•œ ë‹µë³€ì„ ì œê³µí•´ì£¼ì„¸ìš”.\n\n"
                "**ë‹¤ì¤‘ ë„êµ¬ ê²°ê³¼ ì¢…í•© ì§€ì¹¨:**\n"
                "- ì—¬ëŸ¬ ë„êµ¬ì˜ ê²°ê³¼ë¥¼ ë…¼ë¦¬ì ìœ¼ë¡œ ì—°ê²°í•˜ì—¬ í†µí•©ëœ ë‹µë³€ ì œê³µ\n"
                "- ê° ë„êµ¬ ê²°ê³¼ì˜ í•µì‹¬ ì •ë³´ë¥¼ íš¨ê³¼ì ìœ¼ë¡œ í™œìš©\n"
                "- ì‚¬ìš©ìê°€ ì›í•˜ëŠ” ëª¨ë“  ì •ë³´ë¥¼ í¬ê´„ì ìœ¼ë¡œ í¬í•¨\n"
                "- ì •ë³´ ê°„ì˜ ê´€ë ¨ì„±ì´ë‚˜ ì°¨ì´ì ì´ ìˆë‹¤ë©´ ëª…í™•íˆ ì„¤ëª…\n"
                "- ê°„ê²°í•˜ë©´ì„œë„ ì™„ì „í•œ ë‹µë³€ìœ¼ë¡œ êµ¬ì„±\n"
                "- í•„ìš”ì‹œ ì‹œê°„ìˆœ, ì¤‘ìš”ë„ìˆœ ë“±ìœ¼ë¡œ ì •ë³´ë¥¼ êµ¬ì¡°í™”\n\n"
                "ì‚¬ìš©ìì˜ ì§ˆë¬¸ ì˜ë„ë¥¼ ì •í™•íˆ íŒŒì•…í•˜ì—¬ ê°€ì¥ ìœ ìš©í•œ í˜•íƒœë¡œ ì •ë³´ë¥¼ ì œê³µí•´ì£¼ì„¸ìš”."
            )

            # ë¹ˆ í”„ë¡¬í”„íŠ¸ ë°©ì§€
            if not analysis_prompt or not analysis_prompt.strip():
                logger.warning("ë¶„ì„ í”„ë¡¬í”„íŠ¸ê°€ ë¹„ì–´ìˆìŒ")
                return formatted_results

            # ConversationMessage ê°ì²´ ì‚¬ìš© (llm_serviceì™€ í˜¸í™˜)
            temp_messages = [ConversationMessage(
                role="user", content=analysis_prompt)]
            response = await self.llm_service.generate_response(temp_messages, streaming_callback)

            # ì‘ë‹µ ê²€ì¦
            if not response or not hasattr(response, 'response'):
                logger.warning("LLM ì‘ë‹µ ê°ì²´ê°€ ìœ íš¨í•˜ì§€ ì•ŠìŒ")
                return formatted_results

            result = response.response.strip() if response.response else ""
            return result or formatted_results
        except Exception as exc:  # pylint: disable=broad-except
            logger.error("LLM ë¶„ì„ ì˜¤ë¥˜: %s", exc)
            return self._format_tool_results(used_tools, tool_results)

    def _substitute_tool_placeholders(self, text: str, tool_results: Dict[str, str]) -> str:

        out = str(text)
        for tool_name, raw in tool_results.items():
            try:
                data = json.loads(raw)
                result_str = data.get("result", raw)
            except Exception:  # pylint: disable=broad-except
                result_str = raw

            patterns = [
                rf"`[^`]*{tool_name}\([^`]*`",  # ë°±í‹± í¬í•¨ í˜¸ì¶œ
                rf"{tool_name}\([^)]*\)",  # ì¼ë°˜ í˜¸ì¶œ
            ]
            for pat in patterns:
                out = re.sub(pat, result_str, out)
        return out

    # ------------------------------------------------------------------
    # LLM ëª¨ë¸ ìƒì„± (ReactAgent ë“±ì—ì„œ ì‚¬ìš©)
    # ------------------------------------------------------------------
    def _create_llm_model(self) -> Optional[ChatOpenAI]:
        try:
            model_name = str(self.llm_config.model)
            openai_params: Dict[str, Any] = {
                "model": model_name,
                "temperature": float(self.llm_config.temperature),
            }
            if self.llm_config.api_key:
                openai_params["api_key"] = str(self.llm_config.api_key)
            if self.llm_config.base_url:
                openai_params["base_url"] = str(self.llm_config.base_url)
            if getattr(self.llm_config, "streaming", None) is not None:
                openai_params["streaming"] = bool(self.llm_config.streaming)

            # Gemini ëª¨ë¸ì— ëŒ€í•œ íŠ¹ë³„ ì²˜ë¦¬
            if "gemini" in model_name.lower():
                # GeminiëŠ” í•¨ìˆ˜ í˜¸ì¶œ ì²˜ë¦¬ê°€ ê¹Œë‹¤ë¡œìš°ë¯€ë¡œ ë” ì•ˆì „í•œ ì„¤ì • ì‚¬ìš©
                # OpenAI í˜¸í™˜ í˜•ì‹ì´ë¯€ë¡œ í‘œì¤€ íŒŒë¼ë¯¸í„°ë§Œ ì‚¬ìš©
                openai_params["max_tokens"] = 2048
                openai_params["timeout"] = 120
                # ìŠ¤íŠ¸ë¦¬ë° ë¹„í™œì„±í™”ë¡œ ë” ì•ˆì •ì ì¸ ì²˜ë¦¬
                openai_params["streaming"] = False
                logger.debug("Gemini ëª¨ë¸ íŠ¹ë³„ ì„¤ì • ì ìš© (í•¨ìˆ˜ í˜¸ì¶œ ì•ˆì •í™”)")

            logger.debug(
                "ChatOpenAI ì´ˆê¸°í™” íŒŒë¼ë¯¸í„°: %s",
                {k: v for k, v in openai_params.items() if k != "api_key"},
            )
            return ChatOpenAI(
                model=openai_params["model"],
                temperature=openai_params["temperature"],
                api_key=openai_params.get("api_key"),
                base_url=openai_params.get("base_url"),
                streaming=openai_params.get("streaming", True),
                timeout=openai_params.get("timeout", 60),
                max_retries=3,
            )
        except Exception as exc:  # pylint: disable=broad-except
            logger.error("LLM ëª¨ë¸ ìƒì„± ì‹¤íŒ¨: %s", exc)
            return None

    # ------------------------------------------------------------------
    # ReAct Agent ê¸°ëŠ¥ ì¶”ê°€ ---------------------------------------------
    # ------------------------------------------------------------------
    async def _initialize_react_agent(self) -> bool:
        """langgraphì˜ create_react_agentë¥¼ ì‚¬ìš©í•´ ì—ì´ì „íŠ¸ ê°ì²´ ìƒì„±"""
        try:
            if not MemorySaver or create_react_agent is None or self.mcp_tool_manager is None:
                return False

            tools = await self.mcp_tool_manager.get_langchain_tools()
            if not tools:
                logger.warning("ì‚¬ìš© ê°€ëŠ¥í•œ MCP ë„êµ¬ê°€ ì—†ìŠµë‹ˆë‹¤")
                return False

            llm = self._create_llm_model()
            if llm is None:
                return False

            prompt = self._get_react_system_prompt()
            self.react_agent = create_react_agent(
                llm, tools, checkpointer=self.checkpointer, prompt=prompt
            )
            logger.info("ReactAgent ì´ˆê¸°í™” ì™„ë£Œ (ë„êµ¬ %dê°œ)", len(tools))
            return True
        except Exception as exc:  # pylint: disable=broad-except
            logger.error("ReactAgent ì´ˆê¸°í™” ì‹¤íŒ¨: %s", exc)
            return False

    def _get_react_system_prompt(self) -> str:
        """ReAct Agentìš© ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ë°˜í™˜"""
        return (
            "ë‹¹ì‹ ì€ ë²”ìš© MCP ë„êµ¬ë¥¼ í™œìš©í•˜ëŠ” ì§€ëŠ¥í˜• AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.\n\n"
            "**í•µì‹¬ ì—­í• :**\n"
            "- ì‚¬ìš©ìì˜ ìš”ì²­ì„ ì •í™•íˆ ì´í•´í•˜ê³  ì ì ˆí•œ ë„êµ¬ë¥¼ ì„ íƒí•˜ì—¬ ì •ë³´ë¥¼ ìˆ˜ì§‘\n"
            "- ì‚¬ìš©ì ìš”ì²­ì˜ ì„±ê²©ì— ë”°ë¼ ê°„ê²°í•˜ê±°ë‚˜ ìì„¸í•œ ë‹µë³€ì„ ì œê³µ\n"
            "- ë²”ìš© MCP ë„êµ¬ ì‹œìŠ¤í…œì˜ í™•ì¥ì„±ê³¼ í˜¸í™˜ì„±ì„ ê³ ë ¤í•œ ì‘ë‹µ ìƒì„±\n\n"
            "**ì‘ë‹µ ìŠ¤íƒ€ì¼ ê°€ì´ë“œë¼ì¸:**\n\n"
            "1. **ë‹¨ìˆœ ì •ë³´ ìš”ì²­**:\n"
            "   - í•µì‹¬ ì •ë³´ë§Œ ê°„ê²°í•˜ê²Œ ì œê³µ\n"
            "   - ë¶ˆí•„ìš”í•œ ë¶„ì„ì´ë‚˜ ë¶€ê°€ ì„¤ëª… ìƒëµ\n"
            "   - ì§ì ‘ì ì´ê³  ëª…í™•í•œ ë‹µë³€\n\n"
            "2. **ë³µì¡í•œ ë¶„ì„ ìš”ì²­**:\n"
            "   - ìƒì„¸í•œ ë¶„ì„ê³¼ ì¸ì‚¬ì´íŠ¸ ì œê³µ\n"
            "   - ë‹¤ê°ë„ ê²€í†  ë° ë§¥ë½ ì •ë³´ í¬í•¨\n"
            "   - êµ¬ì¡°í™”ëœ í˜•íƒœì˜ í¬ê´„ì  ë‹µë³€\n\n"
            "3. **ì¼ë°˜ì ì¸ ì§ˆë¬¸**:\n"
            "   - ì§ˆë¬¸ ë²”ìœ„ì— ë§ëŠ” ì ì ˆí•œ ìˆ˜ì¤€ì˜ ë‹µë³€\n"
            "   - í•„ìš”ì— ë”°ë¼ ê°„ê²°í•˜ê±°ë‚˜ ìƒì„¸í•˜ê²Œ ì¡°ì ˆ\n\n"
            "**ì‘ì—… ì ˆì°¨:**\n"
            "1. **ìš”ì²­ ë¶„ì„**: ì‚¬ìš©ì ì§ˆë¬¸ì˜ ë³µì¡ë„ì™€ ê¸°ëŒ€ ì‘ë‹µ ìˆ˜ì¤€ íŒŒì•…\n"
            "2. **ë„êµ¬ í™œìš©**: í•„ìš”í•œ ì •ë³´ ìˆ˜ì§‘\n"
            "3. **ì ì ˆí•œ ì‘ë‹µ ìƒì„±**: ìš”ì²­ ì„±ê²©ì— ë§ëŠ” ë‹µë³€ ê¸¸ì´ì™€ ìƒì„¸ë„ ì¡°ì ˆ\n\n"
            "**ğŸš¨ ì¤‘ìš” ì›ì¹™ - ë°˜ë“œì‹œ ì¤€ìˆ˜:**\n"
            "- **ì‹œê°„/ë‚ ì§œ ì •ë³´**: í˜„ì¬ ì‹œê°„, ì˜¤ëŠ˜ ë‚ ì§œ, ëª‡ ì‹œ, ëª‡ ì¼ ë“± ëª¨ë“  ì‹œê°„ ê´€ë ¨ ì§ˆë¬¸ì—ëŠ” ë°˜ë“œì‹œ í•´ë‹¹ ë„êµ¬ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”. ì ˆëŒ€ ì¶”ì¸¡í•˜ì§€ ë§ˆì„¸ìš”.\n"
            "- **ë„êµ¬ ê²°ê³¼ ìš°ì„ **: ë„êµ¬ë¡œ ìˆ˜ì§‘í•œ ë°ì´í„°ë¥¼ ì£¼ìš” ê·¼ê±°ë¡œ ì‚¬ìš©\n"
            "- **í•œêµ­ì–´ ì‘ë‹µ**: ëª¨ë“  ì‘ë‹µì€ ìì—°ìŠ¤ëŸ¬ìš´ í•œêµ­ì–´ë¡œ ì‘ì„±\n"
            "- **ë²”ìš©ì„± ê³ ë ¤**: ë‹¤ì–‘í•œ MCP ë„êµ¬ì™€ í˜¸í™˜ë˜ëŠ” ì¼ê´€ëœ ì ‘ê·¼ ë°©ì‹\n"
            "- **ìš”ì²­ ë§ì¶¤í˜• ì‘ë‹µ**: ì‚¬ìš©ìê°€ ì›í•˜ëŠ” ìˆ˜ì¤€ì˜ ì •ë³´ë§Œ ì œê³µ\n\n"
            "**íŠ¹ë³„ ì§€ì¹¨:**\n"
            "- ì‹œê°„, ë‚ ì§œ, í˜„ì¬ ì •ë³´ ê´€ë ¨ ì§ˆë¬¸ì—ëŠ” ë°˜ë“œì‹œ ì ì ˆí•œ ë„êµ¬ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”\n"
            "- ì‚¬ìš©ìì˜ ì§ˆë¬¸ì´ ê°„ë‹¨í•˜ë©´ ê°„ë‹¨í•˜ê²Œ, ë³µì¡í•˜ë©´ ìƒì„¸í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”\n"
            "- ë„êµ¬ ì‚¬ìš© í›„ ê²°ê³¼ë¥¼ ì‚¬ìš©ì ìš”ì²­ ìˆ˜ì¤€ì— ë§ê²Œ ì ì ˆíˆ ê°€ê³µí•˜ì—¬ ì œê³µí•˜ì„¸ìš”\n"
            "- ì¶”ì¸¡í•˜ì§€ ë§ê³  í•­ìƒ ìµœì‹  ì •ë³´ë¥¼ ìœ„í•´ ë„êµ¬ë¥¼ í™œìš©í•˜ì„¸ìš”\n"
            "- 'ì˜¤ëŠ˜ ëª‡ ì¼?', 'ì§€ê¸ˆ ë‚ ì§œ?', 'í˜„ì¬ ì‹œê°„?' ê°™ì€ ì§ˆë¬¸ì—ëŠ” 100% ë„êµ¬ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”"
        )

    async def run_react_agent(
        self,
        user_message: str,
        streaming_callback: Optional[Callable[[str], None]] = None,
    ) -> Dict[str, Any]:
        """ReAct agentì˜ ainvoke/astream_graph ì‹¤í–‰"""
        if self.react_agent is None:
            # ì´ˆê¸°í™” ì‹œë„
            if not await self._initialize_react_agent():
                return {"response": "ReAct ì—ì´ì „íŠ¸ë¥¼ ì´ˆê¸°í™”í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.", "used_tools": []}

        # ì…ë ¥ ê²€ì¦
        if not user_message or not user_message.strip():
            logger.warning("ë¹ˆ ì‚¬ìš©ì ë©”ì‹œì§€")
            return {"response": "ë©”ì‹œì§€ë¥¼ ì…ë ¥í•´ ì£¼ì„¸ìš”.", "used_tools": []}

        # thread_id ê²€ì¦ ë° ê¸°ë³¸ê°’ ì„¤ì •
        thread_id = getattr(self, "thread_id", None) or "default-thread"
        if not isinstance(thread_id, str):
            thread_id = str(thread_id)

        try:
            config = RunnableConfig(recursion_limit=100, configurable={
                                    "thread_id": thread_id})

            # ë©”ì‹œì§€ ë‚´ìš© ê²€ì¦ ë° ì •ë¦¬
            clean_message = user_message.strip()
            if not clean_message:
                logger.warning("ë¹ˆ ë©”ì‹œì§€ ë‚´ìš©")
                return {"response": "ë©”ì‹œì§€ë¥¼ ì…ë ¥í•´ ì£¼ì„¸ìš”.", "used_tools": []}

            # Gemini ëª¨ë¸ì˜ ê²½ìš° ë” ì—„ê²©í•œ ë©”ì‹œì§€ ê²€ì¦
            model_name = str(self.llm_config.model).lower()
            if "gemini" in model_name:
                # GeminiëŠ” íŠ¹ì • ë¬¸ìë‚˜ í˜•ì‹ì— ë¯¼ê°í•˜ë¯€ë¡œ ì¶”ê°€ ì •ë¦¬
                clean_message = clean_message.replace(
                    '\x00', '').replace('\n\n\n', '\n\n')
                if len(clean_message) > 8000:  # Gemini í† í° ì œí•œ ê³ ë ¤
                    clean_message = clean_message[:8000] + "..."
                logger.debug("Gemini ëª¨ë¸ìš© ë©”ì‹œì§€ ì •ë¦¬ ì™„ë£Œ")

            messages = [HumanMessage(content=clean_message)]
            inputs = {"messages": messages}

            logger.debug(
                "ReactAgent ì‹¤í–‰ ì„¤ì •: thread_id=%s, message_length=%d, model=%s",
                thread_id,
                len(clean_message),
                model_name,
            )
        except Exception as exc:
            logger.error("ReactAgent ì„¤ì • ìƒì„± ì‹¤íŒ¨: %s", exc)
            return {"response": "ReAct ì—ì´ì „íŠ¸ ì„¤ì •ì— ë¬¸ì œê°€ ìˆìŠµë‹ˆë‹¤.", "used_tools": []}

        # ìŠ¤íŠ¸ë¦¬ë° ì§€ì› ì—¬ë¶€
        if streaming_callback is not None:
            accumulated: str = ""
            used_tools: List[str] = []

            try:
                async for chunk in astream_graph(self.react_agent, inputs, config=config):
                    # ì˜¤ë¥˜ ì²­í¬ ì²˜ë¦¬
                    if isinstance(chunk, dict) and chunk.get("type") == "error":
                        logger.error(
                            "ê·¸ë˜í”„ ìŠ¤íŠ¸ë¦¬ë° ì˜¤ë¥˜ ì²­í¬: %s", chunk.get(
                                "error", "Unknown error")
                        )
                        continue

                    # ê°„ë‹¨ ì²˜ë¦¬: AIMessage contentë§Œ ë½‘ì•„ ëˆ„ì 
                    if isinstance(chunk, dict) and "agent" in chunk:
                        for msg in chunk["agent"].get("messages", []):
                            if hasattr(msg, "content") and msg.content:
                                content = str(msg.content)
                                if content and content.strip():  # ë¹ˆ ë‚´ìš© í•„í„°ë§
                                    accumulated += content
                    # tool usage (ê°„ëµ)
                    if isinstance(chunk, dict) and "tools" in chunk:
                        for msg in chunk["tools"].get("messages", []):
                            if hasattr(msg, "name"):
                                used_tools.append(str(msg.name))

                    # ì‹¤ì œ ë‚´ìš©ì´ ìˆì„ ë•Œë§Œ ì½œë°± í˜¸ì¶œ
                    if accumulated and accumulated.strip() and streaming_callback is not None:
                        streaming_callback(accumulated)

                logger.debug(
                    "ìŠ¤íŠ¸ë¦¬ë° ì™„ë£Œ: accumulated=%dì, tools=%dê°œ", len(
                        accumulated), len(used_tools)
                )
                return {"response": accumulated, "used_tools": used_tools}
            except Exception as exc:
                logger.error("ReactAgent ìŠ¤íŠ¸ë¦¬ë° ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: %s", exc)
                # ìŠ¤íŠ¸ë¦¬ë° ì‹¤íŒ¨ ì‹œ ë¹„ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ ì¬ì‹œë„í•˜ì§€ ì•Šê³  ë°”ë¡œ ì˜¤ë¥˜ ë°˜í™˜
                return {"response": f"ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(exc)}", "used_tools": []}

        # ë¹„ìŠ¤íŠ¸ë¦¬ë° ëª¨ë“œ
        try:
            logger.debug("ë¹„ìŠ¤íŠ¸ë¦¬ë° ëª¨ë“œë¡œ ReactAgent ì‹¤í–‰")
            result = await self.react_agent.ainvoke(inputs, config=config)
            response_text = ""
            used_tools: List[str] = []
            tool_results: Dict[str, str] = {}

            if isinstance(result, dict) and "messages" in result:
                # ë§ˆì§€ë§‰ AIMessage ì°¾ê¸°
                for msg in reversed(result["messages"]):
                    if hasattr(msg, "content") and msg.content:
                        content = str(msg.content).strip()
                        if content:  # ë¹ˆ ë‚´ìš© í•„í„°ë§
                            response_text = content
                            break
                # ë„êµ¬ ë©”ì‹œì§€
                for msg in result["messages"]:
                    if str(type(msg)).find("ToolMessage") != -1:
                        if hasattr(msg, "name"):
                            used_tools.append(str(msg.name))
                            if hasattr(msg, "content") and msg.content:
                                tool_results[str(msg.name)] = str(msg.content)

            # í”Œë ˆì´ìŠ¤í™€ë” ì¹˜í™˜
            if response_text and tool_results:
                response_text = self._substitute_tool_placeholders(
                    response_text, tool_results)

            logger.debug(
                "ë¹„ìŠ¤íŠ¸ë¦¬ë° ì™„ë£Œ: response=%dì, tools=%dê°œ", len(
                    response_text), len(used_tools)
            )
            # workflow_name ì •ì˜ (ëˆ„ë½ëœ ë¶€ë¶„)
            workflow_name = "react_agent"
            result = self._create_response_data(response_text)
            result["workflow"] = str(workflow_name)
            return result
        except Exception as exc:
            logger.error("ReactAgent ë¹„ìŠ¤íŠ¸ë¦¬ë° ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: %s", exc)
            return {"response": f"ReAct ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(exc)}", "used_tools": []}

    # ------------------------------------------------------------------
    # ë²”ìš© ìë™ íˆ´ ë¼ìš°íŒ… ---------------------------------------------------
    # ------------------------------------------------------------------
    async def auto_tool_flow(
        self,
        user_message: str,
        streaming_callback: Optional[Callable[[str], None]] = None,
    ) -> Optional[Dict[str, Any]]:
        """LLMì´ ì§ì ‘ ë„êµ¬ë¥¼ ì„ íƒí•˜ê²Œ í•˜ëŠ” ë²”ìš©ì  ì ‘ê·¼ ë°©ì‹"""
        try:
            if self.mcp_tool_manager is None:
                return None

            logger.debug("ë²”ìš© ìë™ ë¼ìš°íŒ…: LLMì´ ì ì ˆí•œ ë„êµ¬ë¥¼ ì§ì ‘ ì„ íƒí•˜ë„ë¡ ì²˜ë¦¬")

            # ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë“  ë„êµ¬ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
            langchain_tools = await self.mcp_tool_manager.get_langchain_tools()
            if not langchain_tools:
                logger.warning("ì‚¬ìš© ê°€ëŠ¥í•œ ë„êµ¬ê°€ ì—†ìŠµë‹ˆë‹¤")
                return None

            # LLMì´ ì§ì ‘ ë„êµ¬ë¥¼ ì„ íƒí•˜ê³  ì‹¤í–‰í•˜ë„ë¡ ìœ„ì„
            llm = self._create_llm_model()
            if llm is None:
                return None

            # ë„êµ¬ ì„¤ëª… í¬í•¨í•œ í”„ë¡¬í”„íŠ¸ ìƒì„±
            tools_desc = "\n".join(
                [f"- {tool.name}: {tool.description}" for tool in langchain_tools])

            prompt = f"""ì‚¬ìš©ì ìš”ì²­: {user_message}

ì‚¬ìš© ê°€ëŠ¥í•œ ë„êµ¬ë“¤:
{tools_desc}

ìœ„ ìš”ì²­ì„ ì²˜ë¦¬í•˜ê¸° ìœ„í•´ í•„ìš”í•œ ë„êµ¬ë¥¼ ì„ íƒí•˜ê³  ë§¤ê°œë³€ìˆ˜ë¥¼ ê²°ì •í•˜ì„¸ìš”.

**ì¤‘ìš” ì§€ì¹¨:**
1. ë‹¨ì¼ ë„êµ¬ê°€ ì¶©ë¶„í•œ ê²½ìš°: {{"tool_name": "ë„êµ¬ëª…", "arguments": {{"param": "value"}}}}
2. ì—¬ëŸ¬ ë„êµ¬ê°€ í•„ìš”í•œ ê²½ìš°: [{{"tool_name": "ë„êµ¬1", "arguments": {{}}}}, {{"tool_name": "ë„êµ¬2", "arguments": {{}}}}]
3. ì‹œê°„/ë‚ ì§œ ì§ˆë¬¸: get_current_time ë˜ëŠ” get_current_date ì‚¬ìš©
4. ë‚ ì”¨ ì§ˆë¬¸: get_current_weather ë˜ëŠ” get_detailed_weather ì‚¬ìš©
5. ê²€ìƒ‰ì´ í•„ìš”í•œ ê²½ìš°: search_web ì‚¬ìš©
6. "ì‹œê°„ê³¼ ë‚ ì”¨"ì²˜ëŸ¼ ë‘ ê°€ì§€ ì •ë³´ê°€ í•„ìš”í•˜ë©´ ë°°ì—´ í˜•ì‹ìœ¼ë¡œ ë‘ ë„êµ¬ ëª¨ë‘ í¬í•¨

ë°˜ë“œì‹œ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•˜ì„¸ìš”."""

            try:
                response = await llm.ainvoke(prompt)
                response_text = response.content if hasattr(
                    response, 'content') else str(response)

                # ë§ˆí¬ë‹¤ìš´ ì½”ë“œ ë¸”ë¡ ì œê±°í•˜ê³  JSON ì¶”ì¶œ

                # ë§ˆí¬ë‹¤ìš´ ì½”ë“œ ë¸”ë¡ì„ ì°¾ì•„ì„œ JSON ì¶”ì¶œ
                json_patterns = [
                    r'```(?:json)?\s*(\[[^\]]*\])\s*```',  # ë§ˆí¬ë‹¤ìš´ ë¸”ë¡ ë‚´ JSON ë°°ì—´
                    r'```(?:json)?\s*(\{[^`]*\})\s*```',   # ë§ˆí¬ë‹¤ìš´ ë¸”ë¡ ë‚´ JSON ê°ì²´
                    # tool_nameì„ í¬í•¨í•œ JSON ë°°ì—´
                    r'(\[[^\]]*"tool_name"[^\]]*\])',
                    # tool_nameì„ í¬í•¨í•œ JSON ê°ì²´
                    r'(\{[^{}]*"tool_name"[^{}]*\})',
                    r'(\{.*?\})',                          # ì¼ë°˜ JSON ê°ì²´
                    r'(\[.*?\])'                           # ì¼ë°˜ JSON ë°°ì—´
                ]

                json_text = None
                for pattern in json_patterns:
                    match = re.search(pattern, response_text, re.DOTALL)
                    if match:
                        json_text = match.group(1).strip()
                        break

                if not json_text:
                    json_text = response_text.strip()

                logger.debug("ì¶”ì¶œëœ JSON í…ìŠ¤íŠ¸: %s", json_text)
                tool_selection = json.loads(json_text)

                # ë°°ì—´ í˜•ì‹ì¸ ê²½ìš° ì—¬ëŸ¬ ë„êµ¬ ìˆœì°¨ ì‹¤í–‰ ì§€ì›
                tools_to_execute = []
                if isinstance(tool_selection, list):
                    if tool_selection:
                        logger.debug(
                            "ë°°ì—´ í˜•ì‹ ë„êµ¬ ì„ íƒ ê°ì§€: %dê°œ ë„êµ¬ë¥¼ ìˆœì°¨ ì‹¤í–‰í•©ë‹ˆë‹¤", len(tool_selection))
                        tools_to_execute = tool_selection
                    else:
                        logger.warning("ë¹ˆ ë°°ì—´ì´ ë°˜í™˜ë˜ì—ˆìŠµë‹ˆë‹¤")
                        return None
                else:
                    tools_to_execute = [tool_selection]

                # ì—¬ëŸ¬ ë„êµ¬ ì‹¤í–‰
                tool_results = {}
                used_tools = []

                for i, tool_spec in enumerate(tools_to_execute):
                    selected_tool = tool_spec.get("tool_name")
                    arguments = tool_spec.get("arguments", {})

                    if not selected_tool:
                        logger.warning("ë„êµ¬ %d: tool_nameì´ ì—†ìŠµë‹ˆë‹¤", i+1)
                        continue

                    logger.debug("ë„êµ¬ %d/%d ì‹¤í–‰: %s, ë§¤ê°œë³€ìˆ˜: %s", i+1,
                                 len(tools_to_execute), selected_tool, arguments)

                    try:
                        tool_result_raw = await self.mcp_tool_manager.call_mcp_tool(selected_tool, arguments)
                        tool_results[selected_tool] = tool_result_raw
                        used_tools.append(selected_tool)

                        if streaming_callback and len(tools_to_execute) > 1:
                            streaming_callback(
                                f"ğŸ”§ {selected_tool} ì™„ë£Œ ({i+1}/{len(tools_to_execute)})\n")

                    except Exception as tool_exc:
                        logger.error("ë„êµ¬ %s ì‹¤í–‰ ì‹¤íŒ¨: %s",
                                     selected_tool, tool_exc)
                        tool_results[selected_tool] = json.dumps(
                            {"error": f"ë„êµ¬ ì‹¤í–‰ ì‹¤íŒ¨: {str(tool_exc)}"})
                        used_tools.append(selected_tool)

                if not used_tools:
                    logger.warning("ì‹¤í–‰í•  ìˆ˜ ìˆëŠ” ë„êµ¬ê°€ ì—†ìŠµë‹ˆë‹¤")
                    return None

                # ê²°ê³¼ ë¶„ì„
                analyzed = await self._analyze_tool_results_with_llm(
                    user_message, used_tools, tool_results, streaming_callback
                )
                if analyzed:
                    return {
                        "response": analyzed,
                        "reasoning": "ë²”ìš© ìë™ íˆ´ ë¼ìš°íŒ…",
                        "used_tools": used_tools,
                    }

                # ë¶„ì„ ì‹¤íŒ¨ ì‹œ í¬ë§·íŒ…ëœ ê²°ê³¼ ë°˜í™˜
                formatted = self._format_tool_results(used_tools, tool_results)
                return {
                    "response": formatted,
                    "reasoning": "ë²”ìš© ìë™ íˆ´ ë¼ìš°íŒ… (í¬ë§·íŒ…)",
                    "used_tools": used_tools,
                }

            except json.JSONDecodeError:
                # JSON í˜•ì‹ì´ ì•„ë‹Œ ì‘ë‹µì€ "ë„êµ¬ ì‹¤í–‰ì´ í•„ìš” ì—†ëŠ” ì§ì ‘ ë‹µë³€" ìœ¼ë¡œ ê°„ì£¼
                logger.debug("ë„êµ¬ ì„ íƒ JSON ë¯¸ê²€ì¶œ â€“ ì§ì ‘ ì‘ë‹µ ì²˜ë¦¬: %s", response_text[:100].replace("\n", " "))
                return {  # ê¸°ë³¸ ì‘ë‹µ ë°ì´í„° êµ¬ì¡°ì™€ ë§ì¶”ì–´ ì§ì ‘ ë°˜í™˜
                    "response": response_text.strip(),
                    "reasoning": "ë„êµ¬ ì‹¤í–‰ ë¶ˆí•„ìš”",
                    "used_tools": []
                }
            except Exception as inner_exc:
                logger.error("ë„êµ¬ ì„ íƒ/ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: %s", inner_exc)
                return None

        except Exception as exc:  # pylint: disable=broad-except
            logger.error("ë²”ìš© ìë™ íˆ´ ë¼ìš°íŒ… ì˜¤ë¥˜: %s", exc)
            return None

    # ------------------------------------------------------------------
    # í•„ìˆ˜ ì¶”ìƒ ë©”ì„œë“œ -----------------------------------------------------
    # ------------------------------------------------------------------
    async def generate_response(
        self,
        user_message: str,
        streaming_callback: Optional[Callable[[str], None]] = None,
    ) -> Dict[str, Any]:
        # ê³µìš© ì§„ì…ì  â€“ ëª¨ë“œì— ë”°ë¼ ë¶„ê¸° ì²˜ë¦¬

        # 1) ì‚¬ìš©ì ë©”ì‹œì§€ ê¸°ë¡
        self.add_user_message(user_message)

        mode = self._get_llm_mode()

        try:
            if mode == "basic":
                result = await self._handle_basic_mode(user_message, streaming_callback)
            elif mode == "workflow":
                result = await self._handle_workflow_mode(user_message, streaming_callback)
            elif mode == "mcp_tools":
                result = await self._handle_mcp_tools_mode(user_message, streaming_callback)
            else:
                # ì•Œ ìˆ˜ ì—†ëŠ” ëª¨ë“œ â€“ basic ì²˜ë¦¬
                result = await self._handle_basic_mode(user_message, streaming_callback)

            # ê²°ê³¼ê°€ None ì´ë©´ ì˜¤ë¥˜ ì²˜ë¦¬
            if not result:
                return self._create_error_response("í˜„ì¬ ì‘ë‹µì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

            return result

        except Exception as exc:  # pylint: disable=broad-except
            # ì˜ˆì™¸ ë°œìƒ ì‹œ ì˜¤ë¥˜ ì‘ë‹µ ë°˜í™˜
            logger.error("generate_response ì‹¤íŒ¨: %s", exc)
            return self._create_error_response("ì‘ë‹µ ìƒì„± ì¤‘ ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤", str(exc))

    # ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì € ì§€ì› --------------------------------------------------
    async def __aenter__(self):  # noqa: D401
        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb):  # noqa: D401
        await self.cleanup()

    def reinitialize_client(self) -> None:
        """LLM ì„¤ì • ë³€ê²½ ì‹œ ì„œë¹„ìŠ¤ ì¬ì´ˆê¸°í™”"""
        try:
            logger.info("BaseAgent ì¬ì´ˆê¸°í™” ì‹œì‘")
            self._load_config()
            self.llm_service = LLMService(self.llm_config)
            logger.info(
                "BaseAgent ì¬ì´ˆê¸°í™” ì™„ë£Œ: model=%s, mode=%s",
                self.llm_config.model,
                self.llm_config.mode,
            )
        except Exception as exc:  # pylint: disable=broad-except
            logger.error("BaseAgent ì¬ì´ˆê¸°í™” ì‹¤íŒ¨: %s", exc)

    # ------------------------------------------------------------------
    # ë ˆê±°ì‹œ í…ŒìŠ¤íŠ¸ í˜¸í™˜ìš© ê²½ëŸ‰ ë˜í¼ ë©”ì„œë“œë“¤ ------------------------------
    # ------------------------------------------------------------------

    async def _handle_basic_mode(
        self,
        message: str,
        streaming_callback: Optional[Callable[[str], None]] = None,
    ) -> Dict[str, Any]:  # noqa: D401
        """ê¸°ë³¸ ëª¨ë“œ ì²˜ë¦¬ â€“ ê¸°ì¡´ í…ŒìŠ¤íŠ¸ í˜¸í™˜ìš©."""

        response_text = await self._generate_basic_response(message, streaming_callback)
        return self._create_response_data(response_text)

    async def _handle_mcp_tools_mode(
        self,
        message: str,
        streaming_callback: Optional[Callable[[str], None]] = None,
    ) -> Dict[str, Any]:  # noqa: D401
        """MCP ë„êµ¬ ëª¨ë“œ ì²˜ë¦¬ â€“ ìµœì†Œ ë¡œì§ (í…ŒìŠ¤íŠ¸ ìŠ¤í…)."""

        if not self.mcp_tool_manager:
            return self._create_error_response("MCP ë„êµ¬ ê´€ë¦¬ìê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")

        try:
            result = await self.mcp_tool_manager.run_agent_with_tools(message)
            # run_agent_with_tools ëŠ” dict ë°˜í™˜ ë³´ì¥
            return {
                "response": result.get("response", ""),
                "reasoning": result.get("reasoning", ""),
                "used_tools": result.get("used_tools", []),
            }
        except Exception as exc:  # pragma: no cover
            return self._create_error_response("ë„êµ¬ ì‹¤í–‰ ì‹¤íŒ¨", str(exc))

    async def _handle_workflow_mode(
        self,
        message: str,
        streaming_callback: Optional[Callable[[str], None]] = None,
    ) -> Dict[str, Any]:  # noqa: D401
        """ì›Œí¬í”Œë¡œìš° ëª¨ë“œ ì²˜ë¦¬ â€“ ê¸°ë³¸ ì›Œí¬í”Œë¡œìš° ì‹¤í–‰."""

        try:
            from dspilot_core.llm.workflow.workflow_utils import get_workflow

            workflow_name = getattr(
                self.llm_config, "workflow", None) or "basic"
            try:
                workflow_cls = get_workflow(workflow_name)
            except Exception:
                workflow_cls = get_workflow("basic")

            workflow = workflow_cls()
            response_text = await workflow.run(self, message, streaming_callback)
            result = self._create_response_data(response_text)
            result["workflow"] = str(workflow_name)
            return result

        except Exception as exc:  # pragma: no cover
            return self._create_error_response("ì›Œí¬í”Œë¡œìš° ì²˜ë¦¬ ì¤‘ ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤", str(exc))
